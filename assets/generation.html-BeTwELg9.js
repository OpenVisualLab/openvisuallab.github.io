import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o as l,c as s,a as e,b as o,d as n,f as t}from"./app-DA55uAbc.js";const d={},h=t('<h1 id="world-models-autonomous-driving-latest-survey" tabindex="-1"><a class="header-anchor" href="#world-models-autonomous-driving-latest-survey"><span>World-Models-Autonomous-Driving-Latest-Survey</span></a></h1><p>A curated list of world model for autonmous driving. Keep updated.</p><h2 id="ğŸ“Œ-introduction" tabindex="-1"><a class="header-anchor" href="#ğŸ“Œ-introduction"><span>ğŸ“Œ Introduction</span></a></h2><h2 id="âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†" tabindex="-1"><a class="header-anchor" href="#âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†"><span>âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†</span></a></h2><h2 id="â¢-è®ºæ–‡æ±‡æ€»" tabindex="-1"><a class="header-anchor" href="#â¢-è®ºæ–‡æ±‡æ€»"><span>â¢ è®ºæ–‡æ±‡æ€»</span></a></h2>',5),c={href:"https://github.com/GigaAI-research/General-World-Models-Survey",target:"_blank",rel:"noopener noreferrer"},u={href:"https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey",target:"_blank",rel:"noopener noreferrer"},p=e("h2",{id:"â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹"},[e("span",null,"â¢ è®¤è¯†ä¸–ç•Œæ¨¡å‹")])],-1),g=e("h3",{id:"_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ"},[e("span",null,"1. ç®€å•ä»‹ç»ï¼ˆä»ä¸–ç•Œæ¨¡å‹--> è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆï¼‰")])],-1),m={href:"https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw",target:"_blank",rel:"noopener noreferrer"},v={href:"https://arxiv.org/abs/1803.10122",target:"_blank",rel:"noopener noreferrer"},b={href:"https://worldmodels.github.io/",target:"_blank",rel:"noopener noreferrer"},f={href:"https://www.bilibili.com/read/cv34465959/",target:"_blank",rel:"noopener noreferrer"},_={href:"https://blog.csdn.net/CV_Autobot/article/details/134002647",target:"_blank",rel:"noopener noreferrer"},A=e("h3",{id:"_2-è®ºæ–‡ç»¼è¿°",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-è®ºæ–‡ç»¼è¿°"},[e("span",null,"2.è®ºæ–‡ç»¼è¿°")])],-1),w=e("strong",null,[e("code",null,"arxiv")],-1),D={href:"https://arxiv.org/abs/2403.02622",target:"_blank",rel:"noopener noreferrer"},y={href:"https://arxiv.org/abs/2403.02622",target:"_blank",rel:"noopener noreferrer"},k=e("strong",null,[e("code",null,"arxiv")],-1),M={href:"https://arxiv.org/pdf/2401.12888.pdf",target:"_blank",rel:"noopener noreferrer"},C=e("strong",null,[e("code",null,"arxiv")],-1),x={href:"https://arxiv.org/pdf/2401.08045.pdf",target:"_blank",rel:"noopener noreferrer"},V=e("h2",{id:"_3-æŒ‘æˆ˜èµ›-workshops-challenges",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3-æŒ‘æˆ˜èµ›-workshops-challenges"},[e("span",null,"3.æŒ‘æˆ˜èµ› Workshops/Challenges")])],-1),S=e("strong",null,[e("code",null,"Challenges")],-1),P={href:"https://github.com/1x-technologies/1xgpt",target:"_blank",rel:"noopener noreferrer"},W=e("li",null,null,-1),T=e("strong",null,[e("code",null,"Challenges")],-1),I={href:"https://opendrivelab.com/challenge2024/",target:"_blank",rel:"noopener noreferrer"},L=e("h2",{id:"tutorials-talks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tutorials-talks"},[e("span",null,"Tutorials/Talks/")])],-1),E=e("strong",null,[e("code",null,"from Wayve")],-1),G={href:"https://www.youtube.com/watch?v=lNOs08byOhw",target:"_blank",rel:"noopener noreferrer"},R=e("li",null,null,-1),N={href:"https://www.youtube.com/watch?v=wMvYjiv6EpY",target:"_blank",rel:"noopener noreferrer"},F=e("h2",{id:"â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸"},[e("span",null,"â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬/ å…¬å¸")])],-1),O={id:"â– -ä¸Šæµ·ailab-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤-https-opendrivelab-com-publications",tabindex:"-1"},U={class:"header-anchor",href:"#â– -ä¸Šæµ·ailab-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤-https-opendrivelab-com-publications"},B={href:"https://opendrivelab.com/publications/",target:"_blank",rel:"noopener noreferrer"},z={id:"â– -é¦™æ¸¯ä¸­æ–‡å¤§å­¦-é™ˆé“ è€å¸ˆå›¢é˜Ÿ-geometric-controllable-visual-generation-a-systematic-solution-video",tabindex:"-1"},q={class:"header-anchor",href:"#â– -é¦™æ¸¯ä¸­æ–‡å¤§å­¦-é™ˆé“ è€å¸ˆå›¢é˜Ÿ-geometric-controllable-visual-generation-a-systematic-solution-video"},j={href:"https://www.bilibili.com/video/BV18T421v7Nf/?spm_id_from=333.337.search-card.all.click",target:"_blank",rel:"noopener noreferrer"},K={id:"â– -æä½³ç§‘æŠ€-æä½³ç§‘æŠ€drivedreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€worlddreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚-æ¨æ–‡",tabindex:"-1"},H={class:"header-anchor",href:"#â– -æä½³ç§‘æŠ€-æä½³ç§‘æŠ€drivedreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€worlddreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚-æ¨æ–‡"},J={href:"https://baijiahao.baidu.com/s?id=1799624134723943641",target:"_blank",rel:"noopener noreferrer"},X=e("h3",{id:"â– -wayveã€teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -wayveã€teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€"},[e("span",null,"â–  Wayveã€Teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€")])],-1),Z={id:"â– -è”šæ¥è½¦ä¼-https-www-qbitai-com-2024-07-172025-html",tabindex:"-1"},Y={class:"header-anchor",href:"#â– -è”šæ¥è½¦ä¼-https-www-qbitai-com-2024-07-172025-html"},Q={href:"https://www.qbitai.com/2024/07/172025.html",target:"_blank",rel:"noopener noreferrer"},$=t('<h2 id="â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘"><span>â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰</span></a></h2><h4 id="world-models-are-adept-at-representing-an-agent-s-spatio-temporal-knowledge-about-its-environment-through-the-prediction-of-future-changes" tabindex="-1"><a class="header-anchor" href="#world-models-are-adept-at-representing-an-agent-s-spatio-temporal-knowledge-about-its-environment-through-the-prediction-of-future-changes"><span>World Models are adept at representing an agent&#39;s spatio-temporal knowledge about its environment through the prediction of future changes.</span></a></h4><h4 id="there-are-two-main-types-of-world-models-in-autonomous-driving-aimed-at-reducing-driving-uncertainty-i-e-world-model-as-neural-driving-simulator-and-world-model-for-end-to-end-driving" tabindex="-1"><a class="header-anchor" href="#there-are-two-main-types-of-world-models-in-autonomous-driving-aimed-at-reducing-driving-uncertainty-i-e-world-model-as-neural-driving-simulator-and-world-model-for-end-to-end-driving"><span>There are two main types of world models in Autonomous Driving aimed at reducing driving uncertainty, i.e., World Model as Neural Driving Simulator and World Model for End-to-end Driving.</span></a></h4><h4 id="in-the-real-environment-methods-like-gaia-1-and-copilot4d-involve-utilizing-generative-models-to-construct-neural-simulators-that-produce-2d-or-3d-future-scenes-to-enhance-predictive-capabilities" tabindex="-1"><a class="header-anchor" href="#in-the-real-environment-methods-like-gaia-1-and-copilot4d-involve-utilizing-generative-models-to-construct-neural-simulators-that-produce-2d-or-3d-future-scenes-to-enhance-predictive-capabilities"><span>In the real environment, methods like GAIA-1 and Copilot4D involve utilizing generative models to construct neural simulators that produce 2D or 3D future scenes to enhance predictive capabilities.</span></a></h4><h4 id="in-the-simulation-environment-methods-such-as-mile-and-trafficbots-are-based-on-reinforcement-learning-enhancing-their-capacity-for-decision-making-and-future-prediction-thereby-paving-the-way-to-end-to-end-autonomous-driving" tabindex="-1"><a class="header-anchor" href="#in-the-simulation-environment-methods-such-as-mile-and-trafficbots-are-based-on-reinforcement-learning-enhancing-their-capacity-for-decision-making-and-future-prediction-thereby-paving-the-way-to-end-to-end-autonomous-driving"><span>In the simulation environment, methods such as MILE and TrafficBots are based on reinforcement learning, enhancing their capacity for decision-making and future prediction, thereby paving the way to end-to-end autonomous driving.</span></a></h4><h3 id="neural-driving-simulator-based-on-world-models" tabindex="-1"><a class="header-anchor" href="#neural-driving-simulator-based-on-world-models"><span>Neural Driving Simulator based on World Models</span></a></h3><h4 id="_2d-scene-generation" tabindex="-1"><a class="header-anchor" href="#_2d-scene-generation"><span>2D Scene Generation</span></a></h4>',7),ee={href:"https://arxiv.org/abs/2309.17080",target:"_blank",rel:"noopener noreferrer"},oe={href:"https://wayve.ai/thinking/scaling-gaia-1/",target:"_blank",rel:"noopener noreferrer"},re={href:"https://www.youtube.com/watch?v=6x-Xb_uT7ts",target:"_blank",rel:"noopener noreferrer"},ne={href:"https://drivedreamer.github.io/",target:"_blank",rel:"noopener noreferrer"},te={href:"https://github.com/JeffWang987/DriveDreamer",target:"_blank",rel:"noopener noreferrer"},ae={href:"https://arxiv.org/abs/2311.13549",target:"_blank",rel:"noopener noreferrer"},ie={href:"https://arxiv.org/abs/2310.07771",target:"_blank",rel:"noopener noreferrer"},le={href:"https://panacea-ad.github.io/",target:"_blank",rel:"noopener noreferrer"},se={href:"https://github.com/wenyuqing/panacea",target:"_blank",rel:"noopener noreferrer"},de={href:"https://drive-wm.github.io/",target:"_blank",rel:"noopener noreferrer"},he={href:"https://github.com/BraveGroup/Drive-WM",target:"_blank",rel:"noopener noreferrer"},ce={href:"https://arxiv.org/abs/2312.02934",target:"_blank",rel:"noopener noreferrer"},ue={href:"https://drivedreamer2.github.io/",target:"_blank",rel:"noopener noreferrer"},pe={href:"https://github.com/f1yfisher/DriveDreamer2",target:"_blank",rel:"noopener noreferrer"},ge={href:"https://arxiv.org/abs/2403.09630",target:"_blank",rel:"noopener noreferrer"},me={href:"https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file",target:"_blank",rel:"noopener noreferrer"},ve={href:"https://subjectdrive.github.io/",target:"_blank",rel:"noopener noreferrer"},be=e("h4",{id:"_3d-scene-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3d-scene-generation"},[e("span",null,"3D Scene Generation")])],-1),fe={href:"https://arxiv.org/abs/2311.01017",target:"_blank",rel:"noopener noreferrer"},_e={href:"https://arxiv.org/abs/2311.16038",target:"_blank",rel:"noopener noreferrer"},Ae={href:"https://github.com/wzzheng/OccWorld",target:"_blank",rel:"noopener noreferrer"},we={href:"https://arxiv.org/abs/2311.11762",target:"_blank",rel:"noopener noreferrer"},De={href:"https://www.zyrianov.org/lidardm/",target:"_blank",rel:"noopener noreferrer"},ye={href:"https://github.com/vzyrianov/lidardm",target:"_blank",rel:"noopener noreferrer"},ke=e("h4",{id:"_4d-pre-training-for-autonomous-driving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_4d-pre-training-for-autonomous-driving"},[e("span",null,"4D Pre-training for Autonomous Driving")])],-1),Me={href:"https://arxiv.org/abs/2308.07234",target:"_blank",rel:"noopener noreferrer"},Ce={href:"https://arxiv.org/abs/2312.17655",target:"_blank",rel:"noopener noreferrer"},xe={href:"https://github.com/OpenDriveLab/ViDAR",target:"_blank",rel:"noopener noreferrer"},Ve=e("li",null,[o("(2024 CVPR) DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving ["),e("a",{href:"XXX"},"Paper"),o("] (PKU)")],-1),Se=e("h3",{id:"end-to-end-driving-based-on-world-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#end-to-end-driving-based-on-world-models"},[e("span",null,"End-to-end Driving based on World Models")])],-1),Pe={href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"},We={href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"},Te={href:"https://github.com/wayveai/mile",target:"_blank",rel:"noopener noreferrer"},Ie={href:"https://arxiv.org/abs/2210.04017",target:"_blank",rel:"noopener noreferrer"},Le={href:"https://ieeexplore.ieee.org/abstract/document/10161243",target:"_blank",rel:"noopener noreferrer"},Ee={href:"https://arxiv.org/abs/2402.16720",target:"_blank",rel:"noopener noreferrer"},Ge=e("h3",{id:"others",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#others"},[e("span",null,"Others")])],-1),Re={href:"http://www.sci.brooklyn.cuny.edu/~parsons/courses/3415-fall-2011/papers/elfes.pdf",target:"_blank",rel:"noopener noreferrer"},Ne=t('<h3 id="â– -unsupervised-vad" tabindex="-1"><a class="header-anchor" href="#â– -unsupervised-vad"><span>â–  Unsupervised VAD</span></a></h3><ul><li><p><strong>Conference Papers</strong><br> [1] Learning Temporal Regularity in Video Sequences <strong>(CVPR 2016)</strong><br> [2] A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework --&gt;<strong>Proposed Shanghaitech dataset.</strong> [2] ğŸ‘Future Frame Prediction for Anomaly Detection -- A New Baseline <strong>(CVPR 2018)</strong><br> [3] ğŸ‘Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection <strong>(ICCV 2019)</strong> --&gt; <strong>The first to employ memory module on video anomaly detection</strong><br> [4] ğŸ‘Object-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection <strong>(CVPR 2019)</strong> --&gt; <strong>The first to combine object detection and vad to achieve object-level anomaly dtection.</strong><br> [5] AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network <strong>(ACM MM 2019)</strong> --&gt; <strong>The first hybrid model</strong><br> [6] ğŸ‘Learning Memory-guided Normality for Anomaly Detection <strong>(CVPR 2020)</strong> --&gt; <strong>Based on MemAE</strong><br> [7] Cluster Attention Contrast for Video Anomaly Detection <strong>(ACM MM 2020)</strong> --&gt; <strong>The first to apply Contrastive Learninig</strong><br> [8] ğŸ‘Anomaly Detection in Video via Self-Supervised and Multi-Task Learning <strong>(CVPR 2021)</strong> --&gt; <strong>object-level</strong><br> [9] ğŸ‘A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction <strong>(ICCV 2021)</strong> --&gt; <strong>Hybrid model</strong> [10] Anomaly Detection in Video Sequence with Appearance-Motion Correspondence (ICCV 2019) --&gt; <strong>Two stream network</strong><br> [11] Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder --&gt; <strong>Two stream network</strong><br> [12] Self-supervised Sparse Representation for Video Anomaly Detection <strong>(ECCV 2022)</strong> --&gt; A first attempt to slove unsupervised and weakly supervised VAD [13] Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles <strong>(ECCV 2022)</strong></p></li><li><p><strong>Joural Papers</strong><br> [1] VideoÂ AnomalyÂ DetectionÂ with Sparse Coding Inspired Deep Neural Networks <strong>(TPAMI 2021)</strong> [2] A Background-Agnostic Framework With Adversarial Training for Abnormal EventÂ DetectionÂ inÂ Video <strong>(TPAMI 2022)</strong><br> [3] Influence-Aware Attention Networks forÂ AnomalyÂ DetectionÂ in SurveillanceÂ Videos <strong>(TCSVT 2022)</strong><br> [4] Bidirectional Spatio-Temporal Feature Learning With Multiscale Evaluation for Video Anomaly Detection <strong>(TCSVT 2022)</strong><br> [5] AnomalyÂ DetectionÂ With Bidirectional Consistency inÂ Videos <strong>(TNNLS 2022)</strong><br> [6] Variational Abnormal BehaviorÂ DetectionÂ With Motion Consistency <strong>(TIP 2022)</strong><br> [7] DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos <strong>(TPAMI 2023)</strong> [8] A Hierarchical Spatio-Temporal Graph Convolutional Neural Network forÂ AnomalyÂ DetectionÂ inÂ Videos <strong>(TCSVT 2023)</strong><br> [9] Learnable Locality-Sensitive Hashing forÂ VideoÂ AnomalyÂ Detection <strong>(TCSVT 2023)</strong><br> [10] A Kalman Variational Autoencoder Model Assisted by Odometric Clustering forÂ VideoÂ Frame Prediction andÂ AnomalyÂ Detection <strong>(TIP 2023)</strong> [11] Abnormal EventÂ DetectionÂ and Localization via Adversarial Event Prediction <strong>(TNNLS 2023)</strong></p></li></ul><h3 id="â– -weakly-supervised-vad" tabindex="-1"><a class="header-anchor" href="#â– -weakly-supervised-vad"><span>â–  Weakly supervised VAD</span></a></h3><p>[1] ğŸ‘ Real-world Anomaly Detection in Surveillance Videos <strong>(CVPR 2018)</strong><br> [2] Weakly Supervised Video Anomaly Detection via Center-Guided Discrimative Learning <strong>(ICME 2020)</strong></p><p>[3] Decouple and Resolve: Transformer-Based Models for OnlineÂ AnomalyÂ DetectionÂ From Weakly LabeledÂ Videos <strong>(TIFS 2023)</strong></p><h2 id="â¢-ç»å…¸é¡¹ç›®" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸é¡¹ç›®"><span>â¢ ç»å…¸é¡¹ç›®</span></a></h2>',6),Fe={href:"https://github.com/cvlab-yonsei/MNAD",target:"_blank",rel:"noopener noreferrer"},Oe=t('<h2 id="â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘" tabindex="-1"><a class="header-anchor" href="#â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘"><span>â¢ å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘--&gt;</span></a></h2><p>ç”Ÿæˆå¼çš„World Modelå¯ä»¥è¢«ç”¨æ¥å½“ä½œä¸€ç§ä»¿çœŸå·¥å…·æ¥ç”Ÿæˆä»¿çœŸæ•°æ®ï¼Œç‰¹åˆ«æ˜¯æä¸ºå°‘è§çš„Corner Caseçš„æ•°æ®ã€‚ ç„¶è€ŒWorld Modelæ›´æœ‰æ½œåŠ›çš„åº”ç”¨æ–¹å‘æ˜¯World Modelå¯èƒ½ä¼šæˆä¸ºåƒGPTä¸€æ ·çš„è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼Œè€Œå…¶ä»–è‡ªåŠ¨é©¾é©¶å…·ä½“ä»»åŠ¡éƒ½ä¼šå›´ç»•è¿™ä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œç ”å‘æ„å»ºã€‚</p><h3 id="_1-definition" tabindex="-1"><a class="header-anchor" href="#_1-definition"><span>1. DEFINITION</span></a></h3><p>The aim of this TASK is to detect and automatically generate high-level explanations of anomalous events in video. Understanding the cause of an anomalous event is crucialas the required response is dependant on its nature andseverity. --&gt; Anomaly Detection &amp; Anoamly Explanation</p><h3 id="_2-related-work" tabindex="-1"><a class="header-anchor" href="#_2-related-work"><span>2. RELATED WORK</span></a></h3>',5),Ue=e("br",null,null,-1),Be=e("br",null,null,-1),ze={href:"https://github.com/zhanghm1995/awesome-world-models-for-AD?tab=readme-ov-file#Table-of-Content",target:"_blank",rel:"noopener noreferrer"},qe={href:"https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving/blob/main/papers.md#world-model--model-based-rl",target:"_blank",rel:"noopener noreferrer"},je={href:"https://github.com/chaytonmin/Awesome-Papers-World-Models-Autonomous-Driving",target:"_blank",rel:"noopener noreferrer"};function Ke(He,Je){const r=i("ExternalLinkIcon");return l(),s("div",null,[h,e("p",null,[o("[1] "),e("a",c,[o("https://github.com/GigaAI-research/General-World-Models-Survey"),n(r)]),o(" è¯¥ repo å†…æœ‰ç›®å‰ä¸–ç•Œæ¨¡å‹æ–¹å‘çš„ä¼˜ç§€è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ï¼šè§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶å’Œè‡ªä¸»ä»£ç†ã€‚å…¶ä¸­è‡ªåŠ¨é©¾é©¶åˆ†æˆç«¯åˆ°ç«¯ã€ä»¥åŠ2Dã€3Dç¥ç»æ¨¡æ‹Ÿå™¨æ–¹æ³•ã€‚ä¸–ç•Œæ¨¡å‹çš„æ–‡çŒ®ã€ å¼€æºcodeã€ ç»¼è¿°ã€‚")]),e("p",null,[o("[2] "),e("a",u,[o("https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey"),n(r)]),o(" è¯¥repo å†…ä»¥â€˜æ—¶é—´â€™ä¸ºé¡ºåºç²¾é€‰ç›¸å…³ä¸–ç•Œè‡ªåŠ¨é©¾é©¶æ¨¡å‹ã€‚ä¸”å¹¶æŒç»­æ›´æ–°ï¼ŒåŒ…æ‹¬ä¸€äº›æŒ‘æˆ˜ã€ç›¸å…³è§†é¢‘ï¼ŒåŒ…æ‹¬æœºå™¨äººé¢†åŸŸçš„ä¸–ç•Œæ¨¡å‹ä½¿ç”¨ï¼ˆå¤§å¤šæ•°ä¸ºæ¨¡ä»¿å­¦ä¹ å¼ºåŒ–å­¦ä¹ æ–¹å‘ï¼‰å¯å‚è€ƒå€Ÿé‰´ã€‚")]),p,g,e("p",null,[o("[1] ä¸–ç•Œæ¨¡å‹ç®€ä»‹ï¼š"),e("a",m,[o("https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw"),n(r)]),o("ä¸–ç•Œæ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤šç§æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¦‚è§†è§‰ã€å¬è§‰å’Œè¯­è¨€ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰æ–¹æ³•æ¥ç†è§£å’Œé¢„æµ‹ç°å®ä¸–ç•Œã€‚å®ƒåŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—ã€è¡¨å¾å­¦ä¹ ã€åŠ¨åŠ›å­¦æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºæ„å»ºç¯å¢ƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œä¸ä»…èƒ½åæ˜ å½“å‰çŠ¶æ€ï¼Œè¿˜èƒ½é¢„æµ‹æœªæ¥å˜åŒ–ã€‚è¿™ç§æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆå¼€å‘å’Œæœºå™¨äººå­¦ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚Yann LeCunæå‡ºçš„è¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒé€šè¿‡è‡ªç›‘ç£å­¦ä¹ è®©AIåƒäººä¸€æ ·ç†è§£ä¸–ç•Œï¼Œå½¢æˆå†…éƒ¨çš„å¿ƒç†è¡¨å¾ï¼Œä»¥æœŸå®ç°é€šç”¨äººå·¥æ™ºèƒ½ã€‚Metaçš„I-JEPAæ¨¡å‹æ˜¯åŸºäºè¿™ä¸€æ„¿æ™¯çš„å®ç°ï¼Œå®ƒé€šè¿‡åˆ†æå’Œè¡¥å…¨å›¾åƒå±•ç¤ºäº†å¯¹ä¸–ç•ŒèƒŒæ™¯çŸ¥è¯†çš„åº”ç”¨ã€‚")]),e("p",null,[o("[2] å½±å“è¾ƒå¤§çš„æ—©æœŸä¸–ç•Œæ¨¡å‹æ–‡ç« ï¼š2018å¹´Jurgenåœ¨NeurIPS ä»¥å¾ªç¯ä¸–ç•Œæ¨¡å‹ä¿ƒè¿›ç­–ç•¥æ¼”å˜â€œRecurrent World Models Facilitate Policy Evolutionâ€çš„titleå‘è¡¨ï¼šé“¾æ¥: "),e("a",v,[o("https://arxiv.org/abs/1803.10122"),n(r)]),o(" ç¤ºä¾‹: "),e("a",b,[o("https://worldmodels.github.io/"),n(r)])]),e("p",null,[o("[3] ä¸–ç•Œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ï¼š "),e("a",f,[o("https://www.bilibili.com/read/cv34465959/"),n(r)])]),e("p",null,[o("[4] ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆä»¥åŠä»¿çœŸå¹³å°: "),e("a",_,[o("https://blog.csdn.net/CV_Autobot/article/details/134002647"),n(r)])]),A,e("ul",null,[e("li",null,[e("p",null,[o("[1] 2024-Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyondï¼Œ "),w,o(),e("a",D,[o("Paper"),n(r)]),o(" æä½³ç§‘æŠ€ ï¼ˆæ¯”è¾ƒå…¨é¢ï¼Œè¯¥ç»¼è¿°é€šè¿‡ 260 ä½™ç¯‡æ–‡çŒ®ï¼Œå¯¹ä¸–ç•Œæ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ä½“ã€é€šç”¨æœºå™¨äººç­‰é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨è¿›è¡Œäº†è¯¦å°½çš„åˆ†æå’Œè®¨è®ºã€‚å¦å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å®¡è§†äº†å½“å‰ä¸–ç•Œæ¨¡å‹çš„æŒ‘æˆ˜å’Œå±€é™æ€§ï¼Œå¹¶å±•æœ›äº†å®ƒä»¬æœªæ¥çš„å‘å±•æ–¹å‘ã€‚ï¼‰")])]),e("li",null,[e("p",null,[o("[2] 2024-World Models for Autonomous Driving: An Initial Surveyï¼ŒIEEE TIV,æ¾³é—¨å¤§å­¦ï¼Œå¤å¨å¤·å¤§å­¦ã€‚"),e("a",y,[o("Paper"),n(r)]),o("ï¼ˆç”»é£æœ‰è¶£ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„ç°çŠ¶å’Œæœªæ¥è¿›å±•è¿›è¡Œäº†åˆæ­¥å›é¡¾ï¼Œæ¶µç›–äº†å®ƒä»¬çš„ç†è®ºåŸºç¡€ã€å®é™…åº”ç”¨ä»¥åŠæ—¨åœ¨å…‹æœç°æœ‰å±€é™æ€§çš„æ­£åœ¨è¿›è¡Œçš„ç ”ç©¶å·¥ä½œã€‚ï¼‰")])]),e("li",null,[e("p",null,[o("[3]2024-Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies "),k,o(),e("a",M,[o("Paper"),n(r)])])]),e("li",null,[e("p",null,[o("[4]2024-Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities "),C,o(),e("a",x,[o("Paper"),n(r)])])])]),V,e("ul",null,[e("li",null,[o("2024-1X World Model Challenge "),S,o(),e("a",P,[o("Link"),n(r)])]),W,e("li",null,[o("2024-CVPR Workshop, Foundation Models for Autonomous Systems, Challenges, Track 4: Predictive World Model "),T,o(),e("a",I,[o("Link"),n(r)])])]),L,e("ul",null,[e("li",null,[o("2023 "),E,o("; "),e("a",G,[o("Video"),n(r)])]),R,e("li",null,[o("2022-Neural World Models for Autonomous Driving "),e("a",N,[o("Video"),n(r)])])]),F,e("h3",O,[e("a",U,[e("span",null,[o("â–  ä¸Šæµ·AILabï¼ˆä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼‰ "),e("a",B,[o("https://opendrivelab.com/publications/"),n(r)])])])]),e("h3",z,[e("a",q,[e("span",null,[o("â–  é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆé™ˆé“ è€å¸ˆå›¢é˜Ÿï¼‰Geometric-Controllable Visual Generation: A Systematic Solution "),e("a",j,[o("Video"),n(r)])])])]),e("h3",K,[e("a",H,[e("span",null,[o("â–  æä½³ç§‘æŠ€ï¼ˆæä½³ç§‘æŠ€DriveDreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€WorldDreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚ï¼‰"),e("a",J,[o("æ¨æ–‡"),n(r)])])])]),X,e("h3",Z,[e("a",Y,[e("span",null,[o("â–  è”šæ¥è½¦ä¼ï¼š"),e("a",Q,[o("https://www.qbitai.com/2024/07/172025.html"),n(r)])])])]),$,e("ul",null,[e("li",null,[o("(2023 Arxiv) GAIA-1: A generative world model for autonomous driving ["),e("a",ee,[o("Paper"),n(r)]),o("]["),e("a",oe,[o("Blog"),n(r)]),o("] (Wayve)")]),e("li",null,[o("(2023 CVPR 2023 workshop) ["),e("a",re,[o("Video"),n(r)]),o("] (Tesla)")]),e("li",null,[o("(2023 Arxiv) DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving ["),e("a",ne,[o("Paper"),n(r)]),o("]["),e("a",te,[o("Code"),n(r)]),o("] (GigaAI)")]),e("li",null,[o("(2023 Arxiv) ADriver-I: A General World Model for Autonomous Driving ["),e("a",ae,[o("Paper"),n(r)]),o("] (MEGVII)")]),e("li",null,[o("(2023 Arxiv) DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model ["),e("a",ie,[o("Paper"),n(r)]),o("] (Baidu)")]),e("li",null,[o("(2023 Arxiv) Panacea: Panoramic and Controllable Video Generation for Autonomous Driving ["),e("a",le,[o("Paper"),n(r)]),o("]["),e("a",se,[o("Code"),n(r)]),o("] (MEGVII)")]),e("li",null,[o("(2024 CVPR) Drive-WM: Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving ["),e("a",de,[o("Paper"),n(r)]),o("]["),e("a",he,[o("Code"),n(r)]),o("] (CASIA)")]),e("li",null,[o("(2023 Arxiv) WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation ["),e("a",ce,[o("Paper"),n(r)]),o("] (Fudan)")]),e("li",null,[o("(2024 Arxiv) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation ["),e("a",ue,[o("Paper"),n(r)]),o("]["),e("a",pe,[o("Code"),n(r)]),o("] (GigaAI)")]),e("li",null,[o("(2024 CVPR) GenAD: Generalized Predictive Model for Autonomous Driving ["),e("a",ge,[o("Paper"),n(r)]),o("]["),e("a",me,[o("Code"),n(r)]),o("] (Shanghai AI Lab)")]),e("li",null,[o("(2024 Arxiv) SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control ["),e("a",ve,[o("Paper"),n(r)]),o("] (MEGVII)")])]),be,e("ul",null,[e("li",null,[o("(2024 ICLR) Copilot4D:Learning unsupervised world models for autonomous driving via discrete diffusion ["),e("a",fe,[o("Paper"),n(r)]),o("] (Waabi)")]),e("li",null,[o("(2023 Arxiv) OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving ["),e("a",_e,[o("Paper"),n(r)]),o("]["),e("a",Ae,[o("Code"),n(r)]),o("] (THU)")]),e("li",null,[o("(2023 Arxiv) MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations ["),e("a",we,[o("Paper"),n(r)]),o("] (KIT)")]),e("li",null,[o("(2024 Arxiv) LidarDM: Generative LiDAR Simulation in a Generated World ["),e("a",De,[o("Paper"),n(r)]),o("]["),e("a",ye,[o("Code"),n(r)]),o("] (MIT)")])]),ke,e("ul",null,[e("li",null,[o("(2023 Arxiv) UniWorld: Autonomous Driving Pre-training via World Models ["),e("a",Me,[o("Paper"),n(r)]),o("] (PKU)")]),e("li",null,[o("(2024 CVPR) ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving ["),e("a",Ce,[o("Paper"),n(r)]),o("]["),e("a",xe,[o("Code"),n(r)]),o("] (Shanghai AI Lab)")]),Ve]),Se,e("ul",null,[e("li",null,[o("(2022 NeurIPS) Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models ["),e("a",Pe,[o("Paper"),n(r)]),o("] (SJTU)")]),e("li",null,[o("(2022 NeurIPS) MILE: Model-Based Imitation Learning for Urban Driving ["),e("a",We,[o("Paper"),n(r)]),o("]["),e("a",Te,[o("Code"),n(r)]),o("] (Wayve)")]),e("li",null,[o("(2022 NeurIPS Deep RL Workshop) SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model ["),e("a",Ie,[o("Paper"),n(r)]),o("] (HIT & THU)")]),e("li",null,[o("(2023 ICRA) TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction ["),e("a",Le,[o("Paper"),n(r)]),o("] (ETH Zurich)")]),e("li",null,[o("(2024 Arxiv) Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2) ["),e("a",Ee,[o("Paper"),n(r)]),o("] (SJTU)")])]),Ge,e("ul",null,[e("li",null,[o("(1989) Using Occupancy Grids for Mobile Robot Perception and Navigation ["),e("a",Re,[o("paper"),n(r)]),o("]")])]),Ne,e("p",null,[o("â—‹ MNAD --> "),e("a",Fe,[o("https://github.com/cvlab-yonsei/MNAD"),n(r)]),o(" å¯ä½œä¸ºbaseline.")]),Oe,e("p",null,[o("[1] Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge (ICCV 2017)"),Ue,o(" [2] X-MAN: Explaining multiple sources of anomalies in video (CVPR workshop 2021)"),Be,o(" [3] Discrete neural representations for explainable anomaly detection (WACV 2022) "),e("a",ze,[o("Awesome-World-Models-for-AD "),n(r)]),o(", "),e("a",qe,[o("World models paper list from Shanghai AI lab"),n(r)]),o(", "),e("a",je,[o("Awesome-Papers-World-Models-Autonomous-Driving"),n(r)]),o(".")])])}const Ye=a(d,[["render",Ke],["__file","generation.html.vue"]]),Qe=JSON.parse('{"path":"/wordmodel/generation.html","title":"World-Models-Autonomous-Driving-Latest-Survey","lang":"en-US","frontmatter":{"description":"World-Models-Autonomous-Driving-Latest-Survey A curated list of world model for autonmous driving. Keep updated. ğŸ“Œ Introduction âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç† â¢ è®ºæ–‡æ±‡æ€» [1] https://github...","head":[["meta",{"property":"og:url","content":"https://openvisuallab.github.io/wordmodel/generation.html"}],["meta",{"property":"og:site_name","content":"OpenVisualLab"}],["meta",{"property":"og:title","content":"World-Models-Autonomous-Driving-Latest-Survey"}],["meta",{"property":"og:description","content":"World-Models-Autonomous-Driving-Latest-Survey A curated list of world model for autonmous driving. Keep updated. ğŸ“Œ Introduction âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç† â¢ è®ºæ–‡æ±‡æ€» [1] https://github..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-09-13T02:15:52.000Z"}],["meta",{"property":"article:author","content":"OpenVisualLab"}],["meta",{"property":"article:modified_time","content":"2024-09-13T02:15:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"World-Models-Autonomous-Driving-Latest-Survey\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-09-13T02:15:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"OpenVisualLab\\",\\"url\\":\\"https://openvisuallab.github.io\\"}]}"]]},"headers":[{"level":2,"title":"ğŸ“Œ Introduction","slug":"ğŸ“Œ-introduction","link":"#ğŸ“Œ-introduction","children":[]},{"level":2,"title":"âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†","slug":"âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†","link":"#âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†","children":[]},{"level":2,"title":"â¢ è®ºæ–‡æ±‡æ€»","slug":"â¢-è®ºæ–‡æ±‡æ€»","link":"#â¢-è®ºæ–‡æ±‡æ€»","children":[]},{"level":2,"title":"â¢ è®¤è¯†ä¸–ç•Œæ¨¡å‹","slug":"â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹","link":"#â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹","children":[{"level":3,"title":"1. ç®€å•ä»‹ç»ï¼ˆä»ä¸–ç•Œæ¨¡å‹--> è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆï¼‰","slug":"_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ","link":"#_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ","children":[]},{"level":3,"title":"2.è®ºæ–‡ç»¼è¿°","slug":"_2-è®ºæ–‡ç»¼è¿°","link":"#_2-è®ºæ–‡ç»¼è¿°","children":[]}]},{"level":2,"title":"3.æŒ‘æˆ˜èµ› Workshops/Challenges","slug":"_3-æŒ‘æˆ˜èµ›-workshops-challenges","link":"#_3-æŒ‘æˆ˜èµ›-workshops-challenges","children":[]},{"level":2,"title":"Tutorials/Talks/","slug":"tutorials-talks","link":"#tutorials-talks","children":[]},{"level":2,"title":"â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬/ å…¬å¸","slug":"â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸","link":"#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸","children":[{"level":3,"title":"â–   ä¸Šæµ·AILabï¼ˆä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼‰ https://opendrivelab.com/publications/","slug":"â– -ä¸Šæµ·ailab-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤-https-opendrivelab-com-publications","link":"#â– -ä¸Šæµ·ailab-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤-https-opendrivelab-com-publications","children":[]},{"level":3,"title":"â–   é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆé™ˆé“ è€å¸ˆå›¢é˜Ÿï¼‰Geometric-Controllable Visual Generation: A Systematic Solution  Video","slug":"â– -é¦™æ¸¯ä¸­æ–‡å¤§å­¦-é™ˆé“ è€å¸ˆå›¢é˜Ÿ-geometric-controllable-visual-generation-a-systematic-solution-video","link":"#â– -é¦™æ¸¯ä¸­æ–‡å¤§å­¦-é™ˆé“ è€å¸ˆå›¢é˜Ÿ-geometric-controllable-visual-generation-a-systematic-solution-video","children":[]},{"level":3,"title":"â–   æä½³ç§‘æŠ€ï¼ˆæä½³ç§‘æŠ€DriveDreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€WorldDreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚ï¼‰æ¨æ–‡","slug":"â– -æä½³ç§‘æŠ€-æä½³ç§‘æŠ€drivedreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€worlddreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚-æ¨æ–‡","link":"#â– -æä½³ç§‘æŠ€-æä½³ç§‘æŠ€drivedreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€worlddreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚-æ¨æ–‡","children":[]},{"level":3,"title":"â–   Wayveã€Teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€","slug":"â– -wayveã€teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€","link":"#â– -wayveã€teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€","children":[]},{"level":3,"title":"â–  è”šæ¥è½¦ä¼ï¼šhttps://www.qbitai.com/2024/07/172025.html","slug":"â– -è”šæ¥è½¦ä¼-https-www-qbitai-com-2024-07-172025-html","link":"#â– -è”šæ¥è½¦ä¼-https-www-qbitai-com-2024-07-172025-html","children":[]}]},{"level":2,"title":"â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰","slug":"â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘","link":"#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘","children":[{"level":3,"title":"Neural Driving Simulator based on World Models","slug":"neural-driving-simulator-based-on-world-models","link":"#neural-driving-simulator-based-on-world-models","children":[]},{"level":3,"title":"End-to-end Driving based on World Models","slug":"end-to-end-driving-based-on-world-models","link":"#end-to-end-driving-based-on-world-models","children":[]},{"level":3,"title":"Others","slug":"others","link":"#others","children":[]},{"level":3,"title":"â–  Unsupervised VAD","slug":"â– -unsupervised-vad","link":"#â– -unsupervised-vad","children":[]},{"level":3,"title":"â–  Weakly supervised VAD","slug":"â– -weakly-supervised-vad","link":"#â– -weakly-supervised-vad","children":[]}]},{"level":2,"title":"â¢ ç»å…¸é¡¹ç›®","slug":"â¢-ç»å…¸é¡¹ç›®","link":"#â¢-ç»å…¸é¡¹ç›®","children":[]},{"level":2,"title":"â¢ å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘-->","slug":"â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘","link":"#â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘","children":[{"level":3,"title":"1. DEFINITION","slug":"_1-definition","link":"#_1-definition","children":[]},{"level":3,"title":"2. RELATED WORK","slug":"_2-related-work","link":"#_2-related-work","children":[]}]}],"git":{"createdTime":1726102486000,"updatedTime":1726193752000,"contributors":[{"name":"mingzhuzhang1","email":"145547769+mingzhuzhang1@users.noreply.github.com","commits":11}]},"readingTime":{"minutes":7.62,"words":2286},"filePathRelative":"wordmodel/generation.md","localizedDate":"September 12, 2024","excerpt":"\\n<p>A curated list of world model for autonmous driving. Keep updated.</p>\\n<h2>ğŸ“Œ Introduction</h2>\\n<h2>âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†</h2>\\n<h2>â¢ è®ºæ–‡æ±‡æ€»</h2>\\n<p>[1] <a href=\\"https://github.com/GigaAI-research/General-World-Models-Survey\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/GigaAI-research/General-World-Models-Survey</a> è¯¥ repo å†…æœ‰ç›®å‰ä¸–ç•Œæ¨¡å‹æ–¹å‘çš„ä¼˜ç§€è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ï¼šè§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶å’Œè‡ªä¸»ä»£ç†ã€‚å…¶ä¸­è‡ªåŠ¨é©¾é©¶åˆ†æˆç«¯åˆ°ç«¯ã€ä»¥åŠ2Dã€3Dç¥ç»æ¨¡æ‹Ÿå™¨æ–¹æ³•ã€‚ä¸–ç•Œæ¨¡å‹çš„æ–‡çŒ®ã€ å¼€æºcodeã€ ç»¼è¿°ã€‚</p>","autoDesc":true}');export{Ye as comp,Qe as data};
