import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as l,o,c as i,a,b as s,d as e}from"./app-MVVvdJ3R.js";const r="/tinyweekly/figs/0106_ECoT.png",c={},h=a("p",null,"Robotic Control via Embodied Chain-of-Thought Reasoning",-1),m=a("p",null,"分享人: Yang Shedan (杨社丹)",-1),p=a("h2",{id:"收获总结",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#收获总结"},[a("span",null,"收获总结")])],-1),d=a("p",null,"1、这篇论文是主要思想是：现有VLA模型通常直接从输入的环境观察映射得到机器人动作，但这种方式缺乏了中间推理；该论文提出在VLA模型预测机器人动作之前，先对任务和环境状态进行推理，推理之后再预测动作，这是一种让人觉得很合理且很容易接受的思路：“三思而后行”。该论文思考问题的角度很值得借鉴，在自己做研究时也要考虑解决什么问题，解决该问题的必要性；",-1),_=a("p",null,"2、分享论文时，要按照研究动机-研究方法-实验结果-总结的思路进行分享，这样可以让观众思路更清晰，更容易听懂；",-1),u=a("p",null,"3、该论文中对环境中的物体信息进行了推理，通过目标检测和分割的方法，获得物体边界框、物体名字等信息，然后再利用这些信息去预测机器人动作，这点也可以借鉴到自己的工作当中，可以利用这些信息辅助子任务的分解，子任务分解时要注意环境中的所有物体信息，而不是只考虑指令中提到的物体信息。",-1),g=a("h2",{id:"分享摘要",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#分享摘要"},[a("span",null,"分享摘要")])],-1),x={href:"https://meeting.tencent.com/crm/2qdpyqRd15",target:"_blank",rel:"noopener noreferrer"},w=a("a",{href:"/tinyweekly/papers/(ECoT)2407.08693v2.pdf"},"论文本地连接",-1),k={href:"https://arxiv.org/abs/2407.08693",target:"_blank",rel:"noopener noreferrer"},f={href:"https://github.com/MichalZawalski/embodied-CoT",target:"_blank",rel:"noopener noreferrer"},C=a("figure",null,[a("img",{src:r,alt:"alt text",tabindex:"0",loading:"lazy"}),a("figcaption",null,"alt text")],-1),L=a("p",null,"分享摘要： 本论文针对VLA直接从环境观察映射到机器人动作，缺少对环境和机械臂状态的推理，难以应对新环境的问题；受到应用COT激活VLM模型的强大推理能力的启发，提出了具身思维链推理 (ECoT)，训练VLA在预测机器人动作之前，对规划、子任务、运动以及基于视觉的特征（如物体边界框和末端执行器位置）执行多步推理，然后再预测机器人动作。此外，在现有大型机器人数据集上为ECoT生成训练数据。与现有VLA方法相比，实验性能表现较好。",-1),T=a("p",null,"论文摘要：学习机器人控制策略的一个关键限制是它们无法在训练数据之外进行推广。最近关于视觉-语言-动作模型 (VLA) 的研究表明，使用大型、互联网预训练的视觉语言模型作为学习机器人策略的骨干可以显著提高其稳健性和泛化能力。然而，大型视觉语言模型在其他领域最令人兴奋的能力之一是它们能够迭代推理复杂问题。是否可以将同样的能力引入机器人技术，让策略通过在行动之前推理给定任务来提高性能？由于可用的训练示例相对简单，因此在标准VLA中，单纯使用“思维链”(CoT)式提示的效果会大大降低。此外，对于需要将其推理建立在感官观察和机器人状态基础上的机器人策略而言，常规CoT中常见的关于子任务的纯语义推理是不够的。为此，我们为VLA引入了具身思维链推理(ECoT)，在预测机器人动作之前，我们训练VLA对计划、子任务、动作和视觉基础特征（如物体边界框和末端执行器位置）执行多步推理。我们设计了一个可扩展的流程，用于在大型机器人数据集上为ECoT生成合成训练数据。我们证明，ECoT在具有挑战性的泛化任务中将当前最强大的开源VLA策略 OpenVLA 的绝对成功率提高了28%，而无需任何额外的机器人训练数据。此外，ECoT使人类更容易解释策略的失败并以自然语言交互方式纠正其行为。 最后，我们表明我们的模型学会了将ECoT推理转移到看不见的体现和任务上。",-1),A=a("hr",null,null,-1),b=a("h2",{id:"问答简记",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#问答简记"},[a("span",null,"问答简记")])],-1),y=a("p",null,[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"Q"),a("mo",null,":")]),a("annotation",{encoding:"application/x-tex"},"Q:")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal"},"Q"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},":")])])]),s(" 实验结果中提到如果推理结果中物体的检测结果不正确会影响最终的机器人动作，这是不是很依赖于目标检测的方法？")],-1),M=a("p",null,[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"A"),a("mo",null,":")]),a("annotation",{encoding:"application/x-tex"},"A:")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6833em"}}),a("span",{class:"mord mathnormal"},"A"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},":")])])]),s(" 目标检测的结果确实会影响最终的机器人动作预测结果，但是现在的一些实验中机械臂操作环境相对比较简单且清晰，对目标检测和分割方法能力的要求不是特别高，论文中采用的Grounding Dino和SAM这些方法能力都是比较强的，一般检测结果正确的还是居多。此外，除了要获取视觉信息外，也会推理子任务分解，机械臂移动方向等这些信息，用于机器人动作预测，所以机器人动作的好坏还是有很多因素影响，不单单是目标检测的结果。")],-1),E=a("hr",null,null,-1),V=a("p",null,[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"Q"),a("mo",null,":")]),a("annotation",{encoding:"application/x-tex"},"Q:")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal"},"Q"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},":")])])]),s(" 这篇论文有没有展示直接在现有的大模型上预测的实验结果，就是不用微调的操作，而是直接让模型去预测机器人动作？")],-1),v=a("p",null,[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"A"),a("mo",null,":")]),a("annotation",{encoding:"application/x-tex"},"A:")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6833em"}}),a("span",{class:"mord mathnormal"},"A"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},":")])])]),s(" 论文中并没有展示这样的实验结果，现有的VLA模型都是会在对应的机器人数据集上训练的。这篇论文是基于OpenVLA模型做的，它也是自己合成了对应的数据集，然后再OpenVLA基础上再训练的。这些数据是基于现有的机器人数据集合成的，就是加上了论文中的推理步骤的信息，合成观察-推理-动作的三元组数据，用这些数据训练ECoT模型。这个数据的合成工作量还是挺大的，因为中间推理的信息很多。")],-1),S=a("hr",null,null,-1),Q=a("p",null,[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"Q"),a("mo",null,":")]),a("annotation",{encoding:"application/x-tex"},"Q:")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),a("span",{class:"mord mathnormal"},"Q"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},":")])])]),s(" 实验是在真实环境上做的，你自己的工作是在仿真环境上做的，是不是在仿真环境上实验结果会好一些，因为仿真是相当理想的状态？")],-1),R=a("p",null,[a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mi",null,"A"),a("mo",null,":")]),a("annotation",{encoding:"application/x-tex"},"A:")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"0.6833em"}}),a("span",{class:"mord mathnormal"},"A"),a("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),a("span",{class:"mrel"},":")])])]),s(" 这篇论文的实验是在真实环境上做的，但是不一定在仿真环境结果会好，因为这个方法是要通过数据训练的，在仿真环境验证性能的话就是用仿真数据进行训练，在真实环境验证性能的话就是用真实环境的数据进行训练，还是看模型的训练结果，不一定在哪个环境下结果会更好。而我自己现在做的工作是training-free的方法，不经过训练，直接推理。这种情况下，可能就是仿真环境下结果会更好一些，因为环境是理想状态，物体的位置这些信息都是真值；而真实环境的话可能会很多因素的影响，比如光线、摄像机性能等，还要采用一些目标检测方法，这个结果也会带来一定影响。")],-1),B=a("h2",{id:"相关论文",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#相关论文"},[a("span",null,"相关论文")])],-1),O=a("p",null,"1、M. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster,P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, andC. Finn. Openvla: An open-source vision-language-action model. 2024. (OpenVLA论文) 2、J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou.Chain-of-thought prompting elicits reasoning in large language models, 2023.（CoT论文）",-1);function D(N,K){const n=l("ExternalLinkIcon");return o(),i("div",null,[h,m,p,d,_,u,g,a("p",null,[a("a",x,[s("腾讯会议链接"),e(n)]),s(),w]),a("p",null,[a("a",k,[s("[paper]"),e(n)]),s(),a("a",f,[s("[github🌟12]"),e(n)])]),C,L,T,A,b,y,M,E,V,v,S,Q,R,B,O])}const I=t(c,[["render",D],["__file","250106_ECoT_CoRL24_ysd.html.vue"]]),X=JSON.parse('{"path":"/browser/epoch_two_24_1209_25_0120/250106_ECoT_CoRL24_ysd.html","title":"ECoT (Yang Shedan)","lang":"en-US","frontmatter":{"title":"ECoT (Yang Shedan)","order":11,"feed":false,"seo":false,"head":[]},"headers":[{"level":2,"title":"收获总结","slug":"收获总结","link":"#收获总结","children":[]},{"level":2,"title":"分享摘要","slug":"分享摘要","link":"#分享摘要","children":[]},{"level":2,"title":"问答简记","slug":"问答简记","link":"#问答简记","children":[]},{"level":2,"title":"相关论文","slug":"相关论文","link":"#相关论文","children":[]}],"git":{"createdTime":1734448211000,"updatedTime":1736254422000,"contributors":[{"name":"ysd1111","email":"94695456+ysd1111@users.noreply.github.com","commits":3}]},"readingTime":{"minutes":6.25,"words":1876},"filePathRelative":"browser/epoch_two_24_1209_25_0120/250106_ECoT_CoRL24_ysd.md","localizedDate":"December 17, 2024"}');export{I as comp,X as data};
