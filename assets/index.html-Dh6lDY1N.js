import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o as s,c as d,a as e,b as r,d as n,e as t,f as l}from"./app-BjHBoYTU.js";const h="/imgs/archiver/world_model/world_model.png",c="/imgs/archiver/world_model/End%20to%20end.png",u="/imgs/archiver/world_model/Generation.png",p="/imgs/archiver/world_model/ad.png",_={},g=l('<blockquote><p>World-Models-Autonomous-Driving-Latest-Survey</p></blockquote><p>A curated list of world model for autonmous driving. Keep updated.</p><h2 id="📌-introduction" tabindex="-1"><a class="header-anchor" href="#📌-introduction"><span>📌 Introduction</span></a></h2><h2 id="✧-世界模型用于自动驾驶场景生成相关文献整理" tabindex="-1"><a class="header-anchor" href="#✧-世界模型用于自动驾驶场景生成相关文献整理"><span>✧ 世界模型用于自动驾驶场景生成相关文献整理</span></a></h2><h2 id="➢-论文汇总" tabindex="-1"><a class="header-anchor" href="#➢-论文汇总"><span>➢ 论文汇总</span></a></h2>',5),f={href:"https://github.com/GigaAI-research/General-World-Models-Survey",target:"_blank",rel:"noopener noreferrer"},m={href:"https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey",target:"_blank",rel:"noopener noreferrer"},v={href:"https://github.com/zhanghm1995/awesome-world-models-for-AD?tab=readme-ov-file#Table-of-Content",target:"_blank",rel:"noopener noreferrer"},b={href:"https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving/blob/main/papers.md#world-model--model-based-rl",target:"_blank",rel:"noopener noreferrer"},k={href:"https://github.com/chaytonmin/Awesome-Papers-World-Models-Autonomous-Driving",target:"_blank",rel:"noopener noreferrer"},w=e("h2",{id:"➢-认识世界模型",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#➢-认识世界模型"},[e("span",null,"➢ 认识世界模型")])],-1),D=e("figure",null,[e("img",{src:h,alt:"world model",tabindex:"0",loading:"lazy"}),e("figcaption",null,"world model")],-1),P=e("h3",{id:"_1-简单介绍-从世界模型-自动驾驶世界模型用于场景生成",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-简单介绍-从世界模型-自动驾驶世界模型用于场景生成"},[e("span",null,"1. 简单介绍（从世界模型--> 自动驾驶世界模型用于场景生成）")])],-1),A={href:"https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw",target:"_blank",rel:"noopener noreferrer"},x={href:"https://arxiv.org/abs/1803.10122",target:"_blank",rel:"noopener noreferrer"},M={href:"https://worldmodels.github.io/",target:"_blank",rel:"noopener noreferrer"},y={href:"https://www.bilibili.com/read/cv34465959/",target:"_blank",rel:"noopener noreferrer"},W={href:"https://blog.csdn.net/CV_Autobot/article/details/134002647",target:"_blank",rel:"noopener noreferrer"},C=e("h3",{id:"_2-论文综述",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-论文综述"},[e("span",null,"2.论文综述")])],-1),L=e("strong",null,[e("code",null,"arxiv")],-1),I={href:"https://arxiv.org/abs/2403.02622",target:"_blank",rel:"noopener noreferrer"},S={href:"https://arxiv.org/abs/2403.02622",target:"_blank",rel:"noopener noreferrer"},G=e("strong",null,[e("code",null,"arxiv")],-1),V={href:"https://arxiv.org/pdf/2401.12888.pdf",target:"_blank",rel:"noopener noreferrer"},R=e("strong",null,[e("code",null,"arxiv")],-1),E={href:"https://arxiv.org/pdf/2401.08045.pdf",target:"_blank",rel:"noopener noreferrer"},T=e("h2",{id:"_3-挑战赛-workshops-challenges",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3-挑战赛-workshops-challenges"},[e("span",null,"3.挑战赛 Workshops/Challenges")])],-1),N=e("strong",null,[e("code",null,"Challenges")],-1),O={href:"https://github.com/1x-technologies/1xgpt",target:"_blank",rel:"noopener noreferrer"},z=e("strong",null,[e("code",null,"Challenges")],-1),F={href:"https://github.com/1x-technologies/1xgpt",target:"_blank",rel:"noopener noreferrer"},U=e("strong",null,[e("code",null,"Challenges")],-1),B={href:"https://opendrivelab.com/challenge2024/",target:"_blank",rel:"noopener noreferrer"},j=e("h2",{id:"tutorials-talks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tutorials-talks"},[e("span",null,"Tutorials/Talks/")])],-1),q=e("strong",null,[e("code",null,"from Wayve")],-1),Y={href:"https://www.youtube.com/watch?v=lNOs08byOhw",target:"_blank",rel:"noopener noreferrer"},H={href:"https://www.youtube.com/watch?v=wMvYjiv6EpY",target:"_blank",rel:"noopener noreferrer"},J=e("h2",{id:"➢-优秀团队-学术大佬-公司",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#➢-优秀团队-学术大佬-公司"},[e("span",null,"➢ 优秀团队 / 学术大佬/ 公司")])],-1),Z={id:"■-上海ailab-上海人工智能实验室-https-opendrivelab-com-publications",tabindex:"-1"},K={class:"header-anchor",href:"#■-上海ailab-上海人工智能实验室-https-opendrivelab-com-publications"},X={href:"https://opendrivelab.com/publications/",target:"_blank",rel:"noopener noreferrer"},Q={id:"■-香港中文大学-陈铠老师团队-geometric-controllable-visual-generation-a-systematic-solution-video",tabindex:"-1"},$={class:"header-anchor",href:"#■-香港中文大学-陈铠老师团队-geometric-controllable-visual-generation-a-systematic-solution-video"},ee={href:"https://www.bilibili.com/video/BV18T421v7Nf/?spm_id_from=333.337.search-card.all.click",target:"_blank",rel:"noopener noreferrer"},re={id:"■-极佳科技-极佳科技drivedreamer自动驾驶世界模型、worlddreamer通用世界模型目前已成功商业化落地。-推文",tabindex:"-1"},oe={class:"header-anchor",href:"#■-极佳科技-极佳科技drivedreamer自动驾驶世界模型、worlddreamer通用世界模型目前已成功商业化落地。-推文"},ne={href:"https://baijiahao.baidu.com/s?id=1799624134723943641",target:"_blank",rel:"noopener noreferrer"},te=e("h4",{id:"■-wayve、tesla、旷视、中科院自动化所",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-wayve、tesla、旷视、中科院自动化所"},[e("span",null,"■ Wayve、Tesla、旷视、中科院自动化所")])],-1),le={id:"■-蔚来车企-https-www-qbitai-com-2024-07-172025-html",tabindex:"-1"},ae={class:"header-anchor",href:"#■-蔚来车企-https-www-qbitai-com-2024-07-172025-html"},ie={href:"https://www.qbitai.com/2024/07/172025.html",target:"_blank",rel:"noopener noreferrer"},se=l('<h2 id="➢-经典论文-推荐加-👍" tabindex="-1"><a class="header-anchor" href="#➢-经典论文-推荐加-👍"><span>➢ 经典论文：（推荐加“👍”）</span></a></h2><h4 id="world-models-are-adept-at-representing-an-agent-s-spatio-temporal-knowledge-about-its-environment-through-the-prediction-of-future-changes" tabindex="-1"><a class="header-anchor" href="#world-models-are-adept-at-representing-an-agent-s-spatio-temporal-knowledge-about-its-environment-through-the-prediction-of-future-changes"><span>+ World Models are adept at representing an agent&#39;s spatio-temporal knowledge about its environment through the prediction of future changes</span></a></h4><h4 id="there-are-two-main-types-of-world-models-in-autonomous-driving-aimed-at-reducing-driving-uncertainty-i-e-world-model-as-neural-driving-simulator-and-world-model-for-end-to-end-driving" tabindex="-1"><a class="header-anchor" href="#there-are-two-main-types-of-world-models-in-autonomous-driving-aimed-at-reducing-driving-uncertainty-i-e-world-model-as-neural-driving-simulator-and-world-model-for-end-to-end-driving"><span>+ There are two main types of world models in Autonomous Driving aimed at reducing driving uncertainty, i.e., World Model as Neural Driving Simulator and World Model for End-to-end Driving</span></a></h4>',3),de=l('<figure><img src="'+c+'" alt="world model" tabindex="0" loading="lazy"><figcaption>world model</figcaption></figure><h4 id="in-the-real-environment-methods-like-gaia-1-and-copilot4d-involve-utilizing-generative-models-to-construct-neural-simulators-that-produce-2d-or-3d-future-scenes-to-enhance-predictive-capabilities" tabindex="-1"><a class="header-anchor" href="#in-the-real-environment-methods-like-gaia-1-and-copilot4d-involve-utilizing-generative-models-to-construct-neural-simulators-that-produce-2d-or-3d-future-scenes-to-enhance-predictive-capabilities"><span>+ In the real environment, methods like GAIA-1 and Copilot4D involve utilizing generative models to construct neural simulators that produce 2D or 3D future scenes to enhance predictive capabilities</span></a></h4><h4 id="in-the-simulation-environment-methods-such-as-mile-and-trafficbots-are-based-on-reinforcement-learning-enhancing-their-capacity-for-decision-making-and-future-prediction-thereby-paving-the-way-to-end-to-end-autonomous-driving" tabindex="-1"><a class="header-anchor" href="#in-the-simulation-environment-methods-such-as-mile-and-trafficbots-are-based-on-reinforcement-learning-enhancing-their-capacity-for-decision-making-and-future-prediction-thereby-paving-the-way-to-end-to-end-autonomous-driving"><span>+ In the simulation environment, methods such as MILE and TrafficBots are based on reinforcement learning, enhancing their capacity for decision-making and future prediction, thereby paving the way to end-to-end autonomous driving</span></a></h4>',3),he=l('<figure><img src="'+u+'" alt="world model" tabindex="0" loading="lazy"><figcaption>world model</figcaption></figure><h3 id="■-neural-driving-simulator-based-on-world-models" tabindex="-1"><a class="header-anchor" href="#■-neural-driving-simulator-based-on-world-models"><span>■ Neural Driving Simulator based on World Models</span></a></h3><h4 id="■-2d-scene-generation" tabindex="-1"><a class="header-anchor" href="#■-2d-scene-generation"><span>■ 2D Scene Generation</span></a></h4>',3),ce={href:"https://arxiv.org/abs/2309.17080",target:"_blank",rel:"noopener noreferrer"},ue={href:"https://wayve.ai/thinking/scaling-gaia-1/",target:"_blank",rel:"noopener noreferrer"},pe={href:"https://www.youtube.com/watch?v=6x-Xb_uT7ts",target:"_blank",rel:"noopener noreferrer"},_e={href:"https://drivedreamer.github.io/",target:"_blank",rel:"noopener noreferrer"},ge={href:"https://github.com/JeffWang987/DriveDreamer",target:"_blank",rel:"noopener noreferrer"},fe={href:"https://arxiv.org/abs/2311.13549",target:"_blank",rel:"noopener noreferrer"},me={href:"https://arxiv.org/abs/2310.07771",target:"_blank",rel:"noopener noreferrer"},ve={href:"https://panacea-ad.github.io/",target:"_blank",rel:"noopener noreferrer"},be={href:"https://github.com/wenyuqing/panacea",target:"_blank",rel:"noopener noreferrer"},ke={href:"https://drive-wm.github.io/",target:"_blank",rel:"noopener noreferrer"},we={href:"https://github.com/BraveGroup/Drive-WM",target:"_blank",rel:"noopener noreferrer"},De={href:"https://arxiv.org/abs/2312.02934",target:"_blank",rel:"noopener noreferrer"},Pe={href:"https://drivedreamer2.github.io/",target:"_blank",rel:"noopener noreferrer"},Ae={href:"https://github.com/f1yfisher/DriveDreamer2",target:"_blank",rel:"noopener noreferrer"},xe={href:"https://arxiv.org/abs/2403.09630",target:"_blank",rel:"noopener noreferrer"},Me={href:"https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file",target:"_blank",rel:"noopener noreferrer"},ye={href:"https://subjectdrive.github.io/",target:"_blank",rel:"noopener noreferrer"},We=e("h4",{id:"■-3d-scene-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-3d-scene-generation"},[e("span",null,"■ 3D Scene Generation")])],-1),Ce={href:"https://arxiv.org/abs/2311.01017",target:"_blank",rel:"noopener noreferrer"},Le={href:"https://arxiv.org/abs/2311.16038",target:"_blank",rel:"noopener noreferrer"},Ie={href:"https://github.com/wzzheng/OccWorld",target:"_blank",rel:"noopener noreferrer"},Se={href:"https://arxiv.org/abs/2311.11762",target:"_blank",rel:"noopener noreferrer"},Ge={href:"https://www.zyrianov.org/lidardm/",target:"_blank",rel:"noopener noreferrer"},Ve={href:"https://github.com/vzyrianov/lidardm",target:"_blank",rel:"noopener noreferrer"},Re=e("h4",{id:"■-4d-pre-training-for-autonomous-driving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-4d-pre-training-for-autonomous-driving"},[e("span",null,"■ 4D Pre-training for Autonomous Driving")])],-1),Ee={href:"https://arxiv.org/abs/2312.17655",target:"_blank",rel:"noopener noreferrer"},Te={href:"https://github.com/OpenDriveLab/ViDAR",target:"_blank",rel:"noopener noreferrer"},Ne=e("li",null,[r("👍(2024 CVPR) DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving ["),e("a",{href:"XXX"},"Paper"),r("] (PKU)")],-1),Oe=e("h3",{id:"■-end-to-end-driving-based-on-world-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-end-to-end-driving-based-on-world-models"},[e("span",null,"■ End-to-end Driving based on World Models")])],-1),ze={href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"},Fe={href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"},Ue={href:"https://github.com/wayveai/mile",target:"_blank",rel:"noopener noreferrer"},Be={href:"https://arxiv.org/abs/2210.04017",target:"_blank",rel:"noopener noreferrer"},je={href:"https://ieeexplore.ieee.org/abstract/document/10161243",target:"_blank",rel:"noopener noreferrer"},qe={href:"https://arxiv.org/abs/2402.16720",target:"_blank",rel:"noopener noreferrer"},Ye=l('<figure><img src="'+p+'" alt="world model" tabindex="0" loading="lazy"><figcaption>world model</figcaption></figure><h3 id="■-按时间顺序更新" tabindex="-1"><a class="header-anchor" href="#■-按时间顺序更新"><span>■ 按时间顺序更新</span></a></h3><h2 id="papers" tabindex="-1"><a class="header-anchor" href="#papers"><span>Papers</span></a></h2>',3),He={href:"https://arxiv.org/abs/2410.10738",target:"_blank",rel:"noopener noreferrer"},Je=e("strong",null,[e("code",null,"Dataset")],-1),Ze={href:"https://arxiv.org/abs/2409.16663",target:"_blank",rel:"noopener noreferrer"},Ke=e("strong",null,[e("code",null,"Planning")],-1),Xe={href:"https://www.arxiv.org/abs/2409.03272",target:"_blank",rel:"noopener noreferrer"},Qe={href:"https://arxiv.org/pdf/2408.14197",target:"_blank",rel:"noopener noreferrer"},$e=e("strong",null,[e("code",null,"ECCV 2024")],-1),er={href:"https://arxiv.org/pdf/2407.15843",target:"_blank",rel:"noopener noreferrer"},rr=e("strong",null,[e("code",null,"arxiv")],-1),or={href:"https://arxiv.org/pdf/2407.05679v1",target:"_blank",rel:"noopener noreferrer"},nr=e("strong",null,[e("code",null,"arxiv")],-1),tr=e("strong",null,[e("code",null,"Planning")],-1),lr={href:"https://arxiv.org/pdf/2406.10714",target:"_blank",rel:"noopener noreferrer"},ar={href:"https://arxiv.org/pdf/2406.08691",target:"_blank",rel:"noopener noreferrer"},ir={href:"https://arxiv.org/pdf/2406.08481",target:"_blank",rel:"noopener noreferrer"},sr={href:"https://arxiv.org/ab/2405.20337",target:"_blank",rel:"noopener noreferrer"},dr={href:"https://github.com/wzzheng/OccSora",target:"_blank",rel:"noopener noreferrer"},hr={href:"https://arxiv.org/abs/2406.01349",target:"_blank",rel:"noopener noreferrer"},cr=e("strong",null,[e("code",null,"NeurIPS 2024")],-1),ur=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),pr={href:"https://arxiv.org/pdf/2405.17398",target:"_blank",rel:"noopener noreferrer"},_r=e("strong",null,[e("code",null,"CVPR 2024")],-1),gr={href:"https://arxiv.org/pdf/2405.04390",target:"_blank",rel:"noopener noreferrer"},fr=e("strong",null,[e("code",null,"CVPR 2024")],-1),mr=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),vr={href:"https://arxiv.org/abs/2310.08370",target:"_blank",rel:"noopener noreferrer"},br={href:"https://github.com/Nightmare-n/UniPAD",target:"_blank",rel:"noopener noreferrer"},kr=e("strong",null,[e("code",null,"CVPR 2024")],-1),wr=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),Dr={href:"https://arxiv.org/pdf/2403.09630.pdf",target:"_blank",rel:"noopener noreferrer"},Pr=e("strong",null,[e("code",null,"arxiv")],-1),Ar={href:"https://arxiv.org/pdf/2402.16720.pdf",target:"_blank",rel:"noopener noreferrer"},xr=e("strong",null,[e("code",null,"CVPR 2024")],-1),Mr=e("strong",null,[e("code",null,"Pre-training")],-1),yr=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),Wr=e("strong",null,[e("code",null,"NuScenes dataset")],-1),Cr={href:"https://arxiv.org/pdf/2312.17655",target:"_blank",rel:"noopener noreferrer"},Lr={href:"https://github.com/OpenDriveLab/ViDAR",target:"_blank",rel:"noopener noreferrer"},Ir=e("strong",null,[e("code",null,"ICLR 2024")],-1),Sr=e("strong",null,[e("code",null,"Future Prediction")],-1),Gr=e("strong",null,[e("code",null,"from Waabi")],-1),Vr=e("strong",null,[e("code",null,"NuScenes, KITTI Odemetry, Argoverse2 Lidar datasets")],-1),Rr={href:"https://arxiv.org/abs/2311.01017",target:"_blank",rel:"noopener noreferrer"},Er=e("strong",null,[e("code",null,"arxiv")],-1),Tr=e("strong",null,[e("code",null,"Generative AI")],-1),Nr={href:"https://arxiv.org/pdf/2310.07771.pdf",target:"_blank",rel:"noopener noreferrer"},Or={href:"https://github.com/shalfun/DrivingDiffusion",target:"_blank",rel:"noopener noreferrer"},zr=e("strong",null,[e("code",null,"arxiv")],-1),Fr=e("strong",null,[e("code",null,"Pre-training")],-1),Ur=e("strong",null,[e("code",null,"CARLA dataset")],-1),Br={href:"https://arxiv.org/pdf/2311.11762.pdf",target:"_blank",rel:"noopener noreferrer"},jr=e("strong",null,[e("code",null,"arxiv")],-1),qr=e("strong",null,[e("code",null,"Generative AI, Planning")],-1),Yr=e("strong",null,[e("code",null,"NuScenes and Waymo datasets")],-1),Hr={href:"https://arxiv.org/pdf/2311.17918.pdf",target:"_blank",rel:"noopener noreferrer"},Jr=e("strong",null,[e("code",null,"arxiv")],-1),Zr=e("strong",null,[e("code",null,"Generative AI")],-1),Kr=e("strong",null,[e("code",null,"NuScenes & one private dataset")],-1),Xr={href:"https://arxiv.org/pdf/2311.13549.pdf",target:"_blank",rel:"noopener noreferrer"},Qr=e("strong",null,[e("code",null,"arxiv")],-1),$r=e("strong",null,[e("code",null,"Occupancy Future Prediction, Planning")],-1),eo=e("strong",null,[e("code",null,"Occ3D dataset for Occupancy Future Prediction, NuScenes for motion planning")],-1),ro={href:"https://arxiv.org/pdf/2311.16038.pdf",target:"_blank",rel:"noopener noreferrer"},oo={href:"https://github.com/wzzheng/OccWorld",target:"_blank",rel:"noopener noreferrer"},no=e("strong",null,[e("code",null,"arxiv")],-1),to=e("strong",null,[e("code",null,"Generative AI")],-1),lo=e("strong",null,[e("code",null,"Wayve's private data")],-1),ao={href:"https://arxiv.org/pdf/2309.17080.pdf",target:"_blank",rel:"noopener noreferrer"},io={span:""},so={href:"https://proceedings.neurips.cc/paper_files/paper/2022/file/b2fe1ee8d936ac08dd26f2ff58986c8f-Paper-Conference.pdf",target:"_blank",rel:"noopener noreferrer"},ho={href:"https://github.com/plai-group/flexible-video-diffusion-modeling",target:"_blank",rel:"noopener noreferrer"},co={href:"https://www.youtube.com/watch?v=cS6JQpEY9cs",target:"_blank",rel:"noopener noreferrer"},uo={href:"https://www.youtube.com/watch?v=687zEGODmHA",target:"_blank",rel:"noopener noreferrer"},po={href:"https://www.youtube.com/watch?v=pea3sH6orMc",target:"_blank",rel:"noopener noreferrer"},_o=e("strong",null,[e("code",null,"arxiv")],-1),go=e("strong",null,[e("code",null,"Generative AI")],-1),fo=e("strong",null,[e("code",null,"NuScenes dataset")],-1),mo={href:"https://arxiv.org/pdf/2309.09777.pdf",target:"_blank",rel:"noopener noreferrer"},vo={href:"https://github.com/JeffWang987/DriveDreamer",target:"_blank",rel:"noopener noreferrer"},bo=e("strong",null,"'PhD Thesis'",-1),ko=e("strong",null,[e("code",null,"from Wayve")],-1),wo={href:"https://arxiv.org/pdf/2306.09179",target:"_blank",rel:"noopener noreferrer"},Do=e("strong",null,[e("code",null,"arxiv")],-1),Po=e("strong",null,[e("code",null,"Pre-training")],-1),Ao=e("strong",null,[e("code",null,"NuScenes dataset")],-1),xo={href:"https://arxiv.org/pdf/2308.07234.pdf",target:"_blank",rel:"noopener noreferrer"},Mo=e("strong",null,[e("code",null,"ICLR 2022 workshop on Generalizable Policy Learning in the Physical World")],-1),yo=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Wo={href:"https://arxiv.org/abs/2204.07184",target:"_blank",rel:"noopener noreferrer"},Co={href:"https://github.com/vladisai/pytorch-ppuu",target:"_blank",rel:"noopener noreferrer"},Lo=e("strong",null,[e("code",null,"NeurIPS 2022 Deep Reinforcement Learning Workshop")],-1),Io=e("strong",null,[e("code",null,"RL")],-1),So=e("strong",null,[e("code",null,"CARLA dataset")],-1),Go={href:"https://arxiv.org/pdf/2210.04017.pdf",target:"_blank",rel:"noopener noreferrer"},Vo=e("strong",null,[e("code",null,"NeurIPS 2022")],-1),Ro=e("strong",null,[e("code",null,"RL")],-1),Eo=e("strong",null,[e("code",null,"from Wayve")],-1),To={href:"https://arxiv.org/pdf/2210.07729.pdf",target:"_blank",rel:"noopener noreferrer"},No={href:"https://github.com/wayveai/mile",target:"_blank",rel:"noopener noreferrer"},Oo=e("strong",null,[e("code",null,"NeurIPS 2022")],-1),zo={href:"https://arxiv.org/pdf/2205.13817.pdf",target:"_blank",rel:"noopener noreferrer"},Fo={href:"https://github.com/panmt/iso-dream",target:"_blank",rel:"noopener noreferrer"},Uo=e("strong",null,[e("code",null,"ICCV 2019")],-1),Bo=e("strong",null,[e("code",null,"Future Prediction")],-1),jo=e("strong",null,[e("code",null,"from Wayve")],-1),qo=e("strong",null,[e("code",null,"NuScenes, Lyft datasets")],-1),Yo={href:"https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf",target:"_blank",rel:"noopener noreferrer"},Ho={href:"https://github.com/wayveai/fiery",target:"_blank",rel:"noopener noreferrer"},Jo=e("strong",null,[e("code",null,"CVPR 2021 Oral")],-1),Zo=e("strong",null,[e("code",null,"RL")],-1),Ko={href:"https://arxiv.org/pdf/2105.00636.pdf",target:"_blank",rel:"noopener noreferrer"},Xo={href:"https://dotchen.github.io/world_on_rails/",target:"_blank",rel:"noopener noreferrer"},Qo={href:"https://github.com/dotchen/WorldOnRails",target:"_blank",rel:"noopener noreferrer"},$o=e("strong",null,[e("code",null,"ICLR 2019")],-1),en=e("strong",null,[e("code",null,"Future Prediction")],-1),rn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),on={href:"https://github.com/Atcold/pytorch-PPUU?tab=readme-ov-file",target:"_blank",rel:"noopener noreferrer"},nn={href:"https://github.com/Atcold/pytorch-PPUU",target:"_blank",rel:"noopener noreferrer"},tn=e("h2",{id:"other-general-world-model-papers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#other-general-world-model-papers"},[e("span",null,"Other General World Model Papers")])],-1),ln={href:"https://arxiv.org/pdf/2405.18418",target:"_blank",rel:"noopener noreferrer"},an={href:"https://world-model.maitrix.org/assets/pandora.pdf",target:"_blank",rel:"noopener noreferrer"},sn=e("li",null,[r("2024-Efficient World Models with Time-Aware and Context-Augmented Tokenization "),e("strong",null,[e("code",null,"ICML 2024")])],-1),dn=e("strong",null,[e("code",null,"ICML 2024")],-1),hn={href:"https://arxiv.org/pdf/2403.09631.pdf",target:"_blank",rel:"noopener noreferrer"},cn=e("strong",null,[e("code",null,"website")],-1),un={href:"https://www.archetypeai.io/blog/introducing-archetype-ai---understand-the-real-world-in-real-time",target:"_blank",rel:"noopener noreferrer"},pn=e("strong",null,[e("code",null,"arxiv")],-1),_n={href:"https://arxiv.org/pdf/2404.05014.pdf",target:"_blank",rel:"noopener noreferrer"},gn={href:"https://github.com/PKU-YuanGroup/MagicTime",target:"_blank",rel:"noopener noreferrer"},fn=e("strong",null,[e("code",null,"arxiv")],-1),mn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),vn={href:"https://arxiv.org/pdf/2403.00504.pdf",target:"_blank",rel:"noopener noreferrer"},bn=e("strong",null,[e("code",null,"arxiv")],-1),kn=e("strong",null,[e("code",null,"Deepmind")],-1),wn={href:"https://arxiv.org/abs/2402.17139",target:"_blank",rel:"noopener noreferrer"},Dn=e("strong",null,[e("code",null,"Deepmind")],-1),Pn={href:"https://arxiv.org/abs/2402.15391v1",target:"_blank",rel:"noopener noreferrer"},An={href:"https://sites.google.com/view/genie-2024/home",target:"_blank",rel:"noopener noreferrer"},xn=e("strong",null,[e("code",null,"OpenAI")],-1),Mn=e("strong",null,[e("code",null,"Generative AI")],-1),yn={href:"https://openai.com/sora",target:"_blank",rel:"noopener noreferrer"},Wn={href:"https://openai.com/research/video-generation-models-as-world-simulators",target:"_blank",rel:"noopener noreferrer"},Cn=e("strong",null,[e("code",null,"arxiv")],-1),Ln=e("strong",null,[e("code",null,"Generative AI")],-1),In={href:"https://arxiv.org/abs/2402.08268",target:"_blank",rel:"noopener noreferrer"},Sn={href:"https://github.com/LargeWorldModel/LWM",target:"_blank",rel:"noopener noreferrer"},Gn=e("strong",null,[e("code",null,"arxiv")],-1),Vn=e("strong",null,[e("code",null,"Generative AI")],-1),Rn={href:"https://arxiv.org/abs/2401.09985",target:"_blank",rel:"noopener noreferrer"},En=e("strong",null,[e("code",null,"NeurIPS 2024")],-1),Tn={href:"https://proceedings.neurips.cc/paper_files/paper/2023/file/d9042abf40782fbce28901c1c9c0e8d8-Paper-Conference.pdf",target:"_blank",rel:"noopener noreferrer"},Nn={href:"https://github.com/Alescontrela/viper_rl",target:"_blank",rel:"noopener noreferrer"},On=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),zn={href:"https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",target:"_blank",rel:"noopener noreferrer"},Fn={href:"https://github.com/facebookresearch/jepa",target:"_blank",rel:"noopener noreferrer"},Un=e("strong",null,[e("code",null,"NeurIPS 2023")],-1),Bn={href:"https://arxiv.org/abs/2307.02064",target:"_blank",rel:"noopener noreferrer"},jn=e("strong",null,[e("code",null,"CVPR 2023")],-1),qn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Yn={href:"https://arxiv.org/abs/2301.08243",target:"_blank",rel:"noopener noreferrer"},Hn={href:"https://github.com/facebookresearch/ijepa",target:"_blank",rel:"noopener noreferrer"},Jn=e("strong",null,[e("code",null,"ICML 2023")],-1),Zn={href:"https://arxiv.org/abs/2210.02396",target:"_blank",rel:"noopener noreferrer"},Kn={href:"https://github.com/wilson1yan/teco",target:"_blank",rel:"noopener noreferrer"},Xn=e("strong",null,[e("code",null,"arxiv")],-1),Qn={href:"https://arxiv.org/abs/2308.01399",target:"_blank",rel:"noopener noreferrer"},$n={href:"https://github.com/jlin816/dynalang",target:"_blank",rel:"noopener noreferrer"},et=e("strong",null,[e("code",null,"ICLR 2023")],-1),rt=e("strong",null,[e("code",null,"RL")],-1),ot={href:"https://arxiv.org/pdf/2209.00588.pdf",target:"_blank",rel:"noopener noreferrer"},nt={href:"https://github.com/eloialonso/iris",target:"_blank",rel:"noopener noreferrer"},tt=e("strong",null,[e("code",null,"arxiv")],-1),lt=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),at=e("strong",null,[e("code",null,"Planning")],-1),it={href:"https://arxiv.org/pdf/2312.17227",target:"_blank",rel:"noopener noreferrer"},st=e("strong",null,[e("code",null,"arxiv")],-1),dt=e("strong",null,[e("code",null,"RL")],-1),ht={href:"https://arxiv.org/pdf/2312.08533.pdf",target:"_blank",rel:"noopener noreferrer"},ct=e("strong",null,[e("code",null,"arxiv")],-1),ut=e("strong",null,[e("code",null,"RL")],-1),pt={href:"https://arxiv.org/abs/2301.04104",target:"_blank",rel:"noopener noreferrer"},_t={href:"https://github.com/danijar/dreamerv3",target:"_blank",rel:"noopener noreferrer"},gt=e("strong",null,[e("code",null,"CoRL 2022")],-1),ft=e("strong",null,[e("code",null,"Robotics")],-1),mt={href:"https://arxiv.org/abs/2206.14176",target:"_blank",rel:"noopener noreferrer"},vt={href:"https://github.com/danijar/daydreamer",target:"_blank",rel:"noopener noreferrer"},bt=e("strong",null,[e("code",null,"CoRL 2022")],-1),kt=e("strong",null,[e("code",null,"Robotics")],-1),wt={href:"https://proceedings.mlr.press/v205/seo23a.html",target:"_blank",rel:"noopener noreferrer"},Dt={href:"https://github.com/younggyoseo/MWM",target:"_blank",rel:"noopener noreferrer"},Pt=e("strong",null,[e("code",null,"openreview")],-1),At=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),xt=e("strong",null,[e("code",null,"General Roadmap for World Models")],-1),Mt={href:"https://openreview.net/forum?id=BZ5a1r-kVsf",target:"_blank",rel:"noopener noreferrer"},yt={href:"https://leshouches2022.github.io/SLIDES/compressed-yann-1.pdf",target:"_blank",rel:"noopener noreferrer"},Wt={href:"https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-02.pdf",target:"_blank",rel:"noopener noreferrer"},Ct={href:"https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-03.pdf",target:"_blank",rel:"noopener noreferrer"},Lt={href:"https://www.youtube.com/playlist?list=PLEIq5bchE3R3Yl5taXdYA04a9kH9yvyGm",target:"_blank",rel:"noopener noreferrer"},It=e("strong",null,[e("code",null,"NeurIPS 2021")],-1),St={href:"https://proceedings.neurips.cc/paper_files/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html",target:"_blank",rel:"noopener noreferrer"},Gt={href:"https://orybkin.github.io/lexa/",target:"_blank",rel:"noopener noreferrer"},Vt=e("strong",null,[e("code",null,"ICLR 2021")],-1),Rt=e("strong",null,[e("code",null,"RL")],-1),Et=e("strong",null,[e("code",null,"from Google & Deepmind")],-1),Tt={href:"https://arxiv.org/pdf/2010.02193.pdf",target:"_blank",rel:"noopener noreferrer"},Nt={href:"https://github.com/danijar/dreamerv2",target:"_blank",rel:"noopener noreferrer"},Ot=e("strong",null,[e("code",null,"ICLR 2020")],-1),zt={href:"https://arxiv.org/abs/1912.01603",target:"_blank",rel:"noopener noreferrer"},Ft={href:"https://github.com/google-research/dreamer",target:"_blank",rel:"noopener noreferrer"},Ut=e("strong",null,[e("code",null,"ICML 2019")],-1),Bt={href:"https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf",target:"_blank",rel:"noopener noreferrer"},jt={href:"https://github.com/google-research/planet",target:"_blank",rel:"noopener noreferrer"},qt=e("strong",null,[e("code",null,"arxiv")],-1),Yt=e("strong",null,[e("code",null,"RL, Planning")],-1),Ht=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Jt={href:"https://arxiv.org/pdf/1705.07177",target:"_blank",rel:"noopener noreferrer"},Zt=e("strong",null,[e("code",null,"NeurIPS 2018")],-1),Kt={href:"https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf",target:"_blank",rel:"noopener noreferrer"},Xt={href:"https://github.com/hardmaru/WorldModelsExperiments",target:"_blank",rel:"noopener noreferrer"},Qt=e("h2",{id:"■-➢-发现的新的有意思的研究方向",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-➢-发现的新的有意思的研究方向"},[e("span",null,"■ ➢ 发现的新的有意思的研究方向-->")])],-1),$t=e("p",null,"生成式的World Model可以被用来当作一种仿真工具来生成仿真数据，特别是极为少见的Corner Case的数据。特别是基于Text to image 的可控条件生成Corner Case，可以进行数据增广，解决真实数据且标注少的现存问题。 然而World Model更有潜力的应用方向是World Model可能会成为像GPT一样的自动驾驶领域的基础模型，而其他自动驾驶具体任务都会围绕这个基础模型进行研发构建。 重点阅读vista 还有一篇是新出的数据集可以进行复现。",-1),el=e("h2",{id:"■-➢可控条件生成",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-➢可控条件生成"},[e("span",null,"■ ➢可控条件生成-->")])],-1),rl={href:"https://github.com/cure-lab/MagicDrive",target:"_blank",rel:"noopener noreferrer"},ol={href:"https://arxiv.org/abs/2310.02601",target:"_blank",rel:"noopener noreferrer"},nl={href:"https://github.com/cure-lab/MagicDrive",target:"_blank",rel:"noopener noreferrer"},tl={href:"https://arxiv.org/abs/2405.14475",target:"_blank",rel:"noopener noreferrer"},ll={href:"https://github.com/flymin/MagicDrive3D",target:"_blank",rel:"noopener noreferrer"},al={href:"https://zhuanlan.zhihu.com/p/684249231",target:"_blank",rel:"noopener noreferrer"},il={href:"https://drive-wm.github.io/",target:"_blank",rel:"noopener noreferrer"},sl=e("li",null,"○ 可控条件生成-->Geodiffusion",-1),dl=e("li",null,"○ -->Detdiffusion",-1),hl=e("li",null,"○ 可控条件生成-->BevControl",-1),cl=e("li",null,"○ 可控条件生成-->BevControl",-1),ul=e("li",null,"○ 可控条件生成--PerLDiff",-1),pl=e("li",null,"○ 可控条件生成学习平台--CarDreamer",-1),_l=e("li",null,"○ 可控条件生成学习平台--DriveArena",-1),gl=e("h2",{id:"■-➢occ世界模型",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#■-➢occ世界模型"},[e("span",null,"■ ➢occ世界模型-->")])],-1),fl=e("strong",null,[e("code",null,"ITSC 2023")],-1),ml=e("strong",null,[e("code",null,"Planning, Neural Predicted-Guided Planning")],-1),vl=e("strong",null,[e("code",null,"Waymo Open Motion dataset")],-1),bl={href:"https://arxiv.org/abs/2305.03303",target:"_blank",rel:"noopener noreferrer"},kl=e("ul",null,[e("li",null,"与occ结合的：occworld、Drive-occword、OccLLaMA、Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving、OccSora")],-1),wl=e("h3",{id:"_1-definition",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-definition"},[e("span",null,"1. DEFINITION")])],-1),Dl=e("p",null,"每篇文章的创新点从单一多视角到环视视角、从单一输入到多模态输入来提高生成质量。还有视频生成的时长，以及轨迹预测，基于决策的世界模型方法。需解决时空不一致性和生成场景连续性。",-1);function Pl(Al,xl){const o=i("ExternalLinkIcon");return s(),d("div",null,[g,e("p",null,[r("[1] "),e("a",f,[r("https://github.com/GigaAI-research/General-World-Models-Survey"),n(o)]),r(" 该 repo 内有目前世界模型方向的优秀论文汇总，包括基本分类：视频生成、自动驾驶和自主代理。其中自动驾驶分成端到端、以及2D、3D神经模拟器方法。世界模型的文献、 开源code、 综述。")]),e("p",null,[r("[2] "),e("a",m,[r("https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey"),n(o)]),r(" 该repo 内以‘时间’为顺序精选相关世界自动驾驶模型。且并持续更新，包括一些挑战、相关视频，包括机器人领域的世界模型使用（大多数为模仿学习强化学习方向）可参考借鉴。")]),e("p",null,[r("[3] "),e("a",v,[r("Awesome-World-Models-for-AD"),n(o)])]),e("p",null,[r("[4] "),e("a",b,[r("World models paper list from Shanghai AI lab"),n(o)])]),e("p",null,[r("[5] "),e("a",k,[r("Awesome-Papers-World-Models-Autonomous-Driving"),n(o)]),r(".")]),w,t("这里图像的路径不用写完全，需要按照vuepress的格式来，而且必须放到src/.vuepress/public文件夹中"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/world_model.png" width="20%" />
</p> `),D,P,e("p",null,[r("[1] 世界模型简介："),e("a",A,[r("https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw"),n(o)]),r("世界模型是一种人工智能技术，旨在通过整合多种感知信息，如视觉、听觉和语言，利用机器学习和深度学习等方法来理解和预测现实世界。它包括感知模块、表征学习、动力学模型和生成模型，用于构建环境的内部表示，不仅能反映当前状态，还能预测未来变化。这种模型在强化学习、自动驾驶、游戏开发和机器人学等领域有广泛应用。Yann LeCun提出的这一概念，强调通过自监督学习让AI像人一样理解世界，形成内部的心理表征，以期实现通用人工智能。Meta的I-JEPA模型是基于这一愿景的实现，它通过分析和补全图像展示了对世界背景知识的应用。")]),e("p",null,[r("[2] 影响较大的早期世界模型文章：2018年Jurgen在NeurIPS 以循环世界模型促进策略演变“Recurrent World Models Facilitate Policy Evolution”的title发表：链接: "),e("a",x,[r("https://arxiv.org/abs/1803.10122"),n(o)]),r(" 示例: "),e("a",M,[r("https://worldmodels.github.io/"),n(o)])]),e("p",null,[r("[3] 世界模型在自动驾驶领域的应用： "),e("a",y,[r("https://www.bilibili.com/read/cv34465959/"),n(o)])]),e("p",null,[r("[4] 世界模型用于自动驾驶场景生成以及仿真平台: "),e("a",W,[r("https://blog.csdn.net/CV_Autobot/article/details/134002647"),n(o)])]),C,e("p",null,[r("[1] 2024-Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond， "),L,r(),e("a",I,[r("Paper"),n(o)]),r(" 极佳科技 （比较全面，该综述通过 260 余篇文献，对世界模型在视频生成、自动驾驶、智能体、通用机器人等领域的研究和应用进行了详尽的分析和讨论。另外，该综述还审视了当前世界模型的挑战和局限性，并展望了它们未来的发展方向。）")]),e("p",null,[r("[2] 2024-World Models for Autonomous Driving: An Initial Survey，IEEE TIV,澳门大学，夏威夷大学。"),e("a",S,[r("Paper"),n(o)]),r("（画风有趣，对自动驾驶世界模型的现状和未来进展进行了初步回顾，涵盖了它们的理论基础、实际应用以及旨在克服现有局限性的正在进行的研究工作。）")]),e("p",null,[r("[3]2024-Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies "),G,r(),e("a",V,[r("Paper"),n(o)])]),e("p",null,[r("[4]2024-Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities "),R,r(),e("a",E,[r("Paper"),n(o)])]),T,e("ul",null,[e("li",null,[e("p",null,[r("2024-1X World Model Challenge "),N,r(),e("a",O,[r("Link"),n(o)])])]),e("li",null,[e("p",null,[r("2024-ECCV Corner case Challenge "),z,r(),e("a",F,[r("Link"),n(o)])])]),e("li",null,[e("p",null,[r("2024-CVPR Workshop, Foundation Models for Autonomous Systems, Challenges, Track 4: Predictive World Model "),U,r(),e("a",B,[r("Link"),n(o)])])])]),j,e("ul",null,[e("li",null,[e("p",null,[r("2023 "),q,r("; "),e("a",Y,[r("Video"),n(o)])])]),e("li",null,[e("p",null,[r("2022-Neural World Models for Autonomous Driving "),e("a",H,[r("Video"),n(o)])])])]),J,e("h4",Z,[e("a",K,[e("span",null,[r("■ 上海AILab（上海人工智能实验室） "),e("a",X,[r("https://opendrivelab.com/publications/"),n(o)])])])]),e("h4",Q,[e("a",$,[e("span",null,[r("■ 香港中文大学（陈铠老师团队）Geometric-Controllable Visual Generation: A Systematic Solution "),e("a",ee,[r("Video"),n(o)])])])]),e("h4",re,[e("a",oe,[e("span",null,[r("■ 极佳科技（极佳科技DriveDreamer自动驾驶世界模型、WorldDreamer通用世界模型目前已成功商业化落地。）"),e("a",ne,[r("推文"),n(o)])])])]),te,e("h4",le,[e("a",ae,[e("span",null,[r("■ 蔚来车企："),e("a",ie,[r("https://www.qbitai.com/2024/07/172025.html"),n(o)])])])]),se,t("这里图像的路径不用写完全，需要按照vuepress的格式来，而且必须放到src/.vuepress/public文件夹中"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/End to end.png" width="100%" />
</p> `),de,t("这里图像的路径不用写完全，需要按照vuepress的格式来，而且必须放到src/.vuepress/public文件夹中"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/Generation.png" width="100%" />
</p> `),he,e("ul",null,[e("li",null,[r("👍(2023 Arxiv) GAIA-1: A generative world model for autonomous driving ["),e("a",ce,[r("Paper"),n(o)]),r("]["),e("a",ue,[r("Blog"),n(o)]),r("] (Wayve)")])]),e("ul",null,[e("li",null,[r("(2023 CVPR 2023 workshop) ["),e("a",pe,[r("Video"),n(o)]),r("] (Tesla)")]),e("li",null,[r("👍(2023 Arxiv) DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving ["),e("a",_e,[r("Paper"),n(o)]),r("]["),e("a",ge,[r("Code"),n(o)]),r("] (GigaAI)")]),e("li",null,[r("(2023 Arxiv) ADriver-I: A General World Model for Autonomous Driving ["),e("a",fe,[r("Paper"),n(o)]),r("] (MEGVII)")]),e("li",null,[r("👍(2023 Arxiv) DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model ["),e("a",me,[r("Paper"),n(o)]),r("] (Baidu)")]),e("li",null,[r("(2023 Arxiv) Panacea: Panoramic and Controllable Video Generation for Autonomous Driving ["),e("a",ve,[r("Paper"),n(o)]),r("]["),e("a",be,[r("Code"),n(o)]),r("] (MEGVII)")]),e("li",null,[r("👍(2024 CVPR) Drive-WM: Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving ["),e("a",ke,[r("Paper"),n(o)]),r("]["),e("a",we,[r("Code"),n(o)]),r("] (CASIA)")]),e("li",null,[r("(2023 Arxiv) WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation ["),e("a",De,[r("Paper"),n(o)]),r("] (Fudan)")]),e("li",null,[r("(2024 Arxiv) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation ["),e("a",Pe,[r("Paper"),n(o)]),r("]["),e("a",Ae,[r("Code"),n(o)]),r("] (GigaAI)")]),e("li",null,[r("(2024 CVPR) GenAD: Generalized Predictive Model for Autonomous Driving ["),e("a",xe,[r("Paper"),n(o)]),r("]["),e("a",Me,[r("Code"),n(o)]),r("] (Shanghai AI Lab)")]),e("li",null,[r("(2024 Arxiv) SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control ["),e("a",ye,[r("Paper"),n(o)]),r("] (MEGVII)")])]),We,e("ul",null,[e("li",null,[r("👍(2024 ICLR) Copilot4D:Learning unsupervised world models for autonomous driving via discrete diffusion ["),e("a",Ce,[r("Paper"),n(o)]),r("] (Waabi)")])]),e("ul",null,[e("li",null,[r("(2023 Arxiv) OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving ["),e("a",Le,[r("Paper"),n(o)]),r("]["),e("a",Ie,[r("Code"),n(o)]),r("] (THU)")]),e("li",null,[r("(2023 Arxiv) MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations ["),e("a",Se,[r("Paper"),n(o)]),r("] (KIT)")]),e("li",null,[r("(2024 Arxiv) LidarDM: Generative LiDAR Simulation in a Generated World ["),e("a",Ge,[r("Paper"),n(o)]),r("]["),e("a",Ve,[r("Code"),n(o)]),r("] (MIT)")])]),Re,e("ul",null,[e("li",null,[r("(2024 CVPR) ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving ["),e("a",Ee,[r("Paper"),n(o)]),r("]["),e("a",Te,[r("Code"),n(o)]),r("] (Shanghai AI Lab)")]),Ne]),Oe,e("ul",null,[e("li",null,[r("👍(2022 NeurIPS) Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models ["),e("a",ze,[r("Paper"),n(o)]),r("] (SJTU)")])]),e("ul",null,[e("li",null,[r("👍(2022 NeurIPS) MILE: Model-Based Imitation Learning for Urban Driving ["),e("a",Fe,[r("Paper"),n(o)]),r("]["),e("a",Ue,[r("Code"),n(o)]),r("] (Wayve)")]),e("li",null,[r("(2022 NeurIPS Deep RL Workshop) SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model ["),e("a",Be,[r("Paper"),n(o)]),r("] (HIT & THU)")]),e("li",null,[r("(2023 ICRA) TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction ["),e("a",je,[r("Paper"),n(o)]),r("] (ETH Zurich)")]),e("li",null,[r("(2024 Arxiv) Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2) ["),e("a",qe,[r("Paper"),n(o)]),r("] (SJTU)")])]),t("这里图像的路径不用写完全，需要按照vuepress的格式来，而且必须放到src/.vuepress/public文件夹中"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/ad.png" width="30%" />
</p> `),Ye,e("ul",null,[e("li",null,[e("p",null,[r("2024-DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model "),e("a",He,[r("Paper"),n(o)]),r(),Je])]),e("li",null,[e("p",null,[r("2024-Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models "),e("a",Ze,[r("Paper"),n(o)]),r(),Ke])]),e("li",null,[e("p",null,[r("2024-OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving "),e("a",Xe,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Drive-OccWorld: Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving "),e("a",Qe,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-CarFormer: Self-Driving with Learned Object-Centric Representations "),$e,r(),e("a",er,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space "),rr,r(),e("a",or,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Planning with Adaptive World Models for Autonomous Driving "),nr,r("; "),tr,r("; "),e("a",lr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-UnO: Unsupervised Occupancy Fields for Perception and Forecasting "),e("a",ar,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-LAW: Enhancing End-to-End Autonomous Driving with Latent World Model "),e("a",ir,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving "),e("a",sr,[r("Paper"),n(o)]),r(", "),e("a",dr,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Delphi: Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation "),e("a",hr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("👍2024-Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability "),cr,r("; "),ur,r(),e("a",pr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving "),_r,r("; __"),e("a",gr,[r("Paper"),n(o)]),r(",")])]),e("li",null,[e("p",null,[r("2024-UniPAD: A Universal Pre-training Paradigm for Autonomous Driving "),fr,r("; "),mr,r(),e("a",vr,[r("Paper"),n(o)]),r(", "),e("a",br,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2024-GenAD: Generalized Predictive Model for Autonomous Driving "),kr,r("; "),wr,r(),e("a",Dr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving "),Pr,r(),e("a",Ar,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving "),xr,r("; "),Mr,r("; "),yr,r("; "),Wr,r(),e("a",Cr,[r("Paper"),n(o)]),r(", "),e("a",Lr,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion "),Ir,r("; "),Sr,r("; "),Gr,r("; "),Vr,r(),e("a",Rr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model "),Er,r("; "),Tr,r(),e("a",Nr,[r("Paper"),n(o)]),r(", "),e("a",Or,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2023-MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations "),zr,r("; "),Fr,r("; "),Ur,r(),e("a",Br,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving "),jr,r("; "),qr,r("; "),Yr,r(),e("a",Hr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-ADriver-I: A General World Model for Autonomous Driving "),Jr,r("; "),Zr,r("; "),Kr,r(),e("a",Xr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving "),Qr,r("; "),$r,r("; "),eo,r(),e("a",ro,[r("Paper"),n(o)]),r(", "),e("a",oo,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2023-GAIA-1: A Generative World Model for Autonomous Driving "),no,r("; "),to,r("; "),lo,r(),e("a",ao,[r("Paper"),n(o)])]),e("details",io,[r(" Related papers & tutorials to understand this paper: "),e("p",null,[r("FDM for video diffusion decoder: "),e("a",so,[r("Paper"),n(o)]),r(", "),e("a",ho,[r("Code"),n(o)])]),e("p",null,[r("Denoising diffusion tutorials: "),e("a",co,[r("CVPR 2022 tutorial"),n(o)]),r(", "),e("a",uo,[r("class from UC Berkeley"),n(o)]),r(", "),e("a",po,[r("Video"),n(o)])])])]),e("li",null,[e("p",null,[r("2023-DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving "),_o,r("; "),go,r("; "),fo,r(),e("a",mo,[r("Paper"),n(o)]),r(", "),e("a",vo,[r("Code (To be released soon)"),n(o)])])]),e("li",null,[e("p",null,[r("2023-Neural World Models for Computer Vision "),bo,r("; "),ko,r(),e("a",wo,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-UniWorld: Autonomous Driving Pre-training via World Models "),Do,r("; "),Po,r("; "),Ao,r(),e("a",xo,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2022-Separating the World and Ego Models for Self-Driving "),Mo,r("; "),yo,r(),e("a",Wo,[r("Paper"),n(o)]),r(", "),e("a",Co,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2022-SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model "),Lo,r("; "),Io,r("; "),So,r(),e("a",Go,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2022-MILE: Model-Based Imitation Learning for Urban Driving "),Vo,r("; "),Ro,r("; "),Eo,r(),e("a",To,[r("Paper"),n(o)]),r(", "),e("a",No,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2022-Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models "),Oo,r(),e("a",zo,[r("Paper"),n(o)]),r(", "),e("a",Fo,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2021-FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras "),Uo,r("; "),Bo,r("; "),jo,r("; "),qo,r(),e("a",Yo,[r("Paper"),n(o)]),r(", "),e("a",Ho,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2021-Learning to drive from a world on rails "),Jo,r("; "),Zo,r(),e("a",Ko,[r("Paper"),n(o)]),r(", "),e("a",Xo,[r("Project Page"),n(o)]),r(", "),e("a",Qo,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2019-Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic "),$o,r("; "),en,r("; "),rn,r(),e("a",on,[r("Paper"),n(o)]),r(", "),e("a",nn,[r("Code"),n(o)])])])]),tn,e("ul",null,[e("li",null,[r("2024-Hierarchical World Models as Visual Whole-Body Humanoid Controllers "),e("a",ln,[r("Paper"),n(o)])]),e("li",null,[r("2024-Pandora: Towards General World Model with Natural Language Actions and Video States "),e("a",an,[r("Paper"),n(o)])]),sn,e("li",null,[r("2024-3D-VLA: A 3D Vision-Language-Action Generative World Model "),dn,r(),e("a",hn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Newton from Archetype AI "),cn,r(),e("a",un,[r("Link"),n(o)])]),e("li",null,[r("2024-MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators "),pn,r(),e("a",_n,[r("Paper"),n(o)]),r(", "),e("a",gn,[r("Code"),n(o)])]),e("li",null,[r("2024-IWM: Learning and Leveraging World Models in Visual Representation Learning "),fn,r(", "),mn,r(),e("a",vn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Video as the New Language for Real-World Decision Making "),bn,r(", "),kn,r(),e("a",wn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Genie: Generative Interactive Environments "),Dn,r(),e("a",Pn,[r("Paper"),n(o)]),r(", "),e("a",An,[r("Website"),n(o)])]),e("li",null,[r("2024-Sora "),xn,r(", "),Mn,r(),e("a",yn,[r("Link"),n(o)]),r(", "),e("a",Wn,[r("Technical Report"),n(o)])]),e("li",null,[r("2024-LWM: World Model on Million-Length Video And Language With RingAttention "),Cn,r("; "),Ln,r(),e("a",In,[r("Paper"),n(o)]),r(", "),e("a",Sn,[r("Code"),n(o)])]),e("li",null,[r("2024-WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens "),Gn,r("; "),Vn,r(),e("a",Rn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Video prediction models as rewards for reinforcement learning "),En,r(),e("a",Tn,[r("Paper"),n(o)]),r(", "),e("a",Nn,[r("Code"),n(o)])]),e("li",null,[r("2024-V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video "),On,r(),e("a",zn,[r("Paper"),n(o)]),r(", "),e("a",Fn,[r("Code"),n(o)])]),e("li",null,[r("2023-Facing Off World Model Backbones: RNNs, Transformers, and S4 "),Un,r(),e("a",Bn,[r("Paper"),n(o)])]),e("li",null,[r("2023-I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture "),jn,r("; "),qn,r(),e("a",Yn,[r("Paper"),n(o)]),r(", "),e("a",Hn,[r("Code"),n(o)])]),e("li",null,[r("2023-Temporally Consistent Transformers for Video Generation "),Jn,r(),e("a",Zn,[r("Paper"),n(o)]),r(", "),e("a",Kn,[r("Code"),n(o)])]),e("li",null,[r("2023-Learning to Model the World with Language "),Xn,r(),e("a",Qn,[r("Paper"),n(o)]),r(", "),e("a",$n,[r("Code"),n(o)])]),e("li",null,[r("2023-Transformers are sample-efficient world models "),et,r(";"),rt,r(),e("a",ot,[r("Paper"),n(o)]),r(", "),e("a",nt,[r("Code"),n(o)])]),e("li",null,[r("2023-Gradient-based Planning with World Models "),tt,r("; "),lt,r("; "),at,r("; "),e("a",it,[r("Paper"),n(o)])]),e("li",null,[r("2023-World Models via Policy-Guided Trajectory Diffusion "),st,r("; "),dt,r("; "),e("a",ht,[r("Paper"),n(o)])]),e("li",null,[r("2023-DreamerV3: Mastering diverse domains through world models "),ct,r(";"),ut,r("; "),e("a",pt,[r("Paper"),n(o)]),r(", "),e("a",_t,[r("Code"),n(o)])]),e("li",null,[r("2022-Daydreamer: World models for physical robot learning "),gt,r("; "),ft,r(),e("a",mt,[r("Paper"),n(o)]),r(", "),e("a",vt,[r("Code"),n(o)])]),e("li",null,[r("2022-Masked World Models for Visual Control "),bt,r("; "),kt,r(),e("a",wt,[r("Paper"),n(o)]),r(", "),e("a",Dt,[r("Code"),n(o)])]),e("li",null,[r("2022-A Path Towards Autonomous Machine Intelligence "),Pt,r("; "),At,r("; "),xt,r("; "),e("a",Mt,[r("Paper"),n(o)]),r("; "),e("a",yt,[r("Slides1"),n(o)]),r(", "),e("a",Wt,[r("Slides2"),n(o)]),r(", "),e("a",Ct,[r("Slides3"),n(o)]),r("; "),e("a",Lt,[r("Videos"),n(o)])]),e("li",null,[r("2021-LEXA:Discovering and Achieving Goals via World Models "),It,r("; "),e("a",St,[r("Paper"),n(o)]),r(", "),e("a",Gt,[r("Website & Code"),n(o)])]),e("li",null,[r("2021-DreamerV2: Mastering Atari with Discrete World Models "),Vt,r("; "),Rt,r("; "),Et,r(),e("a",Tt,[r("Paper"),n(o)]),r(", "),e("a",Nt,[r("Code"),n(o)])]),e("li",null,[r("2020-Dreamer: Dream to Control: Learning Behaviors by Latent Imagination "),Ot,r(),e("a",zt,[r("Paper"),n(o)]),r(", "),e("a",Ft,[r("Code"),n(o)])]),e("li",null,[r("2019-Learning Latent Dynamics for Planning from Pixels "),Ut,r(),e("a",Bt,[r("Paper"),n(o)]),r(", "),e("a",jt,[r("Code"),n(o)])]),e("li",null,[r("2018-Model-Based Planning with Discrete and Continuous Actions "),qt,r("; "),Yt,r("; "),Ht,r("; "),e("a",Jt,[r("Paper"),n(o)])]),e("li",null,[r("2018-Recurrent world models facilitate policy evolution "),Zt,r("; "),e("a",Kt,[r("Paper"),n(o)]),r(", "),e("a",Xt,[r("Code"),n(o)])])]),Qt,$t,el,e("ul",null,[e("li",null,[r("○ 可控条件生成-->magicdrive "),e("a",rl,[r("https://github.com/cure-lab/MagicDrive"),n(o)]),r(" ["),e("a",ol,[r("paper"),n(o)]),r("] ["),e("a",nl,[r("Code"),n(o)]),r("]可作为baseline. 从几何标注中合成的数据可以帮助下游任务,如2D目标检测。因此,本文探讨了text-to-image (T2I)扩散模型在生成街景图像并惠及下游3D感知模型方面的潜力。")]),e("li",null,[r("○ 可控条件生成-->magicdrive3D ["),e("a",tl,[r("paper"),n(o)]),r("] ["),e("a",ll,[r("Code"),n(o)]),r("]")]),e("li",null,[r("○ 可控条件生成-->panacea "),e("a",al,[r("https://zhuanlan.zhihu.com/p/684249231"),n(o)]),r("用于生成多视角且可控的驾驶场景视频，能够合成无限数量的多样化、带标注的样本，这对于自动驾驶的进步有至关重要的意义。 Panacea解决了两个关键挑战：“一致性”和“可控性”。一致性确保时间和视角的一致性，而可控性确保生成的内容与相应的标注对齐。")]),e("li",null,[r("○ 可控条件生成-->drive-WM这是第一个与现有端到端规划模型兼容的自动驾驶世界模型。通过由视角["),e("a",il,[r("主页"),n(o)]),r("]分解促进的联合空间-时间建模，Drive-WM在驾驶场景中生成高保真度的多视图视频。")]),sl,dl,hl,cl,ul,pl,_l]),gl,e("ul",null,[e("li",null,[r("2023-Occupancy Prediction-Guided Neural Planner for Autonomous Driving "),fl,r("; "),ml,r("; "),vl,r(),e("a",bl,[r("Paper"),n(o)])])]),kl,wl,Dl])}const Wl=a(_,[["render",Pl],["__file","index.html.vue"]]),Cl=JSON.parse('{"path":"/archiver/worldmodel/","title":"World Model","lang":"en-US","frontmatter":{"author":"Zhang Mingzhu (张明珠)","title":"World Model","description":"World-Models-Autonomous-Driving-Latest-Survey A curated list of world model for autonmous driving. Keep updated. 📌 Introduction ✧ 世界模型用于自动驾驶场景生成相关文献整理 ➢ 论文汇总 [1] https://github...","head":[["meta",{"property":"og:url","content":"https://openvisuallab.github.io/archiver/worldmodel/"}],["meta",{"property":"og:site_name","content":"OpenVisualLab"}],["meta",{"property":"og:title","content":"World Model"}],["meta",{"property":"og:description","content":"World-Models-Autonomous-Driving-Latest-Survey A curated list of world model for autonmous driving. Keep updated. 📌 Introduction ✧ 世界模型用于自动驾驶场景生成相关文献整理 ➢ 论文汇总 [1] https://github..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://openvisuallab.github.io/imgs/archiver/world_model/world_model.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-10-22T13:03:36.000Z"}],["meta",{"property":"article:author","content":"Zhang Mingzhu (张明珠)"}],["meta",{"property":"article:modified_time","content":"2024-10-22T13:03:36.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"World Model\\",\\"image\\":[\\"https://openvisuallab.github.io/imgs/archiver/world_model/world_model.png\\",\\"https://openvisuallab.github.io/imgs/archiver/world_model/End%20to%20end.png\\",\\"https://openvisuallab.github.io/imgs/archiver/world_model/Generation.png\\",\\"https://openvisuallab.github.io/imgs/archiver/world_model/ad.png\\"],\\"dateModified\\":\\"2024-10-22T13:03:36.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Zhang Mingzhu (张明珠)\\"}]}"]]},"headers":[{"level":2,"title":"📌 Introduction","slug":"📌-introduction","link":"#📌-introduction","children":[]},{"level":2,"title":"✧ 世界模型用于自动驾驶场景生成相关文献整理","slug":"✧-世界模型用于自动驾驶场景生成相关文献整理","link":"#✧-世界模型用于自动驾驶场景生成相关文献整理","children":[]},{"level":2,"title":"➢ 论文汇总","slug":"➢-论文汇总","link":"#➢-论文汇总","children":[]},{"level":2,"title":"➢ 认识世界模型","slug":"➢-认识世界模型","link":"#➢-认识世界模型","children":[{"level":3,"title":"1. 简单介绍（从世界模型--> 自动驾驶世界模型用于场景生成）","slug":"_1-简单介绍-从世界模型-自动驾驶世界模型用于场景生成","link":"#_1-简单介绍-从世界模型-自动驾驶世界模型用于场景生成","children":[]},{"level":3,"title":"2.论文综述","slug":"_2-论文综述","link":"#_2-论文综述","children":[]}]},{"level":2,"title":"3.挑战赛 Workshops/Challenges","slug":"_3-挑战赛-workshops-challenges","link":"#_3-挑战赛-workshops-challenges","children":[]},{"level":2,"title":"Tutorials/Talks/","slug":"tutorials-talks","link":"#tutorials-talks","children":[]},{"level":2,"title":"➢ 优秀团队 / 学术大佬/ 公司","slug":"➢-优秀团队-学术大佬-公司","link":"#➢-优秀团队-学术大佬-公司","children":[]},{"level":2,"title":"➢ 经典论文：（推荐加“👍”）","slug":"➢-经典论文-推荐加-👍","link":"#➢-经典论文-推荐加-👍","children":[{"level":3,"title":"■ Neural Driving Simulator based on World Models","slug":"■-neural-driving-simulator-based-on-world-models","link":"#■-neural-driving-simulator-based-on-world-models","children":[]},{"level":3,"title":"■ End-to-end Driving based on World Models","slug":"■-end-to-end-driving-based-on-world-models","link":"#■-end-to-end-driving-based-on-world-models","children":[]},{"level":3,"title":"■ 按时间顺序更新","slug":"■-按时间顺序更新","link":"#■-按时间顺序更新","children":[]}]},{"level":2,"title":"Papers","slug":"papers","link":"#papers","children":[]},{"level":2,"title":"Other General World Model Papers","slug":"other-general-world-model-papers","link":"#other-general-world-model-papers","children":[]},{"level":2,"title":"■  ➢ 发现的新的有意思的研究方向-->","slug":"■-➢-发现的新的有意思的研究方向","link":"#■-➢-发现的新的有意思的研究方向","children":[]},{"level":2,"title":"■  ➢可控条件生成-->","slug":"■-➢可控条件生成","link":"#■-➢可控条件生成","children":[]},{"level":2,"title":"■  ➢occ世界模型-->","slug":"■-➢occ世界模型","link":"#■-➢occ世界模型","children":[{"level":3,"title":"1. DEFINITION","slug":"_1-definition","link":"#_1-definition","children":[]}]}],"git":{"createdTime":1726102486000,"updatedTime":1729602216000,"contributors":[{"name":"mingzhuzhang1","email":"145547769+mingzhuzhang1@users.noreply.github.com","commits":4},{"name":"2-mo","email":"1982800736@qq.com","commits":2}]},"readingTime":{"minutes":13.56,"words":4069},"filePathRelative":"archiver/worldmodel/README.md","localizedDate":"September 12, 2024","excerpt":"<blockquote>\\n<p>World-Models-Autonomous-Driving-Latest-Survey</p>\\n</blockquote>\\n<p>A curated list of world model for autonmous driving. Keep updated.</p>\\n<h2>📌 Introduction</h2>\\n<h2>✧ 世界模型用于自动驾驶场景生成相关文献整理</h2>\\n<h2>➢ 论文汇总</h2>\\n<p>[1] <a href=\\"https://github.com/GigaAI-research/General-World-Models-Survey\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/GigaAI-research/General-World-Models-Survey</a> 该 repo 内有目前世界模型方向的优秀论文汇总，包括基本分类：视频生成、自动驾驶和自主代理。其中自动驾驶分成端到端、以及2D、3D神经模拟器方法。世界模型的文献、 开源code、 综述。</p>","autoDesc":true}');export{Wl as comp,Cl as data};
