import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as i,o as s,c as d,a as e,b as r,d as n,e as t,f as l}from"./app-C3BjoxCk.js";const h="/imgs/archiver/world_model/world_model.png",c="/imgs/archiver/world_model/End%20to%20end.png",u="/imgs/archiver/world_model/Generation.png",p="/imgs/archiver/world_model/ad.png",_={},g=l('<blockquote><p>World-Models-Autonomous-Driving-Latest-Survey</p></blockquote><p>A curated list of world model for autonmous driving. Keep updated.</p><h2 id="ğŸ“Œ-introduction" tabindex="-1"><a class="header-anchor" href="#ğŸ“Œ-introduction"><span>ğŸ“Œ Introduction</span></a></h2><h2 id="âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†" tabindex="-1"><a class="header-anchor" href="#âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†"><span>âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†</span></a></h2><h2 id="â¢-è®ºæ–‡æ±‡æ€»" tabindex="-1"><a class="header-anchor" href="#â¢-è®ºæ–‡æ±‡æ€»"><span>â¢ è®ºæ–‡æ±‡æ€»</span></a></h2>',5),f={href:"https://github.com/GigaAI-research/General-World-Models-Survey",target:"_blank",rel:"noopener noreferrer"},m={href:"https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey",target:"_blank",rel:"noopener noreferrer"},v={href:"https://github.com/zhanghm1995/awesome-world-models-for-AD?tab=readme-ov-file#Table-of-Content",target:"_blank",rel:"noopener noreferrer"},b={href:"https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving/blob/main/papers.md#world-model--model-based-rl",target:"_blank",rel:"noopener noreferrer"},k={href:"https://github.com/chaytonmin/Awesome-Papers-World-Models-Autonomous-Driving",target:"_blank",rel:"noopener noreferrer"},w=e("h2",{id:"â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹"},[e("span",null,"â¢ è®¤è¯†ä¸–ç•Œæ¨¡å‹")])],-1),D=e("figure",null,[e("img",{src:h,alt:"world model",tabindex:"0",loading:"lazy"}),e("figcaption",null,"world model")],-1),P=e("h3",{id:"_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ"},[e("span",null,"1. ç®€å•ä»‹ç»ï¼ˆä»ä¸–ç•Œæ¨¡å‹--> è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆï¼‰")])],-1),A={href:"https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw",target:"_blank",rel:"noopener noreferrer"},x={href:"https://arxiv.org/abs/1803.10122",target:"_blank",rel:"noopener noreferrer"},M={href:"https://worldmodels.github.io/",target:"_blank",rel:"noopener noreferrer"},y={href:"https://www.bilibili.com/read/cv34465959/",target:"_blank",rel:"noopener noreferrer"},W={href:"https://blog.csdn.net/CV_Autobot/article/details/134002647",target:"_blank",rel:"noopener noreferrer"},C=e("h3",{id:"_2-è®ºæ–‡ç»¼è¿°",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-è®ºæ–‡ç»¼è¿°"},[e("span",null,"2.è®ºæ–‡ç»¼è¿°")])],-1),L=e("strong",null,[e("code",null,"arxiv")],-1),I={href:"https://arxiv.org/abs/2403.02622",target:"_blank",rel:"noopener noreferrer"},S={href:"https://arxiv.org/abs/2403.02622",target:"_blank",rel:"noopener noreferrer"},G=e("strong",null,[e("code",null,"arxiv")],-1),V={href:"https://arxiv.org/pdf/2401.12888.pdf",target:"_blank",rel:"noopener noreferrer"},R=e("strong",null,[e("code",null,"arxiv")],-1),E={href:"https://arxiv.org/pdf/2401.08045.pdf",target:"_blank",rel:"noopener noreferrer"},T=e("h2",{id:"_3-æŒ‘æˆ˜èµ›-workshops-challenges",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3-æŒ‘æˆ˜èµ›-workshops-challenges"},[e("span",null,"3.æŒ‘æˆ˜èµ› Workshops/Challenges")])],-1),N=e("strong",null,[e("code",null,"Challenges")],-1),O={href:"https://github.com/1x-technologies/1xgpt",target:"_blank",rel:"noopener noreferrer"},z=e("strong",null,[e("code",null,"Challenges")],-1),F={href:"https://coda-dataset.github.io/w-coda2024/track2/",target:"_blank",rel:"noopener noreferrer"},U=e("strong",null,[e("code",null,"Challenges")],-1),B={href:"https://opendrivelab.com/challenge2024/",target:"_blank",rel:"noopener noreferrer"},j=e("h2",{id:"tutorials-talks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tutorials-talks"},[e("span",null,"Tutorials/Talks/")])],-1),q=e("strong",null,[e("code",null,"from Wayve")],-1),Y={href:"https://www.youtube.com/watch?v=lNOs08byOhw",target:"_blank",rel:"noopener noreferrer"},H={href:"https://www.youtube.com/watch?v=wMvYjiv6EpY",target:"_blank",rel:"noopener noreferrer"},J=e("h2",{id:"â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸"},[e("span",null,"â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬/ å…¬å¸")])],-1),Z={id:"â– -ä¸Šæµ·ailab-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤-https-opendrivelab-com-publications",tabindex:"-1"},K={class:"header-anchor",href:"#â– -ä¸Šæµ·ailab-ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤-https-opendrivelab-com-publications"},X={href:"https://opendrivelab.com/publications/",target:"_blank",rel:"noopener noreferrer"},Q={id:"â– -é¦™æ¸¯ä¸­æ–‡å¤§å­¦-é™ˆé“ è€å¸ˆå›¢é˜Ÿ-geometric-controllable-visual-generation-a-systematic-solution-video",tabindex:"-1"},$={class:"header-anchor",href:"#â– -é¦™æ¸¯ä¸­æ–‡å¤§å­¦-é™ˆé“ è€å¸ˆå›¢é˜Ÿ-geometric-controllable-visual-generation-a-systematic-solution-video"},ee={href:"https://www.bilibili.com/video/BV18T421v7Nf/?spm_id_from=333.337.search-card.all.click",target:"_blank",rel:"noopener noreferrer"},re={id:"â– -æä½³ç§‘æŠ€-æä½³ç§‘æŠ€drivedreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€worlddreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚-æ¨æ–‡",tabindex:"-1"},oe={class:"header-anchor",href:"#â– -æä½³ç§‘æŠ€-æä½³ç§‘æŠ€drivedreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€worlddreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚-æ¨æ–‡"},ne={href:"https://baijiahao.baidu.com/s?id=1799624134723943641",target:"_blank",rel:"noopener noreferrer"},te=e("h4",{id:"â– -wayveã€teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -wayveã€teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€"},[e("span",null,"â–  Wayveã€Teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€")])],-1),le={id:"â– -è”šæ¥è½¦ä¼-https-www-qbitai-com-2024-07-172025-html",tabindex:"-1"},ae={class:"header-anchor",href:"#â– -è”šæ¥è½¦ä¼-https-www-qbitai-com-2024-07-172025-html"},ie={href:"https://www.qbitai.com/2024/07/172025.html",target:"_blank",rel:"noopener noreferrer"},se=l('<h2 id="â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘" tabindex="-1"><a class="header-anchor" href="#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘"><span>â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰</span></a></h2><h4 id="world-models-are-adept-at-representing-an-agent-s-spatio-temporal-knowledge-about-its-environment-through-the-prediction-of-future-changes" tabindex="-1"><a class="header-anchor" href="#world-models-are-adept-at-representing-an-agent-s-spatio-temporal-knowledge-about-its-environment-through-the-prediction-of-future-changes"><span>+ World Models are adept at representing an agent&#39;s spatio-temporal knowledge about its environment through the prediction of future changes</span></a></h4><h4 id="there-are-two-main-types-of-world-models-in-autonomous-driving-aimed-at-reducing-driving-uncertainty-i-e-world-model-as-neural-driving-simulator-and-world-model-for-end-to-end-driving" tabindex="-1"><a class="header-anchor" href="#there-are-two-main-types-of-world-models-in-autonomous-driving-aimed-at-reducing-driving-uncertainty-i-e-world-model-as-neural-driving-simulator-and-world-model-for-end-to-end-driving"><span>+ There are two main types of world models in Autonomous Driving aimed at reducing driving uncertainty, i.e., World Model as Neural Driving Simulator and World Model for End-to-end Driving</span></a></h4>',3),de=l('<figure><img src="'+c+'" alt="world model" tabindex="0" loading="lazy"><figcaption>world model</figcaption></figure><h4 id="in-the-real-environment-methods-like-gaia-1-and-copilot4d-involve-utilizing-generative-models-to-construct-neural-simulators-that-produce-2d-or-3d-future-scenes-to-enhance-predictive-capabilities" tabindex="-1"><a class="header-anchor" href="#in-the-real-environment-methods-like-gaia-1-and-copilot4d-involve-utilizing-generative-models-to-construct-neural-simulators-that-produce-2d-or-3d-future-scenes-to-enhance-predictive-capabilities"><span>+ In the real environment, methods like GAIA-1 and Copilot4D involve utilizing generative models to construct neural simulators that produce 2D or 3D future scenes to enhance predictive capabilities</span></a></h4><h4 id="in-the-simulation-environment-methods-such-as-mile-and-trafficbots-are-based-on-reinforcement-learning-enhancing-their-capacity-for-decision-making-and-future-prediction-thereby-paving-the-way-to-end-to-end-autonomous-driving" tabindex="-1"><a class="header-anchor" href="#in-the-simulation-environment-methods-such-as-mile-and-trafficbots-are-based-on-reinforcement-learning-enhancing-their-capacity-for-decision-making-and-future-prediction-thereby-paving-the-way-to-end-to-end-autonomous-driving"><span>+ In the simulation environment, methods such as MILE and TrafficBots are based on reinforcement learning, enhancing their capacity for decision-making and future prediction, thereby paving the way to end-to-end autonomous driving</span></a></h4>',3),he=l('<figure><img src="'+u+'" alt="world model" tabindex="0" loading="lazy"><figcaption>world model</figcaption></figure><h3 id="â– -neural-driving-simulator-based-on-world-models" tabindex="-1"><a class="header-anchor" href="#â– -neural-driving-simulator-based-on-world-models"><span>â–  Neural Driving Simulator based on World Models</span></a></h3><h4 id="â– -2d-scene-generation" tabindex="-1"><a class="header-anchor" href="#â– -2d-scene-generation"><span>â–  2D Scene Generation</span></a></h4>',3),ce={href:"https://arxiv.org/abs/2309.17080",target:"_blank",rel:"noopener noreferrer"},ue={href:"https://wayve.ai/thinking/scaling-gaia-1/",target:"_blank",rel:"noopener noreferrer"},pe={href:"https://www.youtube.com/watch?v=6x-Xb_uT7ts",target:"_blank",rel:"noopener noreferrer"},_e={href:"https://drivedreamer.github.io/",target:"_blank",rel:"noopener noreferrer"},ge={href:"https://github.com/JeffWang987/DriveDreamer",target:"_blank",rel:"noopener noreferrer"},fe={href:"https://arxiv.org/abs/2311.13549",target:"_blank",rel:"noopener noreferrer"},me={href:"https://arxiv.org/abs/2310.07771",target:"_blank",rel:"noopener noreferrer"},ve={href:"https://panacea-ad.github.io/",target:"_blank",rel:"noopener noreferrer"},be={href:"https://github.com/wenyuqing/panacea",target:"_blank",rel:"noopener noreferrer"},ke={href:"https://drive-wm.github.io/",target:"_blank",rel:"noopener noreferrer"},we={href:"https://github.com/BraveGroup/Drive-WM",target:"_blank",rel:"noopener noreferrer"},De={href:"https://arxiv.org/abs/2312.02934",target:"_blank",rel:"noopener noreferrer"},Pe={href:"https://drivedreamer2.github.io/",target:"_blank",rel:"noopener noreferrer"},Ae={href:"https://github.com/f1yfisher/DriveDreamer2",target:"_blank",rel:"noopener noreferrer"},xe={href:"https://arxiv.org/abs/2403.09630",target:"_blank",rel:"noopener noreferrer"},Me={href:"https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file",target:"_blank",rel:"noopener noreferrer"},ye={href:"https://subjectdrive.github.io/",target:"_blank",rel:"noopener noreferrer"},We=e("h4",{id:"â– -3d-scene-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -3d-scene-generation"},[e("span",null,"â–  3D Scene Generation")])],-1),Ce={href:"https://arxiv.org/abs/2311.01017",target:"_blank",rel:"noopener noreferrer"},Le={href:"https://arxiv.org/abs/2311.16038",target:"_blank",rel:"noopener noreferrer"},Ie={href:"https://github.com/wzzheng/OccWorld",target:"_blank",rel:"noopener noreferrer"},Se={href:"https://arxiv.org/abs/2311.11762",target:"_blank",rel:"noopener noreferrer"},Ge={href:"https://www.zyrianov.org/lidardm/",target:"_blank",rel:"noopener noreferrer"},Ve={href:"https://github.com/vzyrianov/lidardm",target:"_blank",rel:"noopener noreferrer"},Re=e("h4",{id:"â– -4d-pre-training-for-autonomous-driving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -4d-pre-training-for-autonomous-driving"},[e("span",null,"â–  4D Pre-training for Autonomous Driving")])],-1),Ee={href:"https://arxiv.org/pdf/2410.13571",target:"_blank",rel:"noopener noreferrer"},Te={href:"https://arxiv.org/abs/2312.17655",target:"_blank",rel:"noopener noreferrer"},Ne={href:"https://github.com/OpenDriveLab/ViDAR",target:"_blank",rel:"noopener noreferrer"},Oe=e("li",null,[r("ğŸ‘(2024 CVPR) DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving ["),e("a",{href:"XXX"},"Paper"),r("] (PKU)")],-1),ze=e("h3",{id:"â– -end-to-end-driving-based-on-world-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -end-to-end-driving-based-on-world-models"},[e("span",null,"â–  End-to-end Driving based on World Models")])],-1),Fe={href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"},Ue={href:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"},Be={href:"https://github.com/wayveai/mile",target:"_blank",rel:"noopener noreferrer"},je={href:"https://arxiv.org/abs/2210.04017",target:"_blank",rel:"noopener noreferrer"},qe={href:"https://ieeexplore.ieee.org/abstract/document/10161243",target:"_blank",rel:"noopener noreferrer"},Ye={href:"https://arxiv.org/abs/2402.16720",target:"_blank",rel:"noopener noreferrer"},He=l('<figure><img src="'+p+'" alt="world model" tabindex="0" loading="lazy"><figcaption>world model</figcaption></figure><h3 id="â– -æŒ‰æ—¶é—´é¡ºåºæ›´æ–°" tabindex="-1"><a class="header-anchor" href="#â– -æŒ‰æ—¶é—´é¡ºåºæ›´æ–°"><span>â–  æŒ‰æ—¶é—´é¡ºåºæ›´æ–°</span></a></h3><h2 id="papers" tabindex="-1"><a class="header-anchor" href="#papers"><span>Papers</span></a></h2>',3),Je={href:"https://arxiv.org/pdf/2410.13571",target:"_blank",rel:"noopener noreferrer"},Ze={href:"https://arxiv.org/abs/2410.10738",target:"_blank",rel:"noopener noreferrer"},Ke=e("strong",null,[e("code",null,"Dataset")],-1),Xe={href:"https://arxiv.org/abs/2409.16663",target:"_blank",rel:"noopener noreferrer"},Qe=e("strong",null,[e("code",null,"Planning")],-1),$e={href:"https://www.arxiv.org/abs/2409.03272",target:"_blank",rel:"noopener noreferrer"},er={href:"https://arxiv.org/pdf/2408.14197",target:"_blank",rel:"noopener noreferrer"},rr=e("strong",null,[e("code",null,"ECCV 2024")],-1),or={href:"https://arxiv.org/pdf/2407.15843",target:"_blank",rel:"noopener noreferrer"},nr=e("strong",null,[e("code",null,"arxiv")],-1),tr={href:"https://arxiv.org/pdf/2407.05679v1",target:"_blank",rel:"noopener noreferrer"},lr=e("strong",null,[e("code",null,"arxiv")],-1),ar=e("strong",null,[e("code",null,"Planning")],-1),ir={href:"https://arxiv.org/pdf/2406.10714",target:"_blank",rel:"noopener noreferrer"},sr={href:"https://arxiv.org/pdf/2406.08691",target:"_blank",rel:"noopener noreferrer"},dr={href:"https://arxiv.org/pdf/2406.08481",target:"_blank",rel:"noopener noreferrer"},hr={href:"https://arxiv.org/ab/2405.20337",target:"_blank",rel:"noopener noreferrer"},cr={href:"https://github.com/wzzheng/OccSora",target:"_blank",rel:"noopener noreferrer"},ur={href:"https://arxiv.org/abs/2406.01349",target:"_blank",rel:"noopener noreferrer"},pr=e("strong",null,[e("code",null,"NeurIPS 2024")],-1),_r=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),gr={href:"https://arxiv.org/pdf/2405.17398",target:"_blank",rel:"noopener noreferrer"},fr=e("strong",null,[e("code",null,"CVPR 2024")],-1),mr={href:"https://arxiv.org/pdf/2405.04390",target:"_blank",rel:"noopener noreferrer"},vr=e("strong",null,[e("code",null,"CVPR 2024")],-1),br=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),kr={href:"https://arxiv.org/abs/2310.08370",target:"_blank",rel:"noopener noreferrer"},wr={href:"https://github.com/Nightmare-n/UniPAD",target:"_blank",rel:"noopener noreferrer"},Dr=e("strong",null,[e("code",null,"CVPR 2024")],-1),Pr=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),Ar={href:"https://arxiv.org/pdf/2403.09630.pdf",target:"_blank",rel:"noopener noreferrer"},xr=e("strong",null,[e("code",null,"arxiv")],-1),Mr={href:"https://arxiv.org/pdf/2402.16720.pdf",target:"_blank",rel:"noopener noreferrer"},yr=e("strong",null,[e("code",null,"CVPR 2024")],-1),Wr=e("strong",null,[e("code",null,"Pre-training")],-1),Cr=e("strong",null,[e("code",null,"from Shanghai AI Lab")],-1),Lr=e("strong",null,[e("code",null,"NuScenes dataset")],-1),Ir={href:"https://arxiv.org/pdf/2312.17655",target:"_blank",rel:"noopener noreferrer"},Sr={href:"https://github.com/OpenDriveLab/ViDAR",target:"_blank",rel:"noopener noreferrer"},Gr=e("strong",null,[e("code",null,"ICLR 2024")],-1),Vr=e("strong",null,[e("code",null,"Future Prediction")],-1),Rr=e("strong",null,[e("code",null,"from Waabi")],-1),Er=e("strong",null,[e("code",null,"NuScenes, KITTI Odemetry, Argoverse2 Lidar datasets")],-1),Tr={href:"https://arxiv.org/abs/2311.01017",target:"_blank",rel:"noopener noreferrer"},Nr=e("strong",null,[e("code",null,"arxiv")],-1),Or=e("strong",null,[e("code",null,"Generative AI")],-1),zr={href:"https://arxiv.org/pdf/2310.07771.pdf",target:"_blank",rel:"noopener noreferrer"},Fr={href:"https://github.com/shalfun/DrivingDiffusion",target:"_blank",rel:"noopener noreferrer"},Ur=e("strong",null,[e("code",null,"arxiv")],-1),Br=e("strong",null,[e("code",null,"Pre-training")],-1),jr=e("strong",null,[e("code",null,"CARLA dataset")],-1),qr={href:"https://arxiv.org/pdf/2311.11762.pdf",target:"_blank",rel:"noopener noreferrer"},Yr=e("strong",null,[e("code",null,"arxiv")],-1),Hr=e("strong",null,[e("code",null,"Generative AI, Planning")],-1),Jr=e("strong",null,[e("code",null,"NuScenes and Waymo datasets")],-1),Zr={href:"https://arxiv.org/pdf/2311.17918.pdf",target:"_blank",rel:"noopener noreferrer"},Kr=e("strong",null,[e("code",null,"arxiv")],-1),Xr=e("strong",null,[e("code",null,"Generative AI")],-1),Qr=e("strong",null,[e("code",null,"NuScenes & one private dataset")],-1),$r={href:"https://arxiv.org/pdf/2311.13549.pdf",target:"_blank",rel:"noopener noreferrer"},eo=e("strong",null,[e("code",null,"arxiv")],-1),ro=e("strong",null,[e("code",null,"Occupancy Future Prediction, Planning")],-1),oo=e("strong",null,[e("code",null,"Occ3D dataset for Occupancy Future Prediction, NuScenes for motion planning")],-1),no={href:"https://arxiv.org/pdf/2311.16038.pdf",target:"_blank",rel:"noopener noreferrer"},to={href:"https://github.com/wzzheng/OccWorld",target:"_blank",rel:"noopener noreferrer"},lo=e("strong",null,[e("code",null,"arxiv")],-1),ao=e("strong",null,[e("code",null,"Generative AI")],-1),io=e("strong",null,[e("code",null,"Wayve's private data")],-1),so={href:"https://arxiv.org/pdf/2309.17080.pdf",target:"_blank",rel:"noopener noreferrer"},ho={span:""},co={href:"https://proceedings.neurips.cc/paper_files/paper/2022/file/b2fe1ee8d936ac08dd26f2ff58986c8f-Paper-Conference.pdf",target:"_blank",rel:"noopener noreferrer"},uo={href:"https://github.com/plai-group/flexible-video-diffusion-modeling",target:"_blank",rel:"noopener noreferrer"},po={href:"https://www.youtube.com/watch?v=cS6JQpEY9cs",target:"_blank",rel:"noopener noreferrer"},_o={href:"https://www.youtube.com/watch?v=687zEGODmHA",target:"_blank",rel:"noopener noreferrer"},go={href:"https://www.youtube.com/watch?v=pea3sH6orMc",target:"_blank",rel:"noopener noreferrer"},fo=e("strong",null,[e("code",null,"arxiv")],-1),mo=e("strong",null,[e("code",null,"Generative AI")],-1),vo=e("strong",null,[e("code",null,"NuScenes dataset")],-1),bo={href:"https://arxiv.org/pdf/2309.09777.pdf",target:"_blank",rel:"noopener noreferrer"},ko={href:"https://github.com/JeffWang987/DriveDreamer",target:"_blank",rel:"noopener noreferrer"},wo=e("strong",null,"'PhD Thesis'",-1),Do=e("strong",null,[e("code",null,"from Wayve")],-1),Po={href:"https://arxiv.org/pdf/2306.09179",target:"_blank",rel:"noopener noreferrer"},Ao=e("strong",null,[e("code",null,"arxiv")],-1),xo=e("strong",null,[e("code",null,"Pre-training")],-1),Mo=e("strong",null,[e("code",null,"NuScenes dataset")],-1),yo={href:"https://arxiv.org/pdf/2308.07234.pdf",target:"_blank",rel:"noopener noreferrer"},Wo=e("strong",null,[e("code",null,"ICLR 2022 workshop on Generalizable Policy Learning in the Physical World")],-1),Co=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Lo={href:"https://arxiv.org/abs/2204.07184",target:"_blank",rel:"noopener noreferrer"},Io={href:"https://github.com/vladisai/pytorch-ppuu",target:"_blank",rel:"noopener noreferrer"},So=e("strong",null,[e("code",null,"NeurIPS 2022 Deep Reinforcement Learning Workshop")],-1),Go=e("strong",null,[e("code",null,"RL")],-1),Vo=e("strong",null,[e("code",null,"CARLA dataset")],-1),Ro={href:"https://arxiv.org/pdf/2210.04017.pdf",target:"_blank",rel:"noopener noreferrer"},Eo=e("strong",null,[e("code",null,"NeurIPS 2022")],-1),To=e("strong",null,[e("code",null,"RL")],-1),No=e("strong",null,[e("code",null,"from Wayve")],-1),Oo={href:"https://arxiv.org/pdf/2210.07729.pdf",target:"_blank",rel:"noopener noreferrer"},zo={href:"https://github.com/wayveai/mile",target:"_blank",rel:"noopener noreferrer"},Fo=e("strong",null,[e("code",null,"NeurIPS 2022")],-1),Uo={href:"https://arxiv.org/pdf/2205.13817.pdf",target:"_blank",rel:"noopener noreferrer"},Bo={href:"https://github.com/panmt/iso-dream",target:"_blank",rel:"noopener noreferrer"},jo=e("strong",null,[e("code",null,"ICCV 2019")],-1),qo=e("strong",null,[e("code",null,"Future Prediction")],-1),Yo=e("strong",null,[e("code",null,"from Wayve")],-1),Ho=e("strong",null,[e("code",null,"NuScenes, Lyft datasets")],-1),Jo={href:"https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf",target:"_blank",rel:"noopener noreferrer"},Zo={href:"https://github.com/wayveai/fiery",target:"_blank",rel:"noopener noreferrer"},Ko=e("strong",null,[e("code",null,"CVPR 2021 Oral")],-1),Xo=e("strong",null,[e("code",null,"RL")],-1),Qo={href:"https://arxiv.org/pdf/2105.00636.pdf",target:"_blank",rel:"noopener noreferrer"},$o={href:"https://dotchen.github.io/world_on_rails/",target:"_blank",rel:"noopener noreferrer"},en={href:"https://github.com/dotchen/WorldOnRails",target:"_blank",rel:"noopener noreferrer"},rn=e("strong",null,[e("code",null,"ICLR 2019")],-1),on=e("strong",null,[e("code",null,"Future Prediction")],-1),nn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),tn={href:"https://github.com/Atcold/pytorch-PPUU?tab=readme-ov-file",target:"_blank",rel:"noopener noreferrer"},ln={href:"https://github.com/Atcold/pytorch-PPUU",target:"_blank",rel:"noopener noreferrer"},an=e("h2",{id:"other-general-world-model-papers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#other-general-world-model-papers"},[e("span",null,"Other General World Model Papers")])],-1),sn={href:"https://arxiv.org/pdf/2405.18418",target:"_blank",rel:"noopener noreferrer"},dn={href:"https://world-model.maitrix.org/assets/pandora.pdf",target:"_blank",rel:"noopener noreferrer"},hn=e("li",null,[r("2024-Efficient World Models with Time-Aware and Context-Augmented Tokenization "),e("strong",null,[e("code",null,"ICML 2024")])],-1),cn=e("strong",null,[e("code",null,"ICML 2024")],-1),un={href:"https://arxiv.org/pdf/2403.09631.pdf",target:"_blank",rel:"noopener noreferrer"},pn=e("strong",null,[e("code",null,"website")],-1),_n={href:"https://www.archetypeai.io/blog/introducing-archetype-ai---understand-the-real-world-in-real-time",target:"_blank",rel:"noopener noreferrer"},gn=e("strong",null,[e("code",null,"arxiv")],-1),fn={href:"https://arxiv.org/pdf/2404.05014.pdf",target:"_blank",rel:"noopener noreferrer"},mn={href:"https://github.com/PKU-YuanGroup/MagicTime",target:"_blank",rel:"noopener noreferrer"},vn=e("strong",null,[e("code",null,"arxiv")],-1),bn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),kn={href:"https://arxiv.org/pdf/2403.00504.pdf",target:"_blank",rel:"noopener noreferrer"},wn=e("strong",null,[e("code",null,"arxiv")],-1),Dn=e("strong",null,[e("code",null,"Deepmind")],-1),Pn={href:"https://arxiv.org/abs/2402.17139",target:"_blank",rel:"noopener noreferrer"},An=e("strong",null,[e("code",null,"Deepmind")],-1),xn={href:"https://arxiv.org/abs/2402.15391v1",target:"_blank",rel:"noopener noreferrer"},Mn={href:"https://sites.google.com/view/genie-2024/home",target:"_blank",rel:"noopener noreferrer"},yn=e("strong",null,[e("code",null,"OpenAI")],-1),Wn=e("strong",null,[e("code",null,"Generative AI")],-1),Cn={href:"https://openai.com/sora",target:"_blank",rel:"noopener noreferrer"},Ln={href:"https://openai.com/research/video-generation-models-as-world-simulators",target:"_blank",rel:"noopener noreferrer"},In=e("strong",null,[e("code",null,"arxiv")],-1),Sn=e("strong",null,[e("code",null,"Generative AI")],-1),Gn={href:"https://arxiv.org/abs/2402.08268",target:"_blank",rel:"noopener noreferrer"},Vn={href:"https://github.com/LargeWorldModel/LWM",target:"_blank",rel:"noopener noreferrer"},Rn=e("strong",null,[e("code",null,"arxiv")],-1),En=e("strong",null,[e("code",null,"Generative AI")],-1),Tn={href:"https://arxiv.org/abs/2401.09985",target:"_blank",rel:"noopener noreferrer"},Nn=e("strong",null,[e("code",null,"NeurIPS 2024")],-1),On={href:"https://proceedings.neurips.cc/paper_files/paper/2023/file/d9042abf40782fbce28901c1c9c0e8d8-Paper-Conference.pdf",target:"_blank",rel:"noopener noreferrer"},zn={href:"https://github.com/Alescontrela/viper_rl",target:"_blank",rel:"noopener noreferrer"},Fn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Un={href:"https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",target:"_blank",rel:"noopener noreferrer"},Bn={href:"https://github.com/facebookresearch/jepa",target:"_blank",rel:"noopener noreferrer"},jn=e("strong",null,[e("code",null,"NeurIPS 2023")],-1),qn={href:"https://arxiv.org/abs/2307.02064",target:"_blank",rel:"noopener noreferrer"},Yn=e("strong",null,[e("code",null,"CVPR 2023")],-1),Hn=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Jn={href:"https://arxiv.org/abs/2301.08243",target:"_blank",rel:"noopener noreferrer"},Zn={href:"https://github.com/facebookresearch/ijepa",target:"_blank",rel:"noopener noreferrer"},Kn=e("strong",null,[e("code",null,"ICML 2023")],-1),Xn={href:"https://arxiv.org/abs/2210.02396",target:"_blank",rel:"noopener noreferrer"},Qn={href:"https://github.com/wilson1yan/teco",target:"_blank",rel:"noopener noreferrer"},$n=e("strong",null,[e("code",null,"arxiv")],-1),et={href:"https://arxiv.org/abs/2308.01399",target:"_blank",rel:"noopener noreferrer"},rt={href:"https://github.com/jlin816/dynalang",target:"_blank",rel:"noopener noreferrer"},ot=e("strong",null,[e("code",null,"ICLR 2023")],-1),nt=e("strong",null,[e("code",null,"RL")],-1),tt={href:"https://arxiv.org/pdf/2209.00588.pdf",target:"_blank",rel:"noopener noreferrer"},lt={href:"https://github.com/eloialonso/iris",target:"_blank",rel:"noopener noreferrer"},at=e("strong",null,[e("code",null,"arxiv")],-1),it=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),st=e("strong",null,[e("code",null,"Planning")],-1),dt={href:"https://arxiv.org/pdf/2312.17227",target:"_blank",rel:"noopener noreferrer"},ht=e("strong",null,[e("code",null,"arxiv")],-1),ct=e("strong",null,[e("code",null,"RL")],-1),ut={href:"https://arxiv.org/pdf/2312.08533.pdf",target:"_blank",rel:"noopener noreferrer"},pt=e("strong",null,[e("code",null,"arxiv")],-1),_t=e("strong",null,[e("code",null,"RL")],-1),gt={href:"https://arxiv.org/abs/2301.04104",target:"_blank",rel:"noopener noreferrer"},ft={href:"https://github.com/danijar/dreamerv3",target:"_blank",rel:"noopener noreferrer"},mt=e("strong",null,[e("code",null,"CoRL 2022")],-1),vt=e("strong",null,[e("code",null,"Robotics")],-1),bt={href:"https://arxiv.org/abs/2206.14176",target:"_blank",rel:"noopener noreferrer"},kt={href:"https://github.com/danijar/daydreamer",target:"_blank",rel:"noopener noreferrer"},wt=e("strong",null,[e("code",null,"CoRL 2022")],-1),Dt=e("strong",null,[e("code",null,"Robotics")],-1),Pt={href:"https://proceedings.mlr.press/v205/seo23a.html",target:"_blank",rel:"noopener noreferrer"},At={href:"https://github.com/younggyoseo/MWM",target:"_blank",rel:"noopener noreferrer"},xt=e("strong",null,[e("code",null,"openreview")],-1),Mt=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),yt=e("strong",null,[e("code",null,"General Roadmap for World Models")],-1),Wt={href:"https://openreview.net/forum?id=BZ5a1r-kVsf",target:"_blank",rel:"noopener noreferrer"},Ct={href:"https://leshouches2022.github.io/SLIDES/compressed-yann-1.pdf",target:"_blank",rel:"noopener noreferrer"},Lt={href:"https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-02.pdf",target:"_blank",rel:"noopener noreferrer"},It={href:"https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-03.pdf",target:"_blank",rel:"noopener noreferrer"},St={href:"https://www.youtube.com/playlist?list=PLEIq5bchE3R3Yl5taXdYA04a9kH9yvyGm",target:"_blank",rel:"noopener noreferrer"},Gt=e("strong",null,[e("code",null,"NeurIPS 2021")],-1),Vt={href:"https://proceedings.neurips.cc/paper_files/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html",target:"_blank",rel:"noopener noreferrer"},Rt={href:"https://orybkin.github.io/lexa/",target:"_blank",rel:"noopener noreferrer"},Et=e("strong",null,[e("code",null,"ICLR 2021")],-1),Tt=e("strong",null,[e("code",null,"RL")],-1),Nt=e("strong",null,[e("code",null,"from Google & Deepmind")],-1),Ot={href:"https://arxiv.org/pdf/2010.02193.pdf",target:"_blank",rel:"noopener noreferrer"},zt={href:"https://github.com/danijar/dreamerv2",target:"_blank",rel:"noopener noreferrer"},Ft=e("strong",null,[e("code",null,"ICLR 2020")],-1),Ut={href:"https://arxiv.org/abs/1912.01603",target:"_blank",rel:"noopener noreferrer"},Bt={href:"https://github.com/google-research/dreamer",target:"_blank",rel:"noopener noreferrer"},jt=e("strong",null,[e("code",null,"ICML 2019")],-1),qt={href:"https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf",target:"_blank",rel:"noopener noreferrer"},Yt={href:"https://github.com/google-research/planet",target:"_blank",rel:"noopener noreferrer"},Ht=e("strong",null,[e("code",null,"arxiv")],-1),Jt=e("strong",null,[e("code",null,"RL, Planning")],-1),Zt=e("strong",null,[e("code",null,"from Yann Lecun's Group")],-1),Kt={href:"https://arxiv.org/pdf/1705.07177",target:"_blank",rel:"noopener noreferrer"},Xt=e("strong",null,[e("code",null,"NeurIPS 2018")],-1),Qt={href:"https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf",target:"_blank",rel:"noopener noreferrer"},$t={href:"https://github.com/hardmaru/WorldModelsExperiments",target:"_blank",rel:"noopener noreferrer"},el=e("h2",{id:"â– -â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘"},[e("span",null,"â–  â¢ å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘-->")])],-1),rl=e("p",null,"ç”Ÿæˆå¼çš„World Modelå¯ä»¥è¢«ç”¨æ¥å½“ä½œä¸€ç§ä»¿çœŸå·¥å…·æ¥ç”Ÿæˆä»¿çœŸæ•°æ®ï¼Œç‰¹åˆ«æ˜¯æä¸ºå°‘è§çš„Corner Caseçš„æ•°æ®ã€‚ç‰¹åˆ«æ˜¯åŸºäºText to image çš„å¯æ§æ¡ä»¶ç”ŸæˆCorner Caseï¼Œå¯ä»¥è¿›è¡Œæ•°æ®å¢å¹¿ï¼Œè§£å†³çœŸå®æ•°æ®ä¸”æ ‡æ³¨å°‘çš„ç°å­˜é—®é¢˜ã€‚ ç„¶è€ŒWorld Modelæ›´æœ‰æ½œåŠ›çš„åº”ç”¨æ–¹å‘æ˜¯World Modelå¯èƒ½ä¼šæˆä¸ºåƒGPTä¸€æ ·çš„è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼Œè€Œå…¶ä»–è‡ªåŠ¨é©¾é©¶å…·ä½“ä»»åŠ¡éƒ½ä¼šå›´ç»•è¿™ä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œç ”å‘æ„å»ºã€‚ é‡ç‚¹é˜…è¯»vista è¿˜æœ‰ä¸€ç¯‡æ˜¯æ–°å‡ºçš„æ•°æ®é›†å¯ä»¥è¿›è¡Œå¤ç°ã€‚",-1),ol=e("h2",{id:"â– -â¢å¯æ§æ¡ä»¶ç”Ÿæˆ",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -â¢å¯æ§æ¡ä»¶ç”Ÿæˆ"},[e("span",null,"â–  â¢å¯æ§æ¡ä»¶ç”Ÿæˆ-->")])],-1),nl={href:"https://github.com/cure-lab/MagicDrive",target:"_blank",rel:"noopener noreferrer"},tl={href:"https://arxiv.org/abs/2310.02601",target:"_blank",rel:"noopener noreferrer"},ll={href:"https://github.com/cure-lab/MagicDrive",target:"_blank",rel:"noopener noreferrer"},al={href:"https://arxiv.org/abs/2405.14475",target:"_blank",rel:"noopener noreferrer"},il={href:"https://github.com/flymin/MagicDrive3D",target:"_blank",rel:"noopener noreferrer"},sl={href:"https://zhuanlan.zhihu.com/p/684249231",target:"_blank",rel:"noopener noreferrer"},dl={href:"https://drive-wm.github.io/",target:"_blank",rel:"noopener noreferrer"},hl=e("li",null,"â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->Geodiffusion",-1),cl=e("li",null,"â—‹ -->Detdiffusion",-1),ul=e("li",null,"â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->BevControl",-1),pl=e("li",null,"â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->BevControl",-1),_l=e("li",null,"â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ--PerLDiff",-1),gl=e("li",null,"â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆå­¦ä¹ å¹³å°--CarDreamer",-1),fl=e("li",null,"â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆå­¦ä¹ å¹³å°--DriveArena",-1),ml=e("h2",{id:"â– -â¢occä¸–ç•Œæ¨¡å‹",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#â– -â¢occä¸–ç•Œæ¨¡å‹"},[e("span",null,"â–  â¢occä¸–ç•Œæ¨¡å‹-->")])],-1),vl=e("strong",null,[e("code",null,"ITSC 2023")],-1),bl=e("strong",null,[e("code",null,"Planning, Neural Predicted-Guided Planning")],-1),kl=e("strong",null,[e("code",null,"Waymo Open Motion dataset")],-1),wl={href:"https://arxiv.org/abs/2305.03303",target:"_blank",rel:"noopener noreferrer"},Dl=e("ul",null,[e("li",null,"ä¸occç»“åˆçš„ï¼šoccworldã€Drive-occwordã€OccLLaMAã€Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Drivingã€OccSora")],-1),Pl=e("h3",{id:"_1-definition",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-definition"},[e("span",null,"1. DEFINITION")])],-1),Al=e("p",null,"æ¯ç¯‡æ–‡ç« çš„åˆ›æ–°ç‚¹ä»å•ä¸€å¤šè§†è§’åˆ°ç¯è§†è§†è§’ã€ä»å•ä¸€è¾“å…¥åˆ°å¤šæ¨¡æ€è¾“å…¥æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¿˜æœ‰è§†é¢‘ç”Ÿæˆçš„æ—¶é•¿ï¼Œä»¥åŠè½¨è¿¹é¢„æµ‹ï¼ŒåŸºäºå†³ç­–çš„ä¸–ç•Œæ¨¡å‹æ–¹æ³•ã€‚éœ€è§£å†³æ—¶ç©ºä¸ä¸€è‡´æ€§å’Œç”Ÿæˆåœºæ™¯è¿ç»­æ€§ã€‚",-1);function xl(Ml,yl){const o=i("ExternalLinkIcon");return s(),d("div",null,[g,e("p",null,[r("[1] "),e("a",f,[r("https://github.com/GigaAI-research/General-World-Models-Survey"),n(o)]),r(" è¯¥ repo å†…æœ‰ç›®å‰ä¸–ç•Œæ¨¡å‹æ–¹å‘çš„ä¼˜ç§€è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ï¼šè§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶å’Œè‡ªä¸»ä»£ç†ã€‚å…¶ä¸­è‡ªåŠ¨é©¾é©¶åˆ†æˆç«¯åˆ°ç«¯ã€ä»¥åŠ2Dã€3Dç¥ç»æ¨¡æ‹Ÿå™¨æ–¹æ³•ã€‚ä¸–ç•Œæ¨¡å‹çš„æ–‡çŒ®ã€ å¼€æºcodeã€ ç»¼è¿°ã€‚")]),e("p",null,[r("[2] "),e("a",m,[r("https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey"),n(o)]),r(" è¯¥repo å†…ä»¥â€˜æ—¶é—´â€™ä¸ºé¡ºåºç²¾é€‰ç›¸å…³ä¸–ç•Œè‡ªåŠ¨é©¾é©¶æ¨¡å‹ã€‚ä¸”å¹¶æŒç»­æ›´æ–°ï¼ŒåŒ…æ‹¬ä¸€äº›æŒ‘æˆ˜ã€ç›¸å…³è§†é¢‘ï¼ŒåŒ…æ‹¬æœºå™¨äººé¢†åŸŸçš„ä¸–ç•Œæ¨¡å‹ä½¿ç”¨ï¼ˆå¤§å¤šæ•°ä¸ºæ¨¡ä»¿å­¦ä¹ å¼ºåŒ–å­¦ä¹ æ–¹å‘ï¼‰å¯å‚è€ƒå€Ÿé‰´ã€‚")]),e("p",null,[r("[3] "),e("a",v,[r("Awesome-World-Models-for-AD"),n(o)])]),e("p",null,[r("[4] "),e("a",b,[r("World models paper list from Shanghai AI lab"),n(o)])]),e("p",null,[r("[5] "),e("a",k,[r("Awesome-Papers-World-Models-Autonomous-Driving"),n(o)]),r(".")]),w,t("è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/world_model.png" width="20%" />
</p> `),D,P,e("p",null,[r("[1] ä¸–ç•Œæ¨¡å‹ç®€ä»‹ï¼š"),e("a",A,[r("https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw"),n(o)]),r("ä¸–ç•Œæ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤šç§æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¦‚è§†è§‰ã€å¬è§‰å’Œè¯­è¨€ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰æ–¹æ³•æ¥ç†è§£å’Œé¢„æµ‹ç°å®ä¸–ç•Œã€‚å®ƒåŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—ã€è¡¨å¾å­¦ä¹ ã€åŠ¨åŠ›å­¦æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºæ„å»ºç¯å¢ƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œä¸ä»…èƒ½åæ˜ å½“å‰çŠ¶æ€ï¼Œè¿˜èƒ½é¢„æµ‹æœªæ¥å˜åŒ–ã€‚è¿™ç§æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆå¼€å‘å’Œæœºå™¨äººå­¦ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚Yann LeCunæå‡ºçš„è¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒé€šè¿‡è‡ªç›‘ç£å­¦ä¹ è®©AIåƒäººä¸€æ ·ç†è§£ä¸–ç•Œï¼Œå½¢æˆå†…éƒ¨çš„å¿ƒç†è¡¨å¾ï¼Œä»¥æœŸå®ç°é€šç”¨äººå·¥æ™ºèƒ½ã€‚Metaçš„I-JEPAæ¨¡å‹æ˜¯åŸºäºè¿™ä¸€æ„¿æ™¯çš„å®ç°ï¼Œå®ƒé€šè¿‡åˆ†æå’Œè¡¥å…¨å›¾åƒå±•ç¤ºäº†å¯¹ä¸–ç•ŒèƒŒæ™¯çŸ¥è¯†çš„åº”ç”¨ã€‚")]),e("p",null,[r("[2] å½±å“è¾ƒå¤§çš„æ—©æœŸä¸–ç•Œæ¨¡å‹æ–‡ç« ï¼š2018å¹´Jurgenåœ¨NeurIPS ä»¥å¾ªç¯ä¸–ç•Œæ¨¡å‹ä¿ƒè¿›ç­–ç•¥æ¼”å˜â€œRecurrent World Models Facilitate Policy Evolutionâ€çš„titleå‘è¡¨ï¼šé“¾æ¥: "),e("a",x,[r("https://arxiv.org/abs/1803.10122"),n(o)]),r(" ç¤ºä¾‹: "),e("a",M,[r("https://worldmodels.github.io/"),n(o)])]),e("p",null,[r("[3] ä¸–ç•Œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ï¼š "),e("a",y,[r("https://www.bilibili.com/read/cv34465959/"),n(o)])]),e("p",null,[r("[4] ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆä»¥åŠä»¿çœŸå¹³å°: "),e("a",W,[r("https://blog.csdn.net/CV_Autobot/article/details/134002647"),n(o)])]),C,e("p",null,[r("[1] 2024-Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyondï¼Œ "),L,r(),e("a",I,[r("Paper"),n(o)]),r(" æä½³ç§‘æŠ€ ï¼ˆæ¯”è¾ƒå…¨é¢ï¼Œè¯¥ç»¼è¿°é€šè¿‡ 260 ä½™ç¯‡æ–‡çŒ®ï¼Œå¯¹ä¸–ç•Œæ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ä½“ã€é€šç”¨æœºå™¨äººç­‰é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨è¿›è¡Œäº†è¯¦å°½çš„åˆ†æå’Œè®¨è®ºã€‚å¦å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å®¡è§†äº†å½“å‰ä¸–ç•Œæ¨¡å‹çš„æŒ‘æˆ˜å’Œå±€é™æ€§ï¼Œå¹¶å±•æœ›äº†å®ƒä»¬æœªæ¥çš„å‘å±•æ–¹å‘ã€‚ï¼‰")]),e("p",null,[r("[2] 2024-World Models for Autonomous Driving: An Initial Surveyï¼ŒIEEE TIV,æ¾³é—¨å¤§å­¦ï¼Œå¤å¨å¤·å¤§å­¦ã€‚"),e("a",S,[r("Paper"),n(o)]),r("ï¼ˆç”»é£æœ‰è¶£ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„ç°çŠ¶å’Œæœªæ¥è¿›å±•è¿›è¡Œäº†åˆæ­¥å›é¡¾ï¼Œæ¶µç›–äº†å®ƒä»¬çš„ç†è®ºåŸºç¡€ã€å®é™…åº”ç”¨ä»¥åŠæ—¨åœ¨å…‹æœç°æœ‰å±€é™æ€§çš„æ­£åœ¨è¿›è¡Œçš„ç ”ç©¶å·¥ä½œã€‚ï¼‰")]),e("p",null,[r("[3]2024-Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies "),G,r(),e("a",V,[r("Paper"),n(o)])]),e("p",null,[r("[4]2024-Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities "),R,r(),e("a",E,[r("Paper"),n(o)])]),T,e("ul",null,[e("li",null,[e("p",null,[r("2024-1X World Model Challenge "),N,r(),e("a",O,[r("Link"),n(o)])])]),e("li",null,[e("p",null,[r("2024-ECCV Corner case Challenge "),z,r(),e("a",F,[r("Link"),n(o)])])]),e("li",null,[e("p",null,[r("2024-CVPR Workshop, Foundation Models for Autonomous Systems, Challenges, Track 4: Predictive World Model "),U,r(),e("a",B,[r("Link"),n(o)])])])]),j,e("ul",null,[e("li",null,[e("p",null,[r("2023 "),q,r("; "),e("a",Y,[r("Video"),n(o)])])]),e("li",null,[e("p",null,[r("2022-Neural World Models for Autonomous Driving "),e("a",H,[r("Video"),n(o)])])])]),J,e("h4",Z,[e("a",K,[e("span",null,[r("â–  ä¸Šæµ·AILabï¼ˆä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼‰ "),e("a",X,[r("https://opendrivelab.com/publications/"),n(o)])])])]),e("h4",Q,[e("a",$,[e("span",null,[r("â–  é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆé™ˆé“ è€å¸ˆå›¢é˜Ÿï¼‰Geometric-Controllable Visual Generation: A Systematic Solution "),e("a",ee,[r("Video"),n(o)])])])]),e("h4",re,[e("a",oe,[e("span",null,[r("â–  æä½³ç§‘æŠ€ï¼ˆæä½³ç§‘æŠ€DriveDreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€WorldDreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚ï¼‰"),e("a",ne,[r("æ¨æ–‡"),n(o)])])])]),te,e("h4",le,[e("a",ae,[e("span",null,[r("â–  è”šæ¥è½¦ä¼ï¼š"),e("a",ie,[r("https://www.qbitai.com/2024/07/172025.html"),n(o)])])])]),se,t("è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/End to end.png" width="100%" />
</p> `),de,t("è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/Generation.png" width="100%" />
</p> `),he,e("ul",null,[e("li",null,[r("ğŸ‘(2023 Arxiv) GAIA-1: A generative world model for autonomous driving ["),e("a",ce,[r("Paper"),n(o)]),r("]["),e("a",ue,[r("Blog"),n(o)]),r("] (Wayve)")])]),e("ul",null,[e("li",null,[r("(2023 CVPR 2023 workshop) ["),e("a",pe,[r("Video"),n(o)]),r("] (Tesla)")]),e("li",null,[r("ğŸ‘(2023 Arxiv) DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving ["),e("a",_e,[r("Paper"),n(o)]),r("]["),e("a",ge,[r("Code"),n(o)]),r("] (GigaAI)")]),e("li",null,[r("(2023 Arxiv) ADriver-I: A General World Model for Autonomous Driving ["),e("a",fe,[r("Paper"),n(o)]),r("] (MEGVII)")]),e("li",null,[r("ğŸ‘(2023 Arxiv) DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model ["),e("a",me,[r("Paper"),n(o)]),r("] (Baidu)")]),e("li",null,[r("(2023 Arxiv) Panacea: Panoramic and Controllable Video Generation for Autonomous Driving ["),e("a",ve,[r("Paper"),n(o)]),r("]["),e("a",be,[r("Code"),n(o)]),r("] (MEGVII)")]),e("li",null,[r("ğŸ‘(2024 CVPR) Drive-WM: Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving ["),e("a",ke,[r("Paper"),n(o)]),r("]["),e("a",we,[r("Code"),n(o)]),r("] (CASIA)")]),e("li",null,[r("(2023 Arxiv) WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation ["),e("a",De,[r("Paper"),n(o)]),r("] (Fudan)")]),e("li",null,[r("(2024 Arxiv) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation ["),e("a",Pe,[r("Paper"),n(o)]),r("]["),e("a",Ae,[r("Code"),n(o)]),r("] (GigaAI)")]),e("li",null,[r("(2024 CVPR) GenAD: Generalized Predictive Model for Autonomous Driving ["),e("a",xe,[r("Paper"),n(o)]),r("]["),e("a",Me,[r("Code"),n(o)]),r("] (Shanghai AI Lab)")]),e("li",null,[r("(2024 Arxiv) SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control ["),e("a",ye,[r("Paper"),n(o)]),r("] (MEGVII)")])]),We,e("ul",null,[e("li",null,[r("ğŸ‘(2024 ICLR) Copilot4D:Learning unsupervised world models for autonomous driving via discrete diffusion ["),e("a",Ce,[r("Paper"),n(o)]),r("] (Waabi)")])]),e("ul",null,[e("li",null,[r("(2023 Arxiv) OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving ["),e("a",Le,[r("Paper"),n(o)]),r("]["),e("a",Ie,[r("Code"),n(o)]),r("] (THU)")]),e("li",null,[r("(2023 Arxiv) MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations ["),e("a",Se,[r("Paper"),n(o)]),r("] (KIT)")]),e("li",null,[r("(2024 Arxiv) LidarDM: Generative LiDAR Simulation in a Generated World ["),e("a",Ge,[r("Paper"),n(o)]),r("]["),e("a",Ve,[r("Code"),n(o)]),r("] (MIT)")])]),Re,e("ul",null,[e("li",null,[r("(2024 Arxiv)-DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation "),e("a",Ee,[r("Paper"),n(o)])]),e("li",null,[r("(2024 CVPR) ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving ["),e("a",Te,[r("Paper"),n(o)]),r("]["),e("a",Ne,[r("Code"),n(o)]),r("] (Shanghai AI Lab)")]),Oe]),ze,e("ul",null,[e("li",null,[r("ğŸ‘(2022 NeurIPS) Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models ["),e("a",Fe,[r("Paper"),n(o)]),r("] (SJTU)")])]),e("ul",null,[e("li",null,[r("ğŸ‘(2022 NeurIPS) MILE: Model-Based Imitation Learning for Urban Driving ["),e("a",Ue,[r("Paper"),n(o)]),r("]["),e("a",Be,[r("Code"),n(o)]),r("] (Wayve)")]),e("li",null,[r("(2022 NeurIPS Deep RL Workshop) SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model ["),e("a",je,[r("Paper"),n(o)]),r("] (HIT & THU)")]),e("li",null,[r("(2023 ICRA) TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction ["),e("a",qe,[r("Paper"),n(o)]),r("] (ETH Zurich)")]),e("li",null,[r("(2024 Arxiv) Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2) ["),e("a",Ye,[r("Paper"),n(o)]),r("] (SJTU)")])]),t("è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­"),t(` <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/ad.png" width="30%" />
</p> `),He,e("ul",null,[e("li",null,[e("p",null,[r("2024-DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation "),e("a",Je,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model "),e("a",Ze,[r("Paper"),n(o)]),r(),Ke])]),e("li",null,[e("p",null,[r("2024-Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models "),e("a",Xe,[r("Paper"),n(o)]),r(),Qe])]),e("li",null,[e("p",null,[r("2024-OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving "),e("a",$e,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Drive-OccWorld: Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving "),e("a",er,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-CarFormer: Self-Driving with Learned Object-Centric Representations "),rr,r(),e("a",or,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space "),nr,r(),e("a",tr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Planning with Adaptive World Models for Autonomous Driving "),lr,r("; "),ar,r("; "),e("a",ir,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-UnO: Unsupervised Occupancy Fields for Perception and Forecasting "),e("a",sr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-LAW: Enhancing End-to-End Autonomous Driving with Latent World Model "),e("a",dr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving "),e("a",hr,[r("Paper"),n(o)]),r(", "),e("a",cr,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Delphi: Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation "),e("a",ur,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("ğŸ‘2024-Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability "),pr,r("; "),_r,r(),e("a",gr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving "),fr,r("; __"),e("a",mr,[r("Paper"),n(o)]),r(",")])]),e("li",null,[e("p",null,[r("2024-UniPAD: A Universal Pre-training Paradigm for Autonomous Driving "),vr,r("; "),br,r(),e("a",kr,[r("Paper"),n(o)]),r(", "),e("a",wr,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2024-GenAD: Generalized Predictive Model for Autonomous Driving "),Dr,r("; "),Pr,r(),e("a",Ar,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving "),xr,r(),e("a",Mr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2024-ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving "),yr,r("; "),Wr,r("; "),Cr,r("; "),Lr,r(),e("a",Ir,[r("Paper"),n(o)]),r(", "),e("a",Sr,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2024-Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion "),Gr,r("; "),Vr,r("; "),Rr,r("; "),Er,r(),e("a",Tr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model "),Nr,r("; "),Or,r(),e("a",zr,[r("Paper"),n(o)]),r(", "),e("a",Fr,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2023-MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations "),Ur,r("; "),Br,r("; "),jr,r(),e("a",qr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving "),Yr,r("; "),Hr,r("; "),Jr,r(),e("a",Zr,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-ADriver-I: A General World Model for Autonomous Driving "),Kr,r("; "),Xr,r("; "),Qr,r(),e("a",$r,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving "),eo,r("; "),ro,r("; "),oo,r(),e("a",no,[r("Paper"),n(o)]),r(", "),e("a",to,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2023-GAIA-1: A Generative World Model for Autonomous Driving "),lo,r("; "),ao,r("; "),io,r(),e("a",so,[r("Paper"),n(o)])]),e("details",ho,[r(" Related papers & tutorials to understand this paper: "),e("p",null,[r("FDM for video diffusion decoder: "),e("a",co,[r("Paper"),n(o)]),r(", "),e("a",uo,[r("Code"),n(o)])]),e("p",null,[r("Denoising diffusion tutorials: "),e("a",po,[r("CVPR 2022 tutorial"),n(o)]),r(", "),e("a",_o,[r("class from UC Berkeley"),n(o)]),r(", "),e("a",go,[r("Video"),n(o)])])])]),e("li",null,[e("p",null,[r("2023-DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving "),fo,r("; "),mo,r("; "),vo,r(),e("a",bo,[r("Paper"),n(o)]),r(", "),e("a",ko,[r("Code (To be released soon)"),n(o)])])]),e("li",null,[e("p",null,[r("2023-Neural World Models for Computer Vision "),wo,r("; "),Do,r(),e("a",Po,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2023-UniWorld: Autonomous Driving Pre-training via World Models "),Ao,r("; "),xo,r("; "),Mo,r(),e("a",yo,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2022-Separating the World and Ego Models for Self-Driving "),Wo,r("; "),Co,r(),e("a",Lo,[r("Paper"),n(o)]),r(", "),e("a",Io,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2022-SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model "),So,r("; "),Go,r("; "),Vo,r(),e("a",Ro,[r("Paper"),n(o)])])]),e("li",null,[e("p",null,[r("2022-MILE: Model-Based Imitation Learning for Urban Driving "),Eo,r("; "),To,r("; "),No,r(),e("a",Oo,[r("Paper"),n(o)]),r(", "),e("a",zo,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2022-Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models "),Fo,r(),e("a",Uo,[r("Paper"),n(o)]),r(", "),e("a",Bo,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2021-FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras "),jo,r("; "),qo,r("; "),Yo,r("; "),Ho,r(),e("a",Jo,[r("Paper"),n(o)]),r(", "),e("a",Zo,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2021-Learning to drive from a world on rails "),Ko,r("; "),Xo,r(),e("a",Qo,[r("Paper"),n(o)]),r(", "),e("a",$o,[r("Project Page"),n(o)]),r(", "),e("a",en,[r("Code"),n(o)])])]),e("li",null,[e("p",null,[r("2019-Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic "),rn,r("; "),on,r("; "),nn,r(),e("a",tn,[r("Paper"),n(o)]),r(", "),e("a",ln,[r("Code"),n(o)])])])]),an,e("ul",null,[e("li",null,[r("2024-Hierarchical World Models as Visual Whole-Body Humanoid Controllers "),e("a",sn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Pandora: Towards General World Model with Natural Language Actions and Video States "),e("a",dn,[r("Paper"),n(o)])]),hn,e("li",null,[r("2024-3D-VLA: A 3D Vision-Language-Action Generative World Model "),cn,r(),e("a",un,[r("Paper"),n(o)])]),e("li",null,[r("2024-Newton from Archetype AI "),pn,r(),e("a",_n,[r("Link"),n(o)])]),e("li",null,[r("2024-MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators "),gn,r(),e("a",fn,[r("Paper"),n(o)]),r(", "),e("a",mn,[r("Code"),n(o)])]),e("li",null,[r("2024-IWM: Learning and Leveraging World Models in Visual Representation Learning "),vn,r(", "),bn,r(),e("a",kn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Video as the New Language for Real-World Decision Making "),wn,r(", "),Dn,r(),e("a",Pn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Genie: Generative Interactive Environments "),An,r(),e("a",xn,[r("Paper"),n(o)]),r(", "),e("a",Mn,[r("Website"),n(o)])]),e("li",null,[r("2024-Sora "),yn,r(", "),Wn,r(),e("a",Cn,[r("Link"),n(o)]),r(", "),e("a",Ln,[r("Technical Report"),n(o)])]),e("li",null,[r("2024-LWM: World Model on Million-Length Video And Language With RingAttention "),In,r("; "),Sn,r(),e("a",Gn,[r("Paper"),n(o)]),r(", "),e("a",Vn,[r("Code"),n(o)])]),e("li",null,[r("2024-WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens "),Rn,r("; "),En,r(),e("a",Tn,[r("Paper"),n(o)])]),e("li",null,[r("2024-Video prediction models as rewards for reinforcement learning "),Nn,r(),e("a",On,[r("Paper"),n(o)]),r(", "),e("a",zn,[r("Code"),n(o)])]),e("li",null,[r("2024-V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video "),Fn,r(),e("a",Un,[r("Paper"),n(o)]),r(", "),e("a",Bn,[r("Code"),n(o)])]),e("li",null,[r("2023-Facing Off World Model Backbones: RNNs, Transformers, and S4 "),jn,r(),e("a",qn,[r("Paper"),n(o)])]),e("li",null,[r("2023-I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture "),Yn,r("; "),Hn,r(),e("a",Jn,[r("Paper"),n(o)]),r(", "),e("a",Zn,[r("Code"),n(o)])]),e("li",null,[r("2023-Temporally Consistent Transformers for Video Generation "),Kn,r(),e("a",Xn,[r("Paper"),n(o)]),r(", "),e("a",Qn,[r("Code"),n(o)])]),e("li",null,[r("2023-Learning to Model the World with Language "),$n,r(),e("a",et,[r("Paper"),n(o)]),r(", "),e("a",rt,[r("Code"),n(o)])]),e("li",null,[r("2023-Transformers are sample-efficient world models "),ot,r(";"),nt,r(),e("a",tt,[r("Paper"),n(o)]),r(", "),e("a",lt,[r("Code"),n(o)])]),e("li",null,[r("2023-Gradient-based Planning with World Models "),at,r("; "),it,r("; "),st,r("; "),e("a",dt,[r("Paper"),n(o)])]),e("li",null,[r("2023-World Models via Policy-Guided Trajectory Diffusion "),ht,r("; "),ct,r("; "),e("a",ut,[r("Paper"),n(o)])]),e("li",null,[r("2023-DreamerV3: Mastering diverse domains through world models "),pt,r(";"),_t,r("; "),e("a",gt,[r("Paper"),n(o)]),r(", "),e("a",ft,[r("Code"),n(o)])]),e("li",null,[r("2022-Daydreamer: World models for physical robot learning "),mt,r("; "),vt,r(),e("a",bt,[r("Paper"),n(o)]),r(", "),e("a",kt,[r("Code"),n(o)])]),e("li",null,[r("2022-Masked World Models for Visual Control "),wt,r("; "),Dt,r(),e("a",Pt,[r("Paper"),n(o)]),r(", "),e("a",At,[r("Code"),n(o)])]),e("li",null,[r("2022-A Path Towards Autonomous Machine Intelligence "),xt,r("; "),Mt,r("; "),yt,r("; "),e("a",Wt,[r("Paper"),n(o)]),r("; "),e("a",Ct,[r("Slides1"),n(o)]),r(", "),e("a",Lt,[r("Slides2"),n(o)]),r(", "),e("a",It,[r("Slides3"),n(o)]),r("; "),e("a",St,[r("Videos"),n(o)])]),e("li",null,[r("2021-LEXA:Discovering and Achieving Goals via World Models "),Gt,r("; "),e("a",Vt,[r("Paper"),n(o)]),r(", "),e("a",Rt,[r("Website & Code"),n(o)])]),e("li",null,[r("2021-DreamerV2: Mastering Atari with Discrete World Models "),Et,r("; "),Tt,r("; "),Nt,r(),e("a",Ot,[r("Paper"),n(o)]),r(", "),e("a",zt,[r("Code"),n(o)])]),e("li",null,[r("2020-Dreamer: Dream to Control: Learning Behaviors by Latent Imagination "),Ft,r(),e("a",Ut,[r("Paper"),n(o)]),r(", "),e("a",Bt,[r("Code"),n(o)])]),e("li",null,[r("2019-Learning Latent Dynamics for Planning from Pixels "),jt,r(),e("a",qt,[r("Paper"),n(o)]),r(", "),e("a",Yt,[r("Code"),n(o)])]),e("li",null,[r("2018-Model-Based Planning with Discrete and Continuous Actions "),Ht,r("; "),Jt,r("; "),Zt,r("; "),e("a",Kt,[r("Paper"),n(o)])]),e("li",null,[r("2018-Recurrent world models facilitate policy evolution "),Xt,r("; "),e("a",Qt,[r("Paper"),n(o)]),r(", "),e("a",$t,[r("Code"),n(o)])])]),el,rl,ol,e("ul",null,[e("li",null,[r("â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->magicdrive "),e("a",nl,[r("https://github.com/cure-lab/MagicDrive"),n(o)]),r(" ["),e("a",tl,[r("paper"),n(o)]),r("] ["),e("a",ll,[r("Code"),n(o)]),r("]å¯ä½œä¸ºbaseline. ä»å‡ ä½•æ ‡æ³¨ä¸­åˆæˆçš„æ•°æ®å¯ä»¥å¸®åŠ©ä¸‹æ¸¸ä»»åŠ¡,å¦‚2Dç›®æ ‡æ£€æµ‹ã€‚å› æ­¤,æœ¬æ–‡æ¢è®¨äº†text-to-image (T2I)æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¡—æ™¯å›¾åƒå¹¶æƒ åŠä¸‹æ¸¸3Dæ„ŸçŸ¥æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚")]),e("li",null,[r("â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->magicdrive3D ["),e("a",al,[r("paper"),n(o)]),r("] ["),e("a",il,[r("Code"),n(o)]),r("]")]),e("li",null,[r("â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->panacea "),e("a",sl,[r("https://zhuanlan.zhihu.com/p/684249231"),n(o)]),r("ç”¨äºç”Ÿæˆå¤šè§†è§’ä¸”å¯æ§çš„é©¾é©¶åœºæ™¯è§†é¢‘ï¼Œèƒ½å¤Ÿåˆæˆæ— é™æ•°é‡çš„å¤šæ ·åŒ–ã€å¸¦æ ‡æ³¨çš„æ ·æœ¬ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶çš„è¿›æ­¥æœ‰è‡³å…³é‡è¦çš„æ„ä¹‰ã€‚ Panaceaè§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šâ€œä¸€è‡´æ€§â€å’Œâ€œå¯æ§æ€§â€ã€‚ä¸€è‡´æ€§ç¡®ä¿æ—¶é—´å’Œè§†è§’çš„ä¸€è‡´æ€§ï¼Œè€Œå¯æ§æ€§ç¡®ä¿ç”Ÿæˆçš„å†…å®¹ä¸ç›¸åº”çš„æ ‡æ³¨å¯¹é½ã€‚")]),e("li",null,[r("â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->drive-WMè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ç°æœ‰ç«¯åˆ°ç«¯è§„åˆ’æ¨¡å‹å…¼å®¹çš„è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€‚é€šè¿‡ç”±è§†è§’["),e("a",dl,[r("ä¸»é¡µ"),n(o)]),r("]åˆ†è§£ä¿ƒè¿›çš„è”åˆç©ºé—´-æ—¶é—´å»ºæ¨¡ï¼ŒDrive-WMåœ¨é©¾é©¶åœºæ™¯ä¸­ç”Ÿæˆé«˜ä¿çœŸåº¦çš„å¤šè§†å›¾è§†é¢‘ã€‚")]),hl,cl,ul,pl,_l,gl,fl]),ml,e("ul",null,[e("li",null,[r("2023-Occupancy Prediction-Guided Neural Planner for Autonomous Driving "),vl,r("; "),bl,r("; "),kl,r(),e("a",wl,[r("Paper"),n(o)])])]),Dl,Pl,Al])}const Ll=a(_,[["render",xl],["__file","index.html.vue"]]),Il=JSON.parse('{"path":"/archiver/worldmodel/","title":"World Model","lang":"en-US","frontmatter":{"author":"Zhang Mingzhu (å¼ æ˜ç )","title":"World Model","description":"World-Models-Autonomous-Driving-Latest-Survey A curated list of world model for autonmous driving. Keep updated. ğŸ“Œ Introduction âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç† â¢ è®ºæ–‡æ±‡æ€» [1] https://github...","head":[["meta",{"property":"og:url","content":"https://openvisuallab.github.io/archiver/worldmodel/"}],["meta",{"property":"og:site_name","content":"OpenVisualLab"}],["meta",{"property":"og:title","content":"World Model"}],["meta",{"property":"og:description","content":"World-Models-Autonomous-Driving-Latest-Survey A curated list of world model for autonmous driving. Keep updated. ğŸ“Œ Introduction âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç† â¢ è®ºæ–‡æ±‡æ€» [1] https://github..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://openvisuallab.github.io/imgs/archiver/world_model/world_model.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-11-19T08:58:19.000Z"}],["meta",{"property":"article:author","content":"Zhang Mingzhu (å¼ æ˜ç )"}],["meta",{"property":"article:modified_time","content":"2024-11-19T08:58:19.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"World Model\\",\\"image\\":[\\"https://openvisuallab.github.io/imgs/archiver/world_model/world_model.png\\",\\"https://openvisuallab.github.io/imgs/archiver/world_model/End%20to%20end.png\\",\\"https://openvisuallab.github.io/imgs/archiver/world_model/Generation.png\\",\\"https://openvisuallab.github.io/imgs/archiver/world_model/ad.png\\"],\\"dateModified\\":\\"2024-11-19T08:58:19.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Zhang Mingzhu (å¼ æ˜ç )\\"}]}"]]},"headers":[{"level":2,"title":"ğŸ“Œ Introduction","slug":"ğŸ“Œ-introduction","link":"#ğŸ“Œ-introduction","children":[]},{"level":2,"title":"âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†","slug":"âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†","link":"#âœ§-ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†","children":[]},{"level":2,"title":"â¢ è®ºæ–‡æ±‡æ€»","slug":"â¢-è®ºæ–‡æ±‡æ€»","link":"#â¢-è®ºæ–‡æ±‡æ€»","children":[]},{"level":2,"title":"â¢ è®¤è¯†ä¸–ç•Œæ¨¡å‹","slug":"â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹","link":"#â¢-è®¤è¯†ä¸–ç•Œæ¨¡å‹","children":[{"level":3,"title":"1. ç®€å•ä»‹ç»ï¼ˆä»ä¸–ç•Œæ¨¡å‹--> è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆï¼‰","slug":"_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ","link":"#_1-ç®€å•ä»‹ç»-ä»ä¸–ç•Œæ¨¡å‹-è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆ","children":[]},{"level":3,"title":"2.è®ºæ–‡ç»¼è¿°","slug":"_2-è®ºæ–‡ç»¼è¿°","link":"#_2-è®ºæ–‡ç»¼è¿°","children":[]}]},{"level":2,"title":"3.æŒ‘æˆ˜èµ› Workshops/Challenges","slug":"_3-æŒ‘æˆ˜èµ›-workshops-challenges","link":"#_3-æŒ‘æˆ˜èµ›-workshops-challenges","children":[]},{"level":2,"title":"Tutorials/Talks/","slug":"tutorials-talks","link":"#tutorials-talks","children":[]},{"level":2,"title":"â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬/ å…¬å¸","slug":"â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸","link":"#â¢-ä¼˜ç§€å›¢é˜Ÿ-å­¦æœ¯å¤§ä½¬-å…¬å¸","children":[]},{"level":2,"title":"â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰","slug":"â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘","link":"#â¢-ç»å…¸è®ºæ–‡-æ¨èåŠ -ğŸ‘","children":[{"level":3,"title":"â–  Neural Driving Simulator based on World Models","slug":"â– -neural-driving-simulator-based-on-world-models","link":"#â– -neural-driving-simulator-based-on-world-models","children":[]},{"level":3,"title":"â–  End-to-end Driving based on World Models","slug":"â– -end-to-end-driving-based-on-world-models","link":"#â– -end-to-end-driving-based-on-world-models","children":[]},{"level":3,"title":"â–  æŒ‰æ—¶é—´é¡ºåºæ›´æ–°","slug":"â– -æŒ‰æ—¶é—´é¡ºåºæ›´æ–°","link":"#â– -æŒ‰æ—¶é—´é¡ºåºæ›´æ–°","children":[]}]},{"level":2,"title":"Papers","slug":"papers","link":"#papers","children":[]},{"level":2,"title":"Other General World Model Papers","slug":"other-general-world-model-papers","link":"#other-general-world-model-papers","children":[]},{"level":2,"title":"â–   â¢ å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘-->","slug":"â– -â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘","link":"#â– -â¢-å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘","children":[]},{"level":2,"title":"â–   â¢å¯æ§æ¡ä»¶ç”Ÿæˆ-->","slug":"â– -â¢å¯æ§æ¡ä»¶ç”Ÿæˆ","link":"#â– -â¢å¯æ§æ¡ä»¶ç”Ÿæˆ","children":[]},{"level":2,"title":"â–   â¢occä¸–ç•Œæ¨¡å‹-->","slug":"â– -â¢occä¸–ç•Œæ¨¡å‹","link":"#â– -â¢occä¸–ç•Œæ¨¡å‹","children":[{"level":3,"title":"1. DEFINITION","slug":"_1-definition","link":"#_1-definition","children":[]}]}],"git":{"createdTime":1726102486000,"updatedTime":1732006699000,"contributors":[{"name":"mingzhuzhang1","email":"145547769+mingzhuzhang1@users.noreply.github.com","commits":6},{"name":"2-mo","email":"1982800736@qq.com","commits":2}]},"readingTime":{"minutes":13.68,"words":4103},"filePathRelative":"archiver/worldmodel/README.md","localizedDate":"September 12, 2024","excerpt":"<blockquote>\\n<p>World-Models-Autonomous-Driving-Latest-Survey</p>\\n</blockquote>\\n<p>A curated list of world model for autonmous driving. Keep updated.</p>\\n<h2>ğŸ“Œ Introduction</h2>\\n<h2>âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†</h2>\\n<h2>â¢ è®ºæ–‡æ±‡æ€»</h2>\\n<p>[1] <a href=\\"https://github.com/GigaAI-research/General-World-Models-Survey\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/GigaAI-research/General-World-Models-Survey</a> è¯¥ repo å†…æœ‰ç›®å‰ä¸–ç•Œæ¨¡å‹æ–¹å‘çš„ä¼˜ç§€è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ï¼šè§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶å’Œè‡ªä¸»ä»£ç†ã€‚å…¶ä¸­è‡ªåŠ¨é©¾é©¶åˆ†æˆç«¯åˆ°ç«¯ã€ä»¥åŠ2Dã€3Dç¥ç»æ¨¡æ‹Ÿå™¨æ–¹æ³•ã€‚ä¸–ç•Œæ¨¡å‹çš„æ–‡çŒ®ã€ å¼€æºcodeã€ ç»¼è¿°ã€‚</p>","autoDesc":true}');export{Ll as comp,Il as data};
