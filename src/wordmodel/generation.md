---
title: ‰∏ñÁïåÊ®°ÂûãÁî®‰∫éËßÜÈ¢ëÁîüÊàê 
---
# World-Models-Autonomous-Driving-Latest-Survey
A curated list of world model for autonmous driving. Keep updated.
## üìå Introduction

In our report, we present a holistic examination of recent advancements in world model research, encompassing profound philosophical perspectives and detailed discussions. Our analysis delves deeply into the literature surrounding world models for **video generation**, **autonomous driving**, and **autonomous agents**, uncovering their applications in media production, artistic expression, end-to-end driving, games, and robots. We assess the existing challenges and limitations of world models and delve into prospective avenues for future research, with the intention of steering and igniting further progress in world models.

![Framework](./asset/Framework.png "Framework of general world models")

## Papers and Toolboxes for Video Generation World Models 
![VideoGen](./asset/VideoGen.png "video generation world models")

| Methods | Task | Github|
|:-----:|:-----:|:-----:|
| [Open-Sora-Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan/blob/main/docs/Report-v1.0.0.md)  | T2V Generation | [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/Open-Sora-Plan)|
| [Open-Sora](https://github.com/hpcaitech/Open-Sora/blob/main/docs/zh_CN/README.md)  | T2V Generation | [![Star](https://img.shields.io/github/stars/hpcaitech/Open-Sora.svg?style=social&label=Star)](https://github.com/hpcaitech/Open-Sora)|
| [Sora](https://openai.com/sora)  | T2V Generation & Editing | -|
| [IRC-GAN](https://www.ijcai.org/Proceedings/2019/0307.pdf) | T2V Generation | -|
| [TGANs-C](https://arxiv.org/pdf/1804.08264) | T2V Generation | -|
| [TFGANs](https://www.ijcai.org/Proceedings/2019/0276.pdf) | T2V Generation | -|
| [StoryGAN](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_StoryGAN_A_Sequential_Conditional_GAN_for_Story_Visualization_CVPR_2019_paper.pdf) | T2V Generation | [![Star](https://img.shields.io/github/stars/yitong91/StoryGAN.svg?style=social&label=Star)](https://github.com/yitong91/StoryGAN)|
| [TiVGAN](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171240) | T2V Generation | -|
| [GODIVA](https://arxiv.org/pdf/2104.14806) | T2V Generation | [![Star](https://img.shields.io/github/stars/breadbread1984/GODIVA.svg?style=social&label=Star)](https://github.com/breadbread1984/GODIVA)|
| [VideoGPT](https://arxiv.org/abs/2104.10157) | C2V Generation | [![Star](https://img.shields.io/github/stars/wilson1yan/VideoGPT.svg?style=social&label=Star)](https://github.com/wilson1yan/VideoGPT)|
| [StoryDALL-E](https://link.springer.com/chapter/10.1007/978-3-031-19836-6_5) | C2V Generation | -|
| [CogVideo](https://arxiv.org/pdf/2205.15868) | T2V Generation | [![Star](https://img.shields.io/github/stars/THUDM/CogVideo.svg?style=social&label=Star)](https://github.com/THUDM/CogVideo)|
| [Imagen Video](https://arxiv.org/pdf/2210.02303) | T2V Generation | -|
| [MAGViT](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf) | C2V Generation | [![Star](https://img.shields.io/github/stars/google-research/magvit.svg?style=social&label=Star)](https://github.com/google-research/magvit)|
| [MAGViT-V2](https://arxiv.org/abs/2310.05737) | C2V Generation | [![Star](https://img.shields.io/github/stars/lucidrains/magvit2-pytorch.svg?style=social&label=Star)](https://github.com/lucidrains/magvit2-pytorch)|
| [VideoPoet](https://arxiv.org/abs/2312.14125) | T2V Generation | -|
| [SVD](https://arxiv.org/abs/2311.15127) | T2V Generation | [![Star](https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&label=Star)](https://github.com/Stability-AI/generative-models)|
| [WorldDreamer](https://arxiv.org/abs/2401.09985) | T2V Generation | [![Star](https://img.shields.io/github/stars/JeffWang987/WorldDreamer.svg?style=social&label=Star)](https://github.com/JeffWang987/WorldDreamer)|
| [Latte](https://arxiv.org/abs/2401.09985) | T2V Generation | [![Star](https://img.shields.io/github/stars/Vchitect/Latte.svg?style=social&label=Star)](https://github.com/Vchitect/Latte)|
| [StreamingT2V](https://arxiv.org/abs/2403.14773) | T2V Generation | [![Star](https://img.shields.io/github/stars/Picsart-AI-Research/StreamingT2V.svg?style=social&label=Star)](https://github.com/Picsart-AI-Research/StreamingT2V)|


## Papers and Toolboxes for Autonomous Driving World Models 
![Drive](./asset/Drive.png "autonomous driving world models")

| Methods | Task | Github|
|:-----:|:-----:|:-----:|
| [Iso-Dream](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html)  | End-to-end Driving|-|
| [MILE](https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html)  | End-to-end Driving|[![Star](https://img.shields.io/github/stars/wayveai/mile.svg?style=social&label=Star)](https://github.com/wayveai/mile)|
| [SEM2](https://arxiv.org/abs/2210.04017)  | End-to-end Driving|-|
| [TrafficBots](https://ieeexplore.ieee.org/abstract/document/10161243)  | End-to-end Driving|-|
| [Think2Drive](https://arxiv.org/abs/2402.16720)  | End-to-end Driving|-|
| [GAIA-1](https://arxiv.org/abs/2309.17080)  | Neural Driving Simulator (2D) |-|
| [Tesla](https://www.youtube.com/watch?v=6x-Xb_uT7ts) | Neural Driving Simulator|-|
| [DriveDreamer](https://drivedreamer.github.io/)  | Neural Driving Simulator (2D) | [![Star](https://img.shields.io/github/stars/JeffWang987/DriveDreamer.svg?style=social&label=Star)](https://github.com/JeffWang987/DriveDreamer)|
| [ADriver-I](https://arxiv.org/abs/2309.17080)  | Neural Driving Simulator (2D) | -|
| [DrivingDiffusion](https://arxiv.org/abs/2310.07771)  | Neural Driving Simulator (2D) |-|
| [Panacea](https://panacea-ad.github.io/)  | Neural Driving Simulator (2D) | [![Star](https://img.shields.io/github/stars/wenyuqing/panacea.svg?style=social&label=Star)](https://github.com/wenyuqing/panacea)|
| [Drive-WM](https://drive-wm.github.io/)  | Neural Driving Simulator (2D) & End-to-end Driving| [![Star](https://img.shields.io/github/stars/BraveGroup/Drive-WM.svg?style=social&label=Star)](https://github.com/BraveGroup/Drive-WM)|
| [WoVoGen](https://arxiv.org/abs/2312.02934)  | Neural Driving Simulator (2D) | -|
| [DriveDreamer-2](https://drivedreamer2.github.io/)  | Neural Driving Simulator (2D) | [![Star](https://img.shields.io/github/stars/f1yfisher/DriveDreamer2.svg?style=social&label=Star)](https://github.com/f1yfisher/DriveDreamer2)|
| [GenAD](https://arxiv.org/abs/2403.09630)  | Neural Driving Simulator (2D) | [![Star](https://img.shields.io/github/stars/OpenDriveLab/DriveAGI?tab=readme-ov-file.svg?style=social&label=Star)](https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file)|
| [SubjectDrive](https://subjectdrive.github.io/)  | Neural Driving Simulator (2D) | -|
| [Copilot4D](https://arxiv.org/abs/2311.01017)  | Neural Driving Simulator (3D) | -|
| [OccWorld](https://arxiv.org/abs/2311.16038)  | Neural Driving Simulator (3D) | [![Star](https://img.shields.io/github/stars/wzzheng/OccWorld.svg?style=social&label=Star)](https://github.com/wzzheng/OccWorld)|
| [MUVO](https://arxiv.org/abs/2311.11762)  | Neural Driving Simulator (3D) | -|
| [LidarDM](https://www.zyrianov.org/lidardm/)  | Neural Driving Simulator (3D) | -|
| [UniWorld](https://arxiv.org/abs/2308.07234)  | Neural Driving Simulator (3D) & 4D Pre-training| -|
| [ViDAR](https://arxiv.org/abs/2312.17655)  | Neural Driving Simulator (3D) & 4D Pre-training| [![Star](https://img.shields.io/github/stars/OpenDriveLab/ViDAR.svg?style=social&label=Star)](https://github.com/OpenDriveLab/ViDAR)|
| [DriveWorld](XXX)  | Neural Driving Simulator (3D) & 4D Pre-training|-|



## Papers and Toolboxes for Autonomous Agents World Models 
![Agent](./asset/Agent.png "autonomous agents world models")

| Methods | Task | Github|
|:-----:|:-----:|:-----:|
| [PlaNet](https://planetrl.github.io/)  | Robotics | [![Star](https://img.shields.io/github/stars/google-research/planet.svg?style=social&label=Star)](https://github.com/google-research/planet) |
| [World Models](https://worldmodels.github.io/)  | Game Agent | [![Star](https://img.shields.io/github/stars/hardmaru/WorldModelsExperiments.svg?style=social&label=Star)](https://github.com/hardmaru/WorldModelsExperiments) |
| [RobotDreamPolicy](https://piergiaj.github.io/robot-dreaming-policy/)  | Robotics | - |
| [Plan2Explore](https://ramanans1.github.io/plan2explore/)  | Robotics | [![Star](https://img.shields.io/github/stars/ramanans1/plan2explore.svg?style=social&label=Star)](https://github.com/ramanans1/plan2explore) |
| [DreamerV1](https://danijar.com/project/dreamer/)  | Robotics | [![Star](https://img.shields.io/github/stars/danijar/dreamer.svg?style=social&label=Star)](https://github.com/danijar/dreamer) |
| [SimPLe](https://github.com/dhruvramani/model-based-atari/)  | Game Agent | [![Star](https://img.shields.io/github/stars/dhruvramani/model-based-atari.svg?style=social&label=Star)](https://github.com/dhruvramani/model-based-atari) |
| [Dreaming](https://arxiv.org/abs/2007.14535/)  | Robotics | - |
| [DreamerV2](https://danijar.com/project/dreamerv2/)  | Game Agent | [![Star](https://img.shields.io/github/stars/danijar/dreamerv2.svg?style=social&label=Star)](https://github.com/danijar/dreamerv2) |
| [LEXA](https://orybkin.github.io/lexa/)  | Robotics | [![Star](https://img.shields.io/github/stars/orybkin/lexa.svg?style=social&label=Star)](https://github.com/orybkin/lexa) |
| [PathDreamer](https://google-research.github.io/pathdreamer/)  | Indoor Navigation | [![Star](https://img.shields.io/github/stars/google-research/pathdreamer.svg?style=social&label=Star)](https://github.com/google-research/pathdreamer) |
| [DreamerPro](https://github.com/fdeng18/dreamer-pro/)  | Robotics | [![Star](https://img.shields.io/github/stars/fdeng18/dreamer-pro.svg?style=social&label=Star)](https://github.com/fdeng18/dreamer-pro) |
| [DreamingV2](https://arxiv.org/abs/2203.00494/)  | Robotics | - |
| [TransDreamer](https://github.com/changchencc/TransDreamer/)  | Game Agent & Robotics | [![Star](https://img.shields.io/github/stars/changchencc/TransDreamer.svg?style=social&label=Star)](https://github.com/changchencc/TransDreamer) |
| [IRIS](https://github.com/eloialonso/iris/)  | Game Agent | [![Star](https://img.shields.io/github/stars/eloialonso/iris.svg?style=social&label=Star)](https://github.com/eloialonso/iris) |
| [JEPA](https://openreview.net/forum?id=BZ5a1r-kVsf)  | Framework | - |
| [Dr.G](https://github.com/JeongsooHa/DrG/)  | Robotics | [![Star](https://img.shields.io/github/stars/JeongsooHa/DrG.svg?style=social&label=Star)](https://github.com/JeongsooHa/DrG) |
| [SWIM](https://human-world-model.github.io/)  | Robotics | - |
| [DreamerV3](https://danijar.com/project/dreamerv3/)  | Game Agent & Robotics | [![Star](https://img.shields.io/github/stars/danijar/dreamerv3.svg?style=social&label=Star)](https://github.com/danijar/dreamerv3) |
| [HarmonyDream](https://arxiv.org/abs/2310.00344/)  | Game Agent & Robotics | - |
| [DayDreamer](https://danijar.com/project/daydreamer/)  | Robotics | [![Star](https://img.shields.io/github/stars/danijar/daydreamer.svg?style=social&label=Star)](https://github.com/danijar/daydreamer) |
| [TWM](https://github.com/jrobine/twm/)  | Game Agent | [![Star](https://img.shields.io/github/stars/jrobine/twm.svg?style=social&label=Star)](https://github.com/jrobine/twm) |
| [STORM](https://github.com/weipu-zhang/storm/)  | Game Agent | [![Star](https://img.shields.io/github/stars/weipu-zhang/storm.svg?style=social&label=Star)](https://github.com/weipu-zhang/storm) |
| [MC-JEPA](https://arxiv.org/abs/2307.12698/)  | Optics Flow Prediction | - |
| [A-JEPA](https://arxiv.org/abs/2311.15830/)  | Audio Classification | - |
| [I_JEPA](https://github.com/facebookresearch/ijepa/)  | Image Semantics | [![Star](https://img.shields.io/github/stars/facebookresearch/ijepa.svg?style=social&label=Star)](https://github.com/facebookresearch/ijepa) |
| [SafeDreamer](https://sites.google.com/view/safedreamer/)  | Robotics | [![Star](https://img.shields.io/github/stars/PKU-Alignment/SafeDreamer.svg?style=social&label=Star)](https://github.com/PKU-Alignment/SafeDreamer) |
| [Genie](https://sites.google.com/view/genie-2024/home/)  | Generative Interactive Environment | - |
| [V-JEPA](https://github.com/facebookresearch/jepa/)  | Video Prediction | [![Star](https://img.shields.io/github/stars/facebookresearch/jepa.svg?style=social&label=Star)](https://github.com/facebookresearch/jepa) |
| [RoboDreamer](https://robovideo.github.io/)  | Robotics | - |
| [UniSim](https://universal-simulator.github.io/unisim/)  | Generative Interactive Environment | - |

## Papers
* 2024-OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving [Paper](https://www.arxiv.org/abs/2409.03272)
* 2024-Drive-OccWorld: Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving [Paper](https://arxiv.org/pdf/2408.14197)
* 2024-CarFormer: Self-Driving with Learned Object-Centric Representations  __`ECCV 2024`__ [Paper](https://arxiv.org/pdf/2407.15843)
* 2024-BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space  __`arxiv`__ [Paper](https://arxiv.org/pdf/2407.05679v1)
* 2024-Planning with Adaptive World Models for Autonomous Driving  __`arxiv`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2406.10714)
* 2024-UnO: Unsupervised Occupancy Fields for Perception and Forecasting [Paper](https://arxiv.org/pdf/2406.08691)
* 2024-LAW: Enhancing End-to-End Autonomous Driving with Latent World Model [Paper](https://arxiv.org/pdf/2406.08481)
* 2024-OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving [Paper](https://arxiv.org/ab/2405.20337), [Code](https://github.com/wzzheng/OccSora)
* 2024-Delphi: Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation [Paper](https://arxiv.org/abs/2406.01349)
* 2024-Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability __`from Shanghai AI Lab`__ [Paper](https://arxiv.org/pdf/2405.17398)
* 2024-DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving  __`CVPR 2024`__;  __[Paper](https://arxiv.org/pdf/2405.04390),
* 2024-UniPAD: A Universal Pre-training Paradigm for Autonomous Driving __`CVPR 2024`__;  __`from Shanghai AI Lab`__ [Paper](https://arxiv.org/abs/2310.08370), [Code](https://github.com/Nightmare-n/UniPAD)
* 2024-GenAD: Generalized Predictive Model for Autonomous Driving __`CVPR 2024`__;  __`from Shanghai AI Lab`__ [Paper](https://arxiv.org/pdf/2403.09630.pdf) 
* 2024-Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving  __`arxiv`__ [Paper](https://arxiv.org/pdf/2402.16720.pdf)
* 2024-ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving  __`CVPR 2024`__; __`Pre-training`__;  __`from Shanghai AI Lab`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2312.17655), [Code](https://github.com/OpenDriveLab/ViDAR)
* 2024-Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion __`ICLR 2024`__; __`Future Prediction`__; __`from Waabi`__; __`NuScenes, KITTI Odemetry, Argoverse2 Lidar datasets`__  [Paper](https://arxiv.org/abs/2311.01017)
* 2023-DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/pdf/2310.07771.pdf), [Code](https://github.com/shalfun/DrivingDiffusion)
* 2023-MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations __`arxiv`__; __`Pre-training`__; __`CARLA dataset`__ [Paper](https://arxiv.org/pdf/2311.11762.pdf)
* 2023-Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving __`arxiv`__; __`Generative AI, Planning`__; __`NuScenes and Waymo datasets`__ [Paper](https://arxiv.org/pdf/2311.17918.pdf)
* 2023-ADriver-I: A General World Model for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`NuScenes & one private dataset`__ [Paper](https://arxiv.org/pdf/2311.13549.pdf) 
* 2023-OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving __`arxiv`__; __`Occupancy Future Prediction, Planning`__; __`Occ3D dataset for Occupancy Future Prediction, NuScenes for motion planning`__ [Paper](https://arxiv.org/pdf/2311.16038.pdf), [Code](https://github.com/wzzheng/OccWorld)
* 2023-GAIA-1: A Generative World Model for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`Wayve's private data`__ [Paper](https://arxiv.org/pdf/2309.17080.pdf)
  <details span>
  Related papers & tutorials to understand this paper:
    
  FDM for video diffusion decoder: [Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/b2fe1ee8d936ac08dd26f2ff58986c8f-Paper-Conference.pdf), [Code](https://github.com/plai-group/flexible-video-diffusion-modeling)
  
  Denoising diffusion tutorials: [CVPR 2022 tutorial](https://www.youtube.com/watch?v=cS6JQpEY9cs), [class from UC Berkeley](https://www.youtube.com/watch?v=687zEGODmHA), [Video](https://www.youtube.com/watch?v=pea3sH6orMc)
  </details>
* 2023-DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2309.09777.pdf), [Code (To be released soon)](https://github.com/JeffWang987/DriveDreamer)
* 2023-Neural World Models for Computer Vision __'PhD Thesis'__; __`from Wayve`__  [Paper](https://arxiv.org/pdf/2306.09179)
* 2023-UniWorld: Autonomous Driving Pre-training via World Models __`arxiv`__; __`Pre-training`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2308.07234.pdf)
* 2022-Separating the World and Ego Models for Self-Driving __` ICLR 2022 workshop on Generalizable Policy Learning in the Physical World`__; __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/abs/2204.07184), [Code](https://github.com/vladisai/pytorch-ppuu)
* 2022-SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model  __`NeurIPS 2022 Deep Reinforcement Learning Workshop`__; __`RL`__; __`CARLA dataset`__ [Paper](https://arxiv.org/pdf/2210.04017.pdf)
* 2022-MILE: Model-Based Imitation Learning for Urban Driving __`NeurIPS 2022`__; __`RL`__; __`from Wayve`__ [Paper](https://arxiv.org/pdf/2210.07729.pdf), [Code](https://github.com/wayveai/mile)
* 2022-Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models __`NeurIPS 2022`__ [Paper](https://arxiv.org/pdf/2205.13817.pdf), [Code](https://github.com/panmt/iso-dream)
* 2021-FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras __`ICCV 2019`__; __`Future Prediction`__; __`from Wayve`__; __`NuScenes, Lyft datasets`__ [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf), [Code](https://github.com/wayveai/fiery)
* 2021-Learning to drive from a world on rails __`CVPR 2021 Oral`__; __`RL`__ [Paper](https://arxiv.org/pdf/2105.00636.pdf), [Project Page](https://dotchen.github.io/world_on_rails/), [Code](https://github.com/dotchen/WorldOnRails)
* 2019-Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic __`ICLR 2019`__; __`Future Prediction`__; __`from Yann Lecun's Group`__ [Paper](https://github.com/Atcold/pytorch-PPUU?tab=readme-ov-file), [Code](https://github.com/Atcold/pytorch-PPUU)
  
## Workshops/Challenges
* 2024-1X World Model Challenge  __`Challenges`__ [Link](https://github.com/1x-technologies/1xgpt)
* 2024-CVPR Workshop, Foundation Models for Autonomous Systems, Challenges, Track 4: Predictive World Model __`Challenges`__ [Link](https://opendrivelab.com/challenge2024/)

## Tutorials/Talks/
* 2023 __`from Wayve`__; [Video](https://www.youtube.com/watch?v=lNOs08byOhw)
* 2022-Neural World Models for Autonomous Driving [Video](https://www.youtube.com/watch?v=wMvYjiv6EpY)

## Surveys that Contain World Models for AD
* 2024-World Models for Autonomous Driving: An Initial Survey __`arxiv`__ [Paper](https://arxiv.org/abs/2403.02622)
* 2024-Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big
Data System, Data Mining, and Closed-Loop Technologies __`arxiv`__ [Paper](https://arxiv.org/pdf/2401.12888.pdf)
* 2024-Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities __`arxiv`__ [Paper](https://arxiv.org/pdf/2401.08045.pdf)

## Other General World Model Papers
* 2024-Hierarchical World Models as Visual Whole-Body Humanoid Controllers [Paper](https://arxiv.org/pdf/2405.18418)
* 2024-Pandora: Towards General World Model with Natural Language Actions and Video States [Paper](https://world-model.maitrix.org/assets/pandora.pdf)
* 2024-Efficient World Models with Time-Aware and Context-Augmented Tokenization __`ICML 2024`__ 
* 2024-3D-VLA: A 3D Vision-Language-Action Generative World Model __`ICML 2024`__ [Paper](https://arxiv.org/pdf/2403.09631.pdf)
* 2024-Newton from Archetype AI __`website`__ [Link](https://www.archetypeai.io/blog/introducing-archetype-ai---understand-the-real-world-in-real-time)
* 2024-MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators __`arxiv`__ [Paper](https://arxiv.org/pdf/2404.05014.pdf), [Code](https://github.com/PKU-YuanGroup/MagicTime)
* 2024-IWM: Learning and Leveraging World Models in Visual Representation Learning  __`arxiv`__, __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/pdf/2403.00504.pdf)
* 2024-Video as the New Language for Real-World Decision Making __`arxiv`__, __`Deepmind`__ [Paper](https://arxiv.org/abs/2402.17139)
* 2024-Genie: Generative Interactive Environments __`Deepmind`__ [Paper](https://arxiv.org/abs/2402.15391v1), [Website](https://sites.google.com/view/genie-2024/home)
* 2024-Sora __`OpenAI`__, __`Generative AI`__ [Link](https://openai.com/sora), [Technical Report](https://openai.com/research/video-generation-models-as-world-simulators)
* 2024-LWM: World Model on Million-Length Video And Language With RingAttention __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/abs/2402.08268), [Code](https://github.com/LargeWorldModel/LWM)
* 2024-WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/abs/2401.09985)
* 2024-Video prediction models as rewards for reinforcement learning __`NeurIPS 2024`__ [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/d9042abf40782fbce28901c1c9c0e8d8-Paper-Conference.pdf), [Code](https://github.com/Alescontrela/viper_rl)
* 2024-V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video __`from Yann Lecun's Group`__ [Paper](https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/), [Code](https://github.com/facebookresearch/jepa)
* 2023-Facing Off World Model Backbones: RNNs, Transformers, and S4 __`NeurIPS 2023`__ [Paper](https://arxiv.org/abs/2307.02064)
* 2023-I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture __`CVPR 2023`__; __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/abs/2301.08243), [Code](https://github.com/facebookresearch/ijepa)
* 2023-Temporally Consistent Transformers for Video Generation __`ICML 2023`__ [Paper](https://arxiv.org/abs/2210.02396), [Code](https://github.com/wilson1yan/teco)
* 2023-Learning to Model the World with Language __`arxiv`__ [Paper](https://arxiv.org/abs/2308.01399), [Code](https://github.com/jlin816/dynalang)
* 2023-Transformers are sample-efficient world models __`ICLR 2023`__;__`RL`__ [Paper](https://arxiv.org/pdf/2209.00588.pdf), [Code](https://github.com/eloialonso/iris)
* 2023-Gradient-based Planning with World Models __`arxiv`__; __`from Yann Lecun's Group`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2312.17227)
* 2023-World Models via Policy-Guided Trajectory Diffusion __`arxiv`__; __`RL`__; [Paper](https://arxiv.org/pdf/2312.08533.pdf)
* 2023-DreamerV3: Mastering diverse domains through world models __`arxiv`__;__`RL`__; [Paper](https://arxiv.org/abs/2301.04104), [Code](https://github.com/danijar/dreamerv3)
* 2022-Daydreamer: World models for physical robot learning __`CoRL 2022`__; __`Robotics`__ [Paper](https://arxiv.org/abs/2206.14176), [Code](https://github.com/danijar/daydreamer)
* 2022-Masked World Models for Visual Control __`CoRL 2022`__; __`Robotics`__ [Paper](https://proceedings.mlr.press/v205/seo23a.html), [Code](https://github.com/younggyoseo/MWM) 
* 2022-A Path Towards Autonomous Machine Intelligence __`openreview`__; __`from Yann Lecun's Group`__; __`General Roadmap for World Models`__; [Paper](https://openreview.net/forum?id=BZ5a1r-kVsf); [Slides1](https://leshouches2022.github.io/SLIDES/compressed-yann-1.pdf), [Slides2](https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-02.pdf), [Slides3](https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-03.pdf); [Videos](https://www.youtube.com/playlist?list=PLEIq5bchE3R3Yl5taXdYA04a9kH9yvyGm)
* 2021-LEXA:Discovering and Achieving Goals via World Models __`NeurIPS 2021`__; [Paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html), [Website & Code](https://orybkin.github.io/lexa/)
* 2021-DreamerV2: Mastering Atari with Discrete World Models __`ICLR 2021`__; __`RL`__; __`from Google & Deepmind`__ [Paper](https://arxiv.org/pdf/2010.02193.pdf), [Code](https://github.com/danijar/dreamerv2)
* 2020-Dreamer: Dream to Control: Learning Behaviors by Latent Imagination __`ICLR 2020`__ [Paper](https://arxiv.org/abs/1912.01603), [Code](https://github.com/google-research/dreamer)
* 2019-Learning Latent Dynamics for Planning from Pixels __`ICML 2019`__ [Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf), [Code](https://github.com/google-research/planet)
* 2018-Model-Based Planning with Discrete and Continuous Actions __`arxiv`__; __`RL, Planning`__; __`from Yann Lecun's Group`__;  [Paper](https://arxiv.org/pdf/1705.07177)
* 2018-Recurrent world models facilitate policy evolution __`NeurIPS 2018`__; [Paper](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf), [Code](https://github.com/hardmaru/WorldModelsExperiments)

## Other Related Papers
* 2023-Occupancy Prediction-Guided Neural Planner for Autonomous Driving __`ITSC 2023`__; __`Planning, Neural Predicted-Guided Planning`__; __`Waymo Open Motion dataset`__ [Paper](https://arxiv.org/abs/2305.03303)

## Other Related Repos
[Awesome-World-Model](https://github.com/LMD0311/Awesome-World-Model),

## ‚úß ËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµã (Video Anomaly Detection)



## ‚û¢ ËÆ∫ÊñáÊ±áÊÄª  

[1] <https://github.com/fjchange/awesome-video-anomaly-detection> ËØ• repo ÂÜÖÊúâÁõÆÂâç ËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÔºàVADÔºâ ÊñπÂêëÁöÑ‰ºòÁßÄËÆ∫ÊñáÊ±áÊÄªÔºåÂåÖÊã¨Âü∫Êú¨ÂàÜÁ±ª„ÄÅ Â∏∏Áî®Êï∞ÊçÆÂ∫ì‰∏ãËΩΩ„ÄÅ ÂºÄÊ∫êcode„ÄÅ ÁªºËø∞  
[2] <https://github.com/shot1107/anomaly_detection_papers>   ËØ•repo ÂÜÖÊúâÂºÇÂ∏∏Ê£ÄÊµãÊØèÂπ¥È°∂‰ºöÁöÑËÆ∫ÊñáÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÔºåÂèØÂèÇËÄÉÂÄüÈâ¥„ÄÇ

## ‚û¢ ËÆ§ËØÜÂºÇÂ∏∏Ê£ÄÊµã  

### 1. ÁÆÄÂçï‰ªãÁªçÔºà‰ªéÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµã--> ËßÜÈ¢ëÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµãÔºâ  

  [1]  ÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµãÁÆÄ‰ªãÔºö <https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw>
  [2]  Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµã‰ªãÁªçÔºö<https://mp.weixin.qq.com/s/Aghbz4m1eWFCNGgEy8q6Cg>  
  [3]  Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµãÁ†îÁ©∂Áé∞Áä∂Ôºö <https://mp.weixin.qq.com/s/MwpELRlC1cuDgqn4staAzA>  
  [4]  Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑËßÜÈ¢ëÂºÇÂ∏∏Ë°å‰∏∫‰∫ã‰ª∂Ê£ÄÊµãÁÆÄ‰ªã: <https://mp.weixin.qq.com/s/i3Xw2-ivARnF7rBSFtxugw>  
  [5]  Âü∫‰∫éËßÜÈ¢ëÁöÑÂºÇÂ∏∏Ë°å‰∏∫Ê£ÄÊµãÁÆóÊ≥ï‰ªãÁªç: <https://mp.weixin.qq.com/s/Dxsc3oCuO0wYkeFubMfSNw>  
  
### 2.ËÆ∫ÊñáÁªºËø∞  

  [1]  ÈÇ¨ÂºÄ‰øäÁ≠â. ËßÜÈ¢ëÂºÇÂ∏∏Ê£ÄÊµãÊäÄÊúØÁ†îÁ©∂ËøõÂ±ï[J]. ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏éÊé¢Á¥¢, 2022   Ôºà‰∏≠ÊñáÁªºËø∞Ôºå‰ΩÜÊ≤°ÊúâÈÇ£‰πàÂÖ®Èù¢ÔºåÂèØ‰ª•Êúâ‰∏Ä‰∏™ÂàùÊ≠•‰∫ÜËß£Ôºâ
  [2]  Bharathkumar Ramachandra et al. A survey of single-scene video anomaly detection  (TPAMI 2020)  

## ‚û¢ ‰ºòÁßÄÂõ¢Èòü / Â≠¶ÊúØÂ§ß‰Ω¨

### ‚ñ†  È´òÁõõÂçé  ‰∏äÊµ∑ÁßëÊäÄÂ§ßÂ≠¶ÔºàËßÜËßâ‰∏éÊï∞ÊçÆÊô∫ËÉΩ‰∏≠ÂøÉÔºâ  

[1]  A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework **(ICCV 2017)** -->proposed Shanghaitech dataset.  
[2]  Future Frame Prediction for Anomaly Detection ‚Äì A New Baseline **(CVPR 2018)**
[3]  Future Frame Prediction for Anomaly Detection  **(TPAMI 2022)**

### ‚ñ†  Radu Ionescu  SecurifAI/University of Bucharest

[1]  Detecting abnormal events in video using Narrowed Normality Clusters **(WACV 2019)**  
[2]  Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video **(CVPR 2019)**  
[3]  Anomaly Detection in Video via Self-Supervised and Multi-Task Learning **(CVPR 2021)**  
[4]  A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video **(TPAMI 2021)**  
[5]  UBnormal New Benchmark for Supervised Open-Set Video Anomaly Detection **(CVPR 2022)**
[6]  Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection **(CVPR 2022)**

## ‚û¢ ÁªèÂÖ∏ËÆ∫ÊñáÔºöÔºàÊé®ËçêÂä†‚Äúüëç‚ÄùÔºâ  

### ‚ñ† Unsupervised VAD  

* **Conference Papers**  
[1]  Learning Temporal Regularity in Video Sequences **(CVPR 2016)**  
[2]  A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework -->**Proposed Shanghaitech dataset.**
[2]  üëçFuture Frame Prediction for Anomaly Detection -- A New Baseline **(CVPR 2018)**  
[3]  üëçMemorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection **(ICCV 2019)** --> **The first to employ memory module on video anomaly detection**  
[4]  üëçObject-Centric Auto-Encoders and Dummy Anomalies for Abnormal Event Detection **(CVPR 2019)** --> **The first to combine object detection and vad to achieve object-level anomaly dtection.**  
[5]  AnoPCN: Video Anomaly Detection via Deep Predictive Coding Network **(ACM MM 2019)** --> **The first hybrid model**  
[6]  üëçLearning Memory-guided Normality for Anomaly Detection **(CVPR 2020)** --> **Based on MemAE**  
[7]  Cluster Attention Contrast for Video Anomaly Detection **(ACM MM 2020)** --> **The first to apply Contrastive Learninig**  
[8]  üëçAnomaly Detection in Video via Self-Supervised and Multi-Task Learning **(CVPR 2021)** --> **object-level**  
[9]  üëçA Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction **(ICCV 2021)** --> **Hybrid model**
[10]  Anomaly Detection in Video Sequence with Appearance-Motion Correspondence (ICCV 2019) --> **Two stream network**  
[11]  Video Anomaly Detection and Localization via Gaussian Mixture Fully Convolutional Variational Autoencoder --> **Two stream network**  
[12]  Self-supervised Sparse Representation for Video Anomaly Detection **(ECCV 2022)** --> A first attempt to slove unsupervised and weakly supervised VAD
[13]  Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles **(ECCV 2022)**  

* **Joural Papers**  
[1]   Video¬†Anomaly¬†Detection¬†with Sparse Coding Inspired Deep Neural Networks **(TPAMI 2021)**
[2]  A Background-Agnostic Framework With Adversarial Training for Abnormal Event¬†Detection¬†in¬†Video **(TPAMI 2022)**  
[3]  Influence-Aware Attention Networks for¬†Anomaly¬†Detection¬†in Surveillance¬†Videos **(TCSVT 2022)**  
[4]  Bidirectional Spatio-Temporal Feature Learning With Multiscale Evaluation for Video Anomaly Detection **(TCSVT 2022)**  
[5]  Anomaly¬†Detection¬†With Bidirectional Consistency in¬†Videos **(TNNLS 2022)**  
[6]  Variational Abnormal Behavior¬†Detection¬†With Motion Consistency **(TIP 2022)**  
[7]  DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos **(TPAMI 2023)**
[8]  A Hierarchical Spatio-Temporal Graph Convolutional Neural Network for¬†Anomaly¬†Detection¬†in¬†Videos **(TCSVT 2023)**  
[9]  Learnable Locality-Sensitive Hashing for¬†Video¬†Anomaly¬†Detection **(TCSVT 2023)**  
[10]  A Kalman Variational Autoencoder Model Assisted by Odometric Clustering for¬†Video¬†Frame Prediction and¬†Anomaly¬†Detection **(TIP 2023)**
[11]  Abnormal Event¬†Detection¬†and Localization via Adversarial Event Prediction **(TNNLS 2023)**  

### ‚ñ† Weakly supervised VAD

[1] üëç Real-world Anomaly Detection in Surveillance Videos  **(CVPR 2018)**  
[2] Weakly Supervised Video Anomaly Detection via Center-Guided Discrimative Learning **(ICME 2020)**  

[3] Decouple and Resolve: Transformer-Based Models for Online¬†Anomaly¬†Detection¬†From Weakly Labeled¬†Videos **(TIFS 2023)**  

## ‚û¢ ÁªèÂÖ∏È°πÁõÆ  

 ‚óã MNAD --> <https://github.com/cvlab-yonsei/MNAD>  ÂèØ‰Ωú‰∏∫baseline.  

## ‚û¢ ÂèëÁé∞ÁöÑÊñ∞ÁöÑÊúâÊÑèÊÄùÁöÑÁ†îÁ©∂ÊñπÂêë--> Explainable Anomaly Detection (EAD) ÂèØËß£ÈáäÊÄßÂºÇÂ∏∏Ê£ÄÊµã

### 1. DEFINITION

The aim of this TASK is to detect and automatically generate high-level explanations of anomalous events in video. Understanding the cause of an anomalous event is crucialas the required response is dependant on its nature andseverity. --> Anomaly Detection & Anoamly Explanation

### 2. RELATED WORK

[1] Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge (ICCV 2017)  
[2] X-MAN: Explaining multiple sources of anomalies in video (CVPR workshop 2021)  
[3] Discrete neural representations for explainable anomaly detection (WACV 2022)
[Awesome-World-Models-for-AD ](https://github.com/zhanghm1995/awesome-world-models-for-AD?tab=readme-ov-file#Table-of-Content),
[World models paper list from Shanghai AI lab](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving/blob/main/papers.md#world-model--model-based-rl),
[Awesome-Papers-World-Models-Autonomous-Driving](https://github.com/chaytonmin/Awesome-Papers-World-Models-Autonomous-Driving).
    

