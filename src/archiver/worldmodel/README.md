---
author: Zhang Mingzhu (å¼ æ˜ç )
title: World Model
---

> World-Models-Autonomous-Driving-Latest-Survey

A curated list of world model for autonmous driving. Keep updated.

## ğŸ“Œ Introduction

## âœ§ ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆç›¸å…³æ–‡çŒ®æ•´ç†

## â¢ è®ºæ–‡æ±‡æ€»  

[1] <https://github.com/GigaAI-research/General-World-Models-Survey> è¯¥ repo å†…æœ‰ç›®å‰ä¸–ç•Œæ¨¡å‹æ–¹å‘çš„ä¼˜ç§€è®ºæ–‡æ±‡æ€»ï¼ŒåŒ…æ‹¬åŸºæœ¬åˆ†ç±»ï¼šè§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶å’Œè‡ªä¸»ä»£ç†ã€‚å…¶ä¸­è‡ªåŠ¨é©¾é©¶åˆ†æˆç«¯åˆ°ç«¯ã€ä»¥åŠ2Dã€3Dç¥ç»æ¨¡æ‹Ÿå™¨æ–¹æ³•ã€‚ä¸–ç•Œæ¨¡å‹çš„æ–‡çŒ®ã€ å¼€æºcodeã€ ç»¼è¿°ã€‚

[2] <https://github.com/HaoranZhuExplorer/World-Models-Autonomous-Driving-Latest-Survey>   è¯¥repo å†…ä»¥â€˜æ—¶é—´â€™ä¸ºé¡ºåºç²¾é€‰ç›¸å…³ä¸–ç•Œè‡ªåŠ¨é©¾é©¶æ¨¡å‹ã€‚ä¸”å¹¶æŒç»­æ›´æ–°ï¼ŒåŒ…æ‹¬ä¸€äº›æŒ‘æˆ˜ã€ç›¸å…³è§†é¢‘ï¼ŒåŒ…æ‹¬æœºå™¨äººé¢†åŸŸçš„ä¸–ç•Œæ¨¡å‹ä½¿ç”¨ï¼ˆå¤§å¤šæ•°ä¸ºæ¨¡ä»¿å­¦ä¹ å¼ºåŒ–å­¦ä¹ æ–¹å‘ï¼‰å¯å‚è€ƒå€Ÿé‰´ã€‚

[3] [Awesome-World-Models-for-AD](https://github.com/zhanghm1995/awesome-world-models-for-AD?tab=readme-ov-file#Table-of-Content)

[4] [World models paper list from Shanghai AI lab](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving/blob/main/papers.md#world-model--model-based-rl)

[5] [Awesome-Papers-World-Models-Autonomous-Driving](https://github.com/chaytonmin/Awesome-Papers-World-Models-Autonomous-Driving).

## â¢ è®¤è¯†ä¸–ç•Œæ¨¡å‹

<!--è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­-->
<!-- <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/world_model.png" width="20%" />
</p> -->

![world model](/imgs/archiver/world_model/world_model.png)

### 1. ç®€å•ä»‹ç»ï¼ˆä»ä¸–ç•Œæ¨¡å‹--> è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ç”¨äºåœºæ™¯ç”Ÿæˆï¼‰  

  [1]  ä¸–ç•Œæ¨¡å‹ç®€ä»‹ï¼š<https://mp.weixin.qq.com/s/UmT0DjFqRPsjv2m28ySvdw>ä¸–ç•Œæ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤šç§æ„ŸçŸ¥ä¿¡æ¯ï¼Œå¦‚è§†è§‰ã€å¬è§‰å’Œè¯­è¨€ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰æ–¹æ³•æ¥ç†è§£å’Œé¢„æµ‹ç°å®ä¸–ç•Œã€‚å®ƒåŒ…æ‹¬æ„ŸçŸ¥æ¨¡å—ã€è¡¨å¾å­¦ä¹ ã€åŠ¨åŠ›å­¦æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºæ„å»ºç¯å¢ƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œä¸ä»…èƒ½åæ˜ å½“å‰çŠ¶æ€ï¼Œè¿˜èƒ½é¢„æµ‹æœªæ¥å˜åŒ–ã€‚è¿™ç§æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€æ¸¸æˆå¼€å‘å’Œæœºå™¨äººå­¦ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚Yann LeCunæå‡ºçš„è¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒé€šè¿‡è‡ªç›‘ç£å­¦ä¹ è®©AIåƒäººä¸€æ ·ç†è§£ä¸–ç•Œï¼Œå½¢æˆå†…éƒ¨çš„å¿ƒç†è¡¨å¾ï¼Œä»¥æœŸå®ç°é€šç”¨äººå·¥æ™ºèƒ½ã€‚Metaçš„I-JEPAæ¨¡å‹æ˜¯åŸºäºè¿™ä¸€æ„¿æ™¯çš„å®ç°ï¼Œå®ƒé€šè¿‡åˆ†æå’Œè¡¥å…¨å›¾åƒå±•ç¤ºäº†å¯¹ä¸–ç•ŒèƒŒæ™¯çŸ¥è¯†çš„åº”ç”¨ã€‚

  [2]  å½±å“è¾ƒå¤§çš„æ—©æœŸä¸–ç•Œæ¨¡å‹æ–‡ç« ï¼š2018å¹´Jurgenåœ¨NeurIPS ä»¥å¾ªç¯ä¸–ç•Œæ¨¡å‹ä¿ƒè¿›ç­–ç•¥æ¼”å˜â€œRecurrent World Models Facilitate Policy Evolutionâ€çš„titleå‘è¡¨ï¼šé“¾æ¥: <https://arxiv.org/abs/1803.10122> ç¤ºä¾‹: <https://worldmodels.github.io/>  
  
  [3]  ä¸–ç•Œæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ï¼š <https://www.bilibili.com/read/cv34465959/>
  
  [4]  ä¸–ç•Œæ¨¡å‹ç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç”Ÿæˆä»¥åŠä»¿çœŸå¹³å°: <https://blog.csdn.net/CV_Autobot/article/details/134002647>
  
### 2.è®ºæ–‡ç»¼è¿°  

 [1]  2024-Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyondï¼Œ __`arxiv`__ [Paper](https://arxiv.org/abs/2403.02622) æä½³ç§‘æŠ€ ï¼ˆæ¯”è¾ƒå…¨é¢ï¼Œè¯¥ç»¼è¿°é€šè¿‡ 260 ä½™ç¯‡æ–‡çŒ®ï¼Œå¯¹ä¸–ç•Œæ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½ä½“ã€é€šç”¨æœºå™¨äººç­‰é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨è¿›è¡Œäº†è¯¦å°½çš„åˆ†æå’Œè®¨è®ºã€‚å¦å¤–ï¼Œè¯¥ç»¼è¿°è¿˜å®¡è§†äº†å½“å‰ä¸–ç•Œæ¨¡å‹çš„æŒ‘æˆ˜å’Œå±€é™æ€§ï¼Œå¹¶å±•æœ›äº†å®ƒä»¬æœªæ¥çš„å‘å±•æ–¹å‘ã€‚ï¼‰
  
 [2] 2024-World Models for Autonomous Driving: An Initial Surveyï¼ŒIEEE TIV,æ¾³é—¨å¤§å­¦ï¼Œå¤å¨å¤·å¤§å­¦ã€‚[Paper](https://arxiv.org/abs/2403.02622)ï¼ˆç”»é£æœ‰è¶£ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„ç°çŠ¶å’Œæœªæ¥è¿›å±•è¿›è¡Œäº†åˆæ­¥å›é¡¾ï¼Œæ¶µç›–äº†å®ƒä»¬çš„ç†è®ºåŸºç¡€ã€å®é™…åº”ç”¨ä»¥åŠæ—¨åœ¨å…‹æœç°æœ‰å±€é™æ€§çš„æ­£åœ¨è¿›è¡Œçš„ç ”ç©¶å·¥ä½œã€‚ï¼‰

  [3]2024-Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies __`arxiv`__ [Paper](https://arxiv.org/pdf/2401.12888.pdf)

   [4]2024-Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities __`arxiv`__ [Paper](https://arxiv.org/pdf/2401.08045.pdf)  

## 3.æŒ‘æˆ˜èµ› Workshops/Challenges

* 2024-1X World Model Challenge  __`Challenges`__ [Link](https://github.com/1x-technologies/1xgpt)
* 2024-ECCV Corner case Challenge  __`Challenges`__ [Link](https://github.com/1x-technologies/1xgpt)
  
* 2024-CVPR Workshop, Foundation Models for Autonomous Systems, Challenges, Track 4: Predictive World Model __`Challenges`__ [Link](https://opendrivelab.com/challenge2024/)

## Tutorials/Talks/

* 2023 __`from Wayve`__; [Video](https://www.youtube.com/watch?v=lNOs08byOhw)
  
* 2022-Neural World Models for Autonomous Driving [Video](https://www.youtube.com/watch?v=wMvYjiv6EpY)

## â¢ ä¼˜ç§€å›¢é˜Ÿ / å­¦æœ¯å¤§ä½¬/ å…¬å¸

#### â–   ä¸Šæµ·AILabï¼ˆä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ï¼‰ <https://opendrivelab.com/publications/>

#### â–   é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆé™ˆé“ è€å¸ˆå›¢é˜Ÿï¼‰Geometric-Controllable Visual Generation: A Systematic Solution  [Video](https://www.bilibili.com/video/BV18T421v7Nf/?spm_id_from=333.337.search-card.all.click)

#### â–   æä½³ç§‘æŠ€ï¼ˆæä½³ç§‘æŠ€DriveDreamerè‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€WorldDreameré€šç”¨ä¸–ç•Œæ¨¡å‹ç›®å‰å·²æˆåŠŸå•†ä¸šåŒ–è½åœ°ã€‚ï¼‰[æ¨æ–‡](https://baijiahao.baidu.com/s?id=1799624134723943641)

#### â–   Wayveã€Teslaã€æ—·è§†ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€

#### â–  è”šæ¥è½¦ä¼ï¼š<https://www.qbitai.com/2024/07/172025.html>

## â¢ ç»å…¸è®ºæ–‡ï¼šï¼ˆæ¨èåŠ â€œğŸ‘â€ï¼‰  

#### + World Models are adept at representing an agent's spatio-temporal knowledge about its environment through the prediction of future changes

#### + There are two main types of world models in Autonomous Driving aimed at reducing driving uncertainty, i.e., World Model as Neural Driving Simulator and World Model for End-to-end Driving

<!--è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­-->
<!-- <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/End%20to%20end.png" width="20%" />
</p> -->

![world model](/imgs/archiver/world_model/End%20to%20end.png)


#### +  In the real environment, methods like GAIA-1 and Copilot4D involve utilizing generative models to construct neural simulators that produce 2D or 3D future scenes to enhance predictive capabilities


#### + In the simulation environment, methods such as MILE and TrafficBots are based on reinforcement learning, enhancing their capacity for decision-making and future prediction, thereby paving the way to end-to-end autonomous driving

<!--è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­-->
<!-- <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/Generation.png" width="100%" />
</p> -->

![world model](/imgs/archiver/world_model/Generation.png)


### â–  Neural Driving Simulator based on World Models

#### â–  2D Scene Generation

+ ğŸ‘(2023 Arxiv) GAIA-1: A generative world model for autonomous driving [[Paper](https://arxiv.org/abs/2309.17080)][[Blog](https://wayve.ai/thinking/scaling-gaia-1/)] (Wayve)
* (2023 CVPR 2023 workshop) [[Video](https://www.youtube.com/watch?v=6x-Xb_uT7ts)] (Tesla)
* ğŸ‘(2023 Arxiv) DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving [[Paper](https://drivedreamer.github.io/)][[Code](https://github.com/JeffWang987/DriveDreamer)] (GigaAI)
* (2023 Arxiv) ADriver-I: A General World Model for Autonomous Driving [[Paper](https://arxiv.org/abs/2311.13549)] (MEGVII)
* ğŸ‘(2023 Arxiv) DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model [[Paper](https://arxiv.org/abs/2310.07771)] (Baidu)
* (2023 Arxiv) Panacea: Panoramic and Controllable Video Generation for Autonomous Driving [[Paper](https://panacea-ad.github.io/)][[Code](https://github.com/wenyuqing/panacea)] (MEGVII)
* ğŸ‘(2024 CVPR) Drive-WM: Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving [[Paper](https://drive-wm.github.io/)][[Code](https://github.com/BraveGroup/Drive-WM)] (CASIA)
* (2023 Arxiv) WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation [[Paper](https://arxiv.org/abs/2312.02934)] (Fudan)
* (2024 Arxiv) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation [[Paper](https://drivedreamer2.github.io/)][[Code](https://github.com/f1yfisher/DriveDreamer2)] (GigaAI)
* (2024 CVPR) GenAD: Generalized Predictive Model for Autonomous Driving [[Paper](https://arxiv.org/abs/2403.09630)][[Code](https://github.com/OpenDriveLab/DriveAGI?tab=readme-ov-file)] (Shanghai AI Lab)
* (2024 Arxiv) SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control [[Paper](https://subjectdrive.github.io/)] (MEGVII)

#### â–  3D Scene Generation

+ ğŸ‘(2024 ICLR) Copilot4D:Learning unsupervised world models for autonomous driving via discrete diffusion [[Paper](https://arxiv.org/abs/2311.01017)] (Waabi)
* (2023 Arxiv) OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving [[Paper](https://arxiv.org/abs/2311.16038)][[Code](https://github.com/wzzheng/OccWorld)] (THU)
* (2023 Arxiv) MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations [[Paper](https://arxiv.org/abs/2311.11762)] (KIT)
* (2024 Arxiv) LidarDM: Generative LiDAR Simulation in a Generated World [[Paper](https://www.zyrianov.org/lidardm/)][[Code](https://github.com/vzyrianov/lidardm)] (MIT)
  
#### â–  4D Pre-training for Autonomous Driving


* (2024 CVPR) ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving [[Paper](https://arxiv.org/abs/2312.17655)][[Code](https://github.com/OpenDriveLab/ViDAR)] (Shanghai AI Lab)
* ğŸ‘(2024 CVPR) DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving [[Paper](XXX)] (PKU)
  
### â–  End-to-end Driving based on World Models

+ ğŸ‘(2022 NeurIPS) Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9316769afaaeeaad42a9e3633b14e801-Abstract-Conference.html)] (SJTU)
* ğŸ‘(2022 NeurIPS) MILE: Model-Based Imitation Learning for Urban Driving [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/827cb489449ea216e4a257c47e407d18-Abstract-Conference.html)][[Code](https://github.com/wayveai/mile)] (Wayve)
* (2022 NeurIPS Deep RL Workshop) SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model [[Paper](https://arxiv.org/abs/2210.04017)] (HIT & THU)
* (2023 ICRA) TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction [[Paper](https://ieeexplore.ieee.org/abstract/document/10161243)] (ETH Zurich)
* (2024 Arxiv) Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2) [[Paper](https://arxiv.org/abs/2402.16720)] (SJTU)

<!--è¿™é‡Œå›¾åƒçš„è·¯å¾„ä¸ç”¨å†™å®Œå…¨ï¼Œéœ€è¦æŒ‰ç…§vuepressçš„æ ¼å¼æ¥ï¼Œè€Œä¸”å¿…é¡»æ”¾åˆ°src/.vuepress/publicæ–‡ä»¶å¤¹ä¸­-->
<!-- <p style="text-align: center;">
  <img src="/src/.vuepress/public/imgs/archiver/world_model/ad.png" width="100%" />
</p> -->

![world model](/imgs/archiver/world_model/ad.png)


### â–  æŒ‰æ—¶é—´é¡ºåºæ›´æ–°

## Papers
* 2024-DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model [Paper](https://arxiv.org/abs/2410.10738)  __`Dataset`__
* 2024-Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models [Paper](https://arxiv.org/abs/2409.16663)  __`Planning`__
* 2024-OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving [Paper](https://www.arxiv.org/abs/2409.03272)
* 2024-Drive-OccWorld: Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving [Paper](https://arxiv.org/pdf/2408.14197)
* 2024-CarFormer: Self-Driving with Learned Object-Centric Representations  __`ECCV 2024`__ [Paper](https://arxiv.org/pdf/2407.15843)
* 2024-BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space  __`arxiv`__ [Paper](https://arxiv.org/pdf/2407.05679v1)
* 2024-Planning with Adaptive World Models for Autonomous Driving  __`arxiv`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2406.10714)
* 2024-UnO: Unsupervised Occupancy Fields for Perception and Forecasting [Paper](https://arxiv.org/pdf/2406.08691)
* 2024-LAW: Enhancing End-to-End Autonomous Driving with Latent World Model [Paper](https://arxiv.org/pdf/2406.08481)
* 2024-OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving [Paper](https://arxiv.org/ab/2405.20337), [Code](https://github.com/wzzheng/OccSora)
* 2024-Delphi: Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation [Paper](https://arxiv.org/abs/2406.01349)
* ğŸ‘2024-Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability __`NeurIPS 2024`__; __`from Shanghai AI Lab`__ [Paper](https://arxiv.org/pdf/2405.17398)
* 2024-DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving  __`CVPR 2024`__;  __[Paper](https://arxiv.org/pdf/2405.04390),
* 2024-UniPAD: A Universal Pre-training Paradigm for Autonomous Driving __`CVPR 2024`__;  __`from Shanghai AI Lab`__ [Paper](https://arxiv.org/abs/2310.08370), [Code](https://github.com/Nightmare-n/UniPAD)
* 2024-GenAD: Generalized Predictive Model for Autonomous Driving __`CVPR 2024`__;  __`from Shanghai AI Lab`__ [Paper](https://arxiv.org/pdf/2403.09630.pdf)
* 2024-Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving  __`arxiv`__ [Paper](https://arxiv.org/pdf/2402.16720.pdf)
* 2024-ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving  __`CVPR 2024`__; __`Pre-training`__;  __`from Shanghai AI Lab`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2312.17655), [Code](https://github.com/OpenDriveLab/ViDAR)
* 2024-Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion __`ICLR 2024`__; __`Future Prediction`__; __`from Waabi`__; __`NuScenes, KITTI Odemetry, Argoverse2 Lidar datasets`__  [Paper](https://arxiv.org/abs/2311.01017)
* 2023-DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/pdf/2310.07771.pdf), [Code](https://github.com/shalfun/DrivingDiffusion)
* 2023-MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations __`arxiv`__; __`Pre-training`__; __`CARLA dataset`__ [Paper](https://arxiv.org/pdf/2311.11762.pdf)
* 2023-Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving __`arxiv`__; __`Generative AI, Planning`__; __`NuScenes and Waymo datasets`__ [Paper](https://arxiv.org/pdf/2311.17918.pdf)
* 2023-ADriver-I: A General World Model for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`NuScenes & one private dataset`__ [Paper](https://arxiv.org/pdf/2311.13549.pdf)
* 2023-OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving __`arxiv`__; __`Occupancy Future Prediction, Planning`__; __`Occ3D dataset for Occupancy Future Prediction, NuScenes for motion planning`__ [Paper](https://arxiv.org/pdf/2311.16038.pdf), [Code](https://github.com/wzzheng/OccWorld)
* 2023-GAIA-1: A Generative World Model for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`Wayve's private data`__ [Paper](https://arxiv.org/pdf/2309.17080.pdf)
  <details span>
  Related papers & tutorials to understand this paper:

  FDM for video diffusion decoder: [Paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/b2fe1ee8d936ac08dd26f2ff58986c8f-Paper-Conference.pdf), [Code](https://github.com/plai-group/flexible-video-diffusion-modeling)
  
  Denoising diffusion tutorials: [CVPR 2022 tutorial](https://www.youtube.com/watch?v=cS6JQpEY9cs), [class from UC Berkeley](https://www.youtube.com/watch?v=687zEGODmHA), [Video](https://www.youtube.com/watch?v=pea3sH6orMc)
  </details>
* 2023-DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2309.09777.pdf), [Code (To be released soon)](https://github.com/JeffWang987/DriveDreamer)
* 2023-Neural World Models for Computer Vision __'PhD Thesis'__; __`from Wayve`__  [Paper](https://arxiv.org/pdf/2306.09179)
* 2023-UniWorld: Autonomous Driving Pre-training via World Models __`arxiv`__; __`Pre-training`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2308.07234.pdf)
* 2022-Separating the World and Ego Models for Self-Driving __`ICLR 2022 workshop on Generalizable Policy Learning in the Physical World`__; __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/abs/2204.07184), [Code](https://github.com/vladisai/pytorch-ppuu)
* 2022-SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model  __`NeurIPS 2022 Deep Reinforcement Learning Workshop`__; __`RL`__; __`CARLA dataset`__ [Paper](https://arxiv.org/pdf/2210.04017.pdf)
* 2022-MILE: Model-Based Imitation Learning for Urban Driving __`NeurIPS 2022`__; __`RL`__; __`from Wayve`__ [Paper](https://arxiv.org/pdf/2210.07729.pdf), [Code](https://github.com/wayveai/mile)
* 2022-Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models __`NeurIPS 2022`__ [Paper](https://arxiv.org/pdf/2205.13817.pdf), [Code](https://github.com/panmt/iso-dream)
* 2021-FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras __`ICCV 2019`__; __`Future Prediction`__; __`from Wayve`__; __`NuScenes, Lyft datasets`__ [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf), [Code](https://github.com/wayveai/fiery)
* 2021-Learning to drive from a world on rails __`CVPR 2021 Oral`__; __`RL`__ [Paper](https://arxiv.org/pdf/2105.00636.pdf), [Project Page](https://dotchen.github.io/world_on_rails/), [Code](https://github.com/dotchen/WorldOnRails)
* 2019-Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic __`ICLR 2019`__; __`Future Prediction`__; __`from Yann Lecun's Group`__ [Paper](https://github.com/Atcold/pytorch-PPUU?tab=readme-ov-file), [Code](https://github.com/Atcold/pytorch-PPUU)
  
## Other General World Model Papers

* 2024-Hierarchical World Models as Visual Whole-Body Humanoid Controllers [Paper](https://arxiv.org/pdf/2405.18418)
* 2024-Pandora: Towards General World Model with Natural Language Actions and Video States [Paper](https://world-model.maitrix.org/assets/pandora.pdf)
* 2024-Efficient World Models with Time-Aware and Context-Augmented Tokenization __`ICML 2024`__
* 2024-3D-VLA: A 3D Vision-Language-Action Generative World Model __`ICML 2024`__ [Paper](https://arxiv.org/pdf/2403.09631.pdf)
* 2024-Newton from Archetype AI __`website`__ [Link](https://www.archetypeai.io/blog/introducing-archetype-ai---understand-the-real-world-in-real-time)
* 2024-MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators __`arxiv`__ [Paper](https://arxiv.org/pdf/2404.05014.pdf), [Code](https://github.com/PKU-YuanGroup/MagicTime)
* 2024-IWM: Learning and Leveraging World Models in Visual Representation Learning  __`arxiv`__, __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/pdf/2403.00504.pdf)
* 2024-Video as the New Language for Real-World Decision Making __`arxiv`__, __`Deepmind`__ [Paper](https://arxiv.org/abs/2402.17139)
* 2024-Genie: Generative Interactive Environments __`Deepmind`__ [Paper](https://arxiv.org/abs/2402.15391v1), [Website](https://sites.google.com/view/genie-2024/home)
* 2024-Sora __`OpenAI`__, __`Generative AI`__ [Link](https://openai.com/sora), [Technical Report](https://openai.com/research/video-generation-models-as-world-simulators)
* 2024-LWM: World Model on Million-Length Video And Language With RingAttention __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/abs/2402.08268), [Code](https://github.com/LargeWorldModel/LWM)
* 2024-WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/abs/2401.09985)
* 2024-Video prediction models as rewards for reinforcement learning __`NeurIPS 2024`__ [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/d9042abf40782fbce28901c1c9c0e8d8-Paper-Conference.pdf), [Code](https://github.com/Alescontrela/viper_rl)
* 2024-V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video __`from Yann Lecun's Group`__ [Paper](https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/), [Code](https://github.com/facebookresearch/jepa)
* 2023-Facing Off World Model Backbones: RNNs, Transformers, and S4 __`NeurIPS 2023`__ [Paper](https://arxiv.org/abs/2307.02064)
* 2023-I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture __`CVPR 2023`__; __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/abs/2301.08243), [Code](https://github.com/facebookresearch/ijepa)
* 2023-Temporally Consistent Transformers for Video Generation __`ICML 2023`__ [Paper](https://arxiv.org/abs/2210.02396), [Code](https://github.com/wilson1yan/teco)
* 2023-Learning to Model the World with Language __`arxiv`__ [Paper](https://arxiv.org/abs/2308.01399), [Code](https://github.com/jlin816/dynalang)
* 2023-Transformers are sample-efficient world models __`ICLR 2023`__;__`RL`__ [Paper](https://arxiv.org/pdf/2209.00588.pdf), [Code](https://github.com/eloialonso/iris)
* 2023-Gradient-based Planning with World Models __`arxiv`__; __`from Yann Lecun's Group`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2312.17227)
* 2023-World Models via Policy-Guided Trajectory Diffusion __`arxiv`__; __`RL`__; [Paper](https://arxiv.org/pdf/2312.08533.pdf)
* 2023-DreamerV3: Mastering diverse domains through world models __`arxiv`__;__`RL`__; [Paper](https://arxiv.org/abs/2301.04104), [Code](https://github.com/danijar/dreamerv3)
* 2022-Daydreamer: World models for physical robot learning __`CoRL 2022`__; __`Robotics`__ [Paper](https://arxiv.org/abs/2206.14176), [Code](https://github.com/danijar/daydreamer)
* 2022-Masked World Models for Visual Control __`CoRL 2022`__; __`Robotics`__ [Paper](https://proceedings.mlr.press/v205/seo23a.html), [Code](https://github.com/younggyoseo/MWM)
* 2022-A Path Towards Autonomous Machine Intelligence __`openreview`__; __`from Yann Lecun's Group`__; __`General Roadmap for World Models`__; [Paper](https://openreview.net/forum?id=BZ5a1r-kVsf); [Slides1](https://leshouches2022.github.io/SLIDES/compressed-yann-1.pdf), [Slides2](https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-02.pdf), [Slides3](https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-03.pdf); [Videos](https://www.youtube.com/playlist?list=PLEIq5bchE3R3Yl5taXdYA04a9kH9yvyGm)
* 2021-LEXA:Discovering and Achieving Goals via World Models __`NeurIPS 2021`__; [Paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html), [Website & Code](https://orybkin.github.io/lexa/)
* 2021-DreamerV2: Mastering Atari with Discrete World Models __`ICLR 2021`__; __`RL`__; __`from Google & Deepmind`__ [Paper](https://arxiv.org/pdf/2010.02193.pdf), [Code](https://github.com/danijar/dreamerv2)
* 2020-Dreamer: Dream to Control: Learning Behaviors by Latent Imagination __`ICLR 2020`__ [Paper](https://arxiv.org/abs/1912.01603), [Code](https://github.com/google-research/dreamer)
* 2019-Learning Latent Dynamics for Planning from Pixels __`ICML 2019`__ [Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf), [Code](https://github.com/google-research/planet)
* 2018-Model-Based Planning with Discrete and Continuous Actions __`arxiv`__; __`RL, Planning`__; __`from Yann Lecun's Group`__;  [Paper](https://arxiv.org/pdf/1705.07177)
* 2018-Recurrent world models facilitate policy evolution __`NeurIPS 2018`__; [Paper](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf), [Code](https://github.com/hardmaru/WorldModelsExperiments)

## â–   â¢ å‘ç°çš„æ–°çš„æœ‰æ„æ€çš„ç ”ç©¶æ–¹å‘-->

ç”Ÿæˆå¼çš„World Modelå¯ä»¥è¢«ç”¨æ¥å½“ä½œä¸€ç§ä»¿çœŸå·¥å…·æ¥ç”Ÿæˆä»¿çœŸæ•°æ®ï¼Œç‰¹åˆ«æ˜¯æä¸ºå°‘è§çš„Corner Caseçš„æ•°æ®ã€‚ç‰¹åˆ«æ˜¯åŸºäºText to image çš„å¯æ§æ¡ä»¶ç”ŸæˆCorner Caseï¼Œå¯ä»¥è¿›è¡Œæ•°æ®å¢å¹¿ï¼Œè§£å†³çœŸå®æ•°æ®ä¸”æ ‡æ³¨å°‘çš„ç°å­˜é—®é¢˜ã€‚
ç„¶è€ŒWorld Modelæ›´æœ‰æ½œåŠ›çš„åº”ç”¨æ–¹å‘æ˜¯World Modelå¯èƒ½ä¼šæˆä¸ºåƒGPTä¸€æ ·çš„è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼Œè€Œå…¶ä»–è‡ªåŠ¨é©¾é©¶å…·ä½“ä»»åŠ¡éƒ½ä¼šå›´ç»•è¿™ä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œç ”å‘æ„å»ºã€‚
é‡ç‚¹é˜…è¯»vista
è¿˜æœ‰ä¸€ç¯‡æ˜¯æ–°å‡ºçš„æ•°æ®é›†å¯ä»¥è¿›è¡Œå¤ç°ã€‚

## â–   â¢å¯æ§æ¡ä»¶ç”Ÿæˆ-->

* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->magicdrive <https://github.com/cure-lab/MagicDrive>   [[paper](https://arxiv.org/abs/2310.02601)] [[Code](https://github.com/cure-lab/MagicDrive)]å¯ä½œä¸ºbaseline. ä»å‡ ä½•æ ‡æ³¨ä¸­åˆæˆçš„æ•°æ®å¯ä»¥å¸®åŠ©ä¸‹æ¸¸ä»»åŠ¡,å¦‚2Dç›®æ ‡æ£€æµ‹ã€‚å› æ­¤,æœ¬æ–‡æ¢è®¨äº†text-to-image (T2I)æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¡—æ™¯å›¾åƒå¹¶æƒ åŠä¸‹æ¸¸3Dæ„ŸçŸ¥æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->magicdrive3D  [[paper](https://arxiv.org/abs/2405.14475)] [[Code](https://github.com/flymin/MagicDrive3D)]
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->panacea <https://zhuanlan.zhihu.com/p/684249231>ç”¨äºç”Ÿæˆå¤šè§†è§’ä¸”å¯æ§çš„é©¾é©¶åœºæ™¯è§†é¢‘ï¼Œèƒ½å¤Ÿåˆæˆæ— é™æ•°é‡çš„å¤šæ ·åŒ–ã€å¸¦æ ‡æ³¨çš„æ ·æœ¬ï¼Œè¿™å¯¹äºè‡ªåŠ¨é©¾é©¶çš„è¿›æ­¥æœ‰è‡³å…³é‡è¦çš„æ„ä¹‰ã€‚ Panaceaè§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šâ€œä¸€è‡´æ€§â€å’Œâ€œå¯æ§æ€§â€ã€‚ä¸€è‡´æ€§ç¡®ä¿æ—¶é—´å’Œè§†è§’çš„ä¸€è‡´æ€§ï¼Œè€Œå¯æ§æ€§ç¡®ä¿ç”Ÿæˆçš„å†…å®¹ä¸ç›¸åº”çš„æ ‡æ³¨å¯¹é½ã€‚
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->drive-WMè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ç°æœ‰ç«¯åˆ°ç«¯è§„åˆ’æ¨¡å‹å…¼å®¹çš„è‡ªåŠ¨é©¾é©¶ä¸–ç•Œæ¨¡å‹ã€‚é€šè¿‡ç”±è§†è§’[[ä¸»é¡µ](https://drive-wm.github.io/)]åˆ†è§£ä¿ƒè¿›çš„è”åˆç©ºé—´-æ—¶é—´å»ºæ¨¡ï¼ŒDrive-WMåœ¨é©¾é©¶åœºæ™¯ä¸­ç”Ÿæˆé«˜ä¿çœŸåº¦çš„å¤šè§†å›¾è§†é¢‘ã€‚
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->Geodiffusion
* â—‹ -->Detdiffusion
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->BevControl
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ-->BevControl
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆ--PerLDiff
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆå­¦ä¹ å¹³å°--CarDreamer
* â—‹ å¯æ§æ¡ä»¶ç”Ÿæˆå­¦ä¹ å¹³å°--DriveArena

## â–   â¢occä¸–ç•Œæ¨¡å‹-->

* 2023-Occupancy Prediction-Guided Neural Planner for Autonomous Driving __`ITSC 2023`__; __`Planning, Neural Predicted-Guided Planning`__; __`Waymo Open Motion dataset`__ [Paper](https://arxiv.org/abs/2305.03303)

+ ä¸occç»“åˆçš„ï¼šoccworldã€Drive-occwordã€OccLLaMAã€Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Drivingã€OccSora

### 1. DEFINITION

æ¯ç¯‡æ–‡ç« çš„åˆ›æ–°ç‚¹ä»å•ä¸€å¤šè§†è§’åˆ°ç¯è§†è§†è§’ã€ä»å•ä¸€è¾“å…¥åˆ°å¤šæ¨¡æ€è¾“å…¥æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¿˜æœ‰è§†é¢‘ç”Ÿæˆçš„æ—¶é•¿ï¼Œä»¥åŠè½¨è¿¹é¢„æµ‹ï¼ŒåŸºäºå†³ç­–çš„ä¸–ç•Œæ¨¡å‹æ–¹æ³•ã€‚éœ€è§£å†³æ—¶ç©ºä¸ä¸€è‡´æ€§å’Œç”Ÿæˆåœºæ™¯è¿ç»­æ€§ã€‚

